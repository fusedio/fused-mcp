{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "22a99aea-7b52-4a62-b084-f5e0bdddfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fused\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7190247e-8f0b-47de-8953-8b8c77717d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need your local paths\n",
    "PATH_TO_CLAUDE_CONFIG = (\n",
    "    f\"{str(Path.home())}\\AppData\\Roaming\\Claude/claude_desktop_config.json\"\n",
    ")\n",
    "\n",
    "\n",
    "if not os.path.exists(PATH_TO_CLAUDE_CONFIG):\n",
    "    # Creating the config file\n",
    "    os.makedirs(os.path.dirname(PATH_TO_CLAUDE_CONFIG), exist_ok=True)\n",
    "    with open(PATH_TO_CLAUDE_CONFIG, \"w\") as f:\n",
    "        json.dump({}, f)\n",
    "\n",
    "assert os.path.exists(PATH_TO_CLAUDE_CONFIG), (\n",
    "    \"Please update the PATH_TO_CLAUDE_CONFIG variable with the correct path to your Claude config file\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4675bb8e-5e7b-4cfc-9209-810bf52831a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path to the Claude app\n",
    "CLAUDE_APP_PATH = r\"C:\\Users\\user\\AppData\\Local\\AnthropicClaude\\claude.exe\"\n",
    "assert os.path.exists(CLAUDE_APP_PATH), (\n",
    "    \"Please update the CLAUDE_APP_PATH variable with the correct path to your Claude app\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7807a52f-f669-47c4-ab6c-1f542562421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this path if you're not running this from the repo root\n",
    "WORKING_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "092b706b-ac1e-4215-9884-f0435e2235ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_name='utils' source_code='# To use these functions, add the following command in your UDF:\\n# `common = fused.utils.common`\\nfrom __future__ import annotations\\nimport fused\\nimport pandas as pd\\nimport numpy as np\\nimport json\\nimport os\\nfrom numpy.typing import NDArray\\nfrom typing import Dict, List, Literal, Optional, Sequence, Tuple, Union, Any\\nfrom loguru import logger\\nfrom fused.api.api import AnyBaseUdf\\n\\ndef to_pickle(obj):\\n    \"\"\"Encode an object to a pickle byte stream and store in DataFrame.\"\"\"\\n    import pickle\\n    import pandas as pd\\n    return pd.DataFrame(\\n        {\"data_type\": [type(obj).__name__], \"data_content\": [pickle.dumps(obj)]}\\n    )\\n\\ndef from_pickle(df):\\n    \"\"\"Decode an object from a DataFrame containing pickle byte stream.\"\"\"\\n    import pickle\\n    return pickle.loads(df[\"data_content\"].iloc[0])\\n\\ndef df_summary(df, description=\"\", n_head=5, n_tail=5, n_sample=5, n_unique=100, add_details=True):\\n    val = description+\"\\\\n\\\\n\"\\n    val += \"These are stats for df (pd.DataFrame):\\\\n\"\\n    val += f\"{list(df.columns)=} \\\\n\\\\n\"\\n    val += f\"{df.isnull().sum()=} \\\\n\\\\n\"\\n    val += f\"{df.describe().to_json()=} \\\\n\\\\n\"\\n    val += f\"{df.head(n_head).to_json()=} \\\\n\\\\n\"\\n    val += f\"{df.tail(n_tail).to_json()=} \\\\n\\\\n\"\\n    if len(df) > n_sample:\\n        val += f\"{df.sample(n_sample).to_json()=} \\\\n\\\\n\"\\n    if add_details:\\n        if len(df) <= n_unique:\\n            val += f\"{df.to_json()} \\\\n\\\\n\"\\n        else:\\n            for c in df.columns:\\n                value_counts = df[c].value_counts()\\n                df[c].value_counts().head()\\n                val += f\"df[{c}].value_counts()\\\\n{value_counts} \\\\n\\\\n\"\\n                val += f\"{df[c].unique()[:n_unique]} \\\\n\\\\n\"\\n    return val\\n\\ndef get_diff_text(text1: str, text2: str, as_html: bool=True, only_diff: bool=False) -> str:\\n    import difflib\\n    import html\\n    \\n    diff = difflib.ndiff(text1.splitlines(keepends=True), text2.splitlines(keepends=True))\\n    processed_diff = []\\n    \\n    if not as_html:\\n        for line in diff:\\n            if line.startswith(\"+\"):\\n                processed_diff.append(f\"ADD: {line}\")  # Additions\\n            elif line.startswith(\"-\"):\\n                processed_diff.append(f\"DEL: {line}\")  # Deletions\\n            else:\\n                if not only_diff:\\n                    processed_diff.append(f\"  {line}\")  # Unchanged lines\\n        return \"\\\\n\".join(processed_diff)            \\n    \\n    for line in diff:\\n        escaped_line = html.escape(line)  # Escape HTML to preserve special characters\\n        \\n        if line.startswith(\"+\"):\\n            processed_diff.append(f\"<span style=\\'color:green; line-height:normal;\\'> {escaped_line} </span><br>\")  # Green for additions\\n        elif line.startswith(\"-\"):\\n            processed_diff.append(f\"<span style=\\'color:red; line-height:normal;\\'> {escaped_line} </span><br>\")  # Red for deletions\\n        else:\\n            if not only_diff:\\n                processed_diff.append(f\"<span style=\\'color:gray; line-height:normal;\\'> {escaped_line} </span><br>\")  # Gray for unchanged lines\\n    \\n    # HTML structure with a dropdown for selecting background color\\n    html_output = \"\"\"\\n    <div>\\n        <label for=\"backgroundColor\" style=\"color:gray;\">Choose Background Color: </label>\\n        <select id=\"backgroundColor\" onchange=\"document.getElementById(\\'diff-container\\').style.backgroundColor = this.value;\">\\n            <option value=\"#111111\">Dark Gray</option>\\n            <option value=\"#f0f0f0\">Light Gray</option>\\n            <option value=\"#ffffff\">White</option>\\n            <option value=\"#e0f7fa\">Cyan</option>\\n            <option value=\"#ffebee\">Pink</option>\\n            <option value=\"#c8e6c9\">Green</option>\\n        </select>\\n    </div>\\n    <div id=\"diff-container\" style=\"background-color:#111111; padding:10px; font-family:monospace; white-space:pre; line-height:normal;\">\\n        {}</div>\\n    \"\"\".format(\"\".join(processed_diff))\\n    return html_output\\n\\n\\ndef json_path_from_secret(var=\\'gcs_fused\\'):\\n    import json\\n    import tempfile\\n    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode=\"w\") as tmp_file:\\n        json.dump(json.loads(fused.secrets[var]), tmp_file)\\n        tmp_file_path = tmp_file.name\\n    return tmp_file_path\\n    \\ndef gcs_credentials_from_secret(var=\\'gcs_fused\\'):\\n    \"\"\"\\n    example: \\n    \"\"\"\\n    import gcsfs\\n    import json\\n    from google.oauth2 import service_account\\n    \\n    gcs_secret = json.loads(fused.secrets[var])\\n    credentials = service_account.Credentials.from_service_account_info(gcs_secret)\\n    return credentials\\n\\n@fused.cache\\ndef simplify_gdf(gdf, pct=1, args=\\'-o force -clean\\'):\\n    #ref https://github.com/mbloch/mapshaper/blob/master/REFERENCE.md\\n    import geopandas as gpd\\n    import uuid\\n    import json\\n    file_path = f\\'/mount/cache_data/temp/{uuid.uuid4()}.json\\'\\n    print(file_path)\\n    with open(file_path, \"w\") as f:\\n        json.dump(gdf.__geo_interface__, f)\\n    # print(fused.utils.common.run_cmd(\\'npm install -g mapshaper\\',communicate=True)[1].decode())    \\n    print(run_cmd(f\\'/mount/npm/bin/mapshaper {file_path} -simplify {pct}% {args} -o {file_path.replace(\".json\",\"2.json\")}\\',communicate=True))\\n    return gpd.read_file(file_path.replace(\".json\",\"2.json\"))\\n\\ndef html_to_obj(html_str):\\n    from fastapi import Response\\n    return Response(html_str.encode(\\'utf-8\\'), media_type=\"text/html\")\\n\\ndef pydeck_to_obj(map, as_string=False):    \\n        html_str = map.to_html(as_string=True)\\n        if as_string:\\n            return html_str\\n        else: \\n            return html_to_obj(html_str)\\n\\ndef altair_to_obj(chart, as_string=False):    \\n    import io\\n    html_buffer = io.StringIO()\\n    chart.save(html_buffer, format=\"html\")\\n    html_str = html_buffer.getvalue()\\n    html_buffer.close()\\n    if as_string:\\n        return html_str\\n    else: \\n        return html_to_obj(html_str)\\n\\ndef html_params(html_template, params={}, **kw):\\n    \\'\\'\\'Exampl: html_params(\\'<div>{{v1}}{{v2}}</div>\\',{\\'v1\\':\\'hello \\'}, v2=\\'world!\\')\\'\\'\\'\\n    from jinja2 import Template\\n    template = Template(html_template)\\n    return template.render(params, **kw)\\n\\n\\ndef url_redirect(url):\\n    from fastapi import Response\\n    return Response(f\\'<meta http-equiv=\"refresh\" content=\"0; url={url}\">\\'.encode(\\'utf-8\\'), media_type=\"text/html\")\\n    \\n@fused.cache\\ndef read_shapefile(url):\\n    import geopandas as gpd\\n    try:\\n        return gpd.read_file(url)\\n    except:\\n        path = fused.download(url, url)\\n        name=str(path).split(\\'/\\')[-1].split(\\'.\\')[0]\\n        print(name)\\n        try:\\n            return gpd.read_file(f\"zip://{path}!{name}/{name}.shp\")\\n        except:\\n            return gpd.read_file(f\"zip://{path}!{name}.shp\")\\n\\ndef point_to_line(start_lon=-122.4194, start_lat=37.7749, end_lon=-122.2712, end_lat=37.8044, value=0):\\n    import geopandas as gpd\\n    import shapely\\n    from shapely.geometry import LineString    \\n    line = LineString([(start_lon, start_lat), (end_lon, end_lat)])    \\n    return gpd.GeoDataFrame({\"value\": [value]}, geometry=[line], crs=4326)\\n\\ndef interpolate_points(line_shape, point_distance):\\n    points = []\\n    num_points = int(line_shape.length // point_distance)\\n    for i in range(num_points + 1):\\n        point = line_shape.interpolate(i * point_distance)\\n        points.append(point)\\n    return points\\n\\ndef pointify(lines, point_distance, segment_col=\\'segment_id\\'):\\n    import geopandas as gpd\\n    crs_orig = lines.crs\\n    crs_utm=lines.estimate_utm_crs()\\n    lines = lines.to_crs(crs_utm)\\n    from shapely.geometry import Point\\n    import pandas as pd\\n    points_list = []\\n    for _, row in lines.iterrows():\\n        line = row[\\'geometry\\']\\n        points = interpolate_points(line, point_distance)\\n        for point in points:\\n            points_list.append({segment_col: row[segment_col], \\'geometry\\': point})\\n    print(\\'number of points:\\', len(points_list))\\n    points_gdf = gpd.GeoDataFrame(points_list, crs=lines.crs)\\n    return points_gdf.to_crs(crs_orig)\\n\\ndef chunkify(lst, chunk_size):\\n    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\\n\\n@fused.cache\\ndef get_meta_chunk_datestr(base_path, total_row_groups=52, start_year=2020, end_year=2024, n_chunks_row_group=2, n_chunks_datestr=90,):\\n    import pandas as pd\\n    date_list = pd.date_range(start=f\\'{start_year}-01-01\\', end=f\\'{end_year+1}-01-01\\').strftime(\\'%Y-%m-%d\\').tolist()[:-1]\\n    df = pd.DataFrame([[i[0],i[-1]] for i in chunkify(date_list,n_chunks_datestr)], columns=[\\'start_datestr\\',\\'end_datestr\\'])\\n    df[\\'row_group_ids\\']=[chunkify(list(range(total_row_groups)),n_chunks_row_group)]*len(df)\\n    df = df.explode(\\'row_group_ids\\').reset_index(drop=True)\\n    df[\\'path\\'] = df.apply(lambda row:f\"{base_path.strip(\\'/\\')}/file_{row.start_datestr.replace(\\'-\\',\\'\\')}_{row.end_datestr.replace(\\'-\\',\\'\\')}_{row.row_group_ids[0]}_{row.row_group_ids[-1]}.parquet\", axis=1)\\n    df[\\'idx\\'] = df.index\\n    df[\\'row_group\\'] = df[\\'row_group_ids\\']\\n    df = df.explode(\\'row_group\\').drop_duplicates(\\'idx\\').sort_values(\\'row_group\\')\\n    del df[\\'row_group\\']\\n    return df\\n\\ndef write_log(msg=\"Your message.\", name=\\'//default\\', log_type=\\'info\\', rotation=\"10 MB\"):\\n    from loguru import logger\\n    path = fused.file_path(\\'logs/\\' + name + \\'.log\\')\\n    logger.add(path, rotation=rotation)\\n    if log_type==\\'warning\\':\\n        logger.warning(msg)\\n    else:\\n        logger.info(msg)  # Write the log message\\n    logger.remove()  # Remove the log handler\\n    from datetime import datetime\\n    timestamp = datetime.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n    print(f\"{timestamp} | {path} |msg: {msg}\" )\\n\\ndef read_log(n=None, name=\\'default\\', return_log=False):\\n    path = fused.file_path(\\'logs/\\' + name + \\'.log\\')\\n    try:\\n        with open(path, \\'r\\') as file:\\n            log_content = file.readlines()\\n            if n:\\n                log_content = \\'\\'.join(log_content[-n:])  # Return last \\'tail_lines\\' entries\\n            else:\\n                log_content = \\'\\'.join(log_content)\\n            if return_log:\\n                return log_content\\n            else:\\n                print(log_content)\\n    except FileNotFoundError:\\n        if return_log:\\n            return \"Log file not found.\"\\n        else:\\n            print(\"Log file not found.\")\\n\\ndef dilate_bbox(bounds, chip_len, border_pixel):\\n    bbox_crs = bounds.crs\\n    clipped_chip=chip_len-(border_pixel*2)\\n    bounds = bounds.to_crs(bounds.estimate_utm_crs())\\n    length = bounds.area[0]**0.5\\n    buffer_ratio = (chip_len-clipped_chip)/clipped_chip\\n    buffer_distance=length*buffer_ratio/2\\n    bounds.geometry = bounds.buffer(buffer_distance)\\n    bounds = bounds.to_crs(bbox_crs)\\n    return bounds\\n\\ndef read_gdf_file(path):\\n    import geopandas as gpd\\n\\n    extension = path.rsplit(\".\", maxsplit=1)[-1].lower()\\n    if extension in [\"gpkg\", \"shp\", \"geojson\"]:\\n        driver = (\\n            \"GPKG\"\\n            if extension == \"gpkg\"\\n            else (\"ESRI Shapefile\" if extension == \"shp\" else \"GeoJSON\")\\n        )\\n        return gpd.read_file(path, driver=driver)\\n    elif extension == \"zip\":\\n        return gpd.read_file(f\"zip+{path}\")\\n    elif extension in [\"parquet\", \"pq\"]:\\n        return gpd.read_parquet(path)\\n\\n\\ndef url_to_arr(url, return_colormap=False):\\n    from io import BytesIO\\n\\n    import rasterio\\n    import requests\\n    from rasterio.plot import show\\n\\n    response = requests.get(url)\\n    print(response.status_code)\\n    with rasterio.open(BytesIO(response.content)) as dataset:\\n        if return_colormap:\\n            colormap = dataset.colormap\\n            return dataset.read(), dataset.colormap(1)\\n        else:\\n            return dataset.read()\\n\\n\\ndef read_shape_zip(url, file_index=0, name_prefix=\"\"):\\n    \"\"\"This function opens any zipped shapefile\"\"\"\\n    import zipfile\\n\\n    import geopandas as gpd\\n\\n    path = fused.core.download(url, name_prefix + url.split(\"/\")[-1])\\n    fnames = [\\n        i.filename for i in zipfile.ZipFile(path).filelist if i.filename[-4:] == \".shp\"\\n    ]\\n    df = gpd.read_file(f\"{path}!{fnames[file_index]}\")\\n    return df\\n\\n@fused.cache\\ndef stac_to_gdf(bounds, datetime=\\'2024\\', collections=[\"sentinel-2-l2a\"], columns=[\\'id\\', \\'geometry\\', \\'bounds\\', \\'assets\\', \\'datetime\\', \\'eo:cloud_cover\\'], query={\"eo:cloud_cover\": {\"lt\": 20}}, catalog=\\'mspc\\', explode_assets=False, version=0):\\n    import pystac_client\\n    import stac_geoparquet\\n    if catalog.lower()==\\'aws\\':\\n        catalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\\n    elif catalog.lower()==\\'mspc\\':\\n        import planetary_computer\\n        # catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\\n        catalog = pystac_client.Client.open(\\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\\n            modifier=planetary_computer.sign_inplace,\\n        )\\n    else: \\n        catalog = pystac_client.Client.open(catalog)\\n    items = catalog.search(\\n        collections=collections,\\n        bbox=bounds.total_bounds,\\n        datetime=datetime,\\n        query=query,\\n        ).item_collection()\\n    gdf=stac_geoparquet.to_geodataframe([item.to_dict() for item in items])\\n    if explode_assets:\\n        gdf[\\'assets\\'] = gdf.assets.map(lambda x: [{k:x[k][\\'href\\']} for k in x]) \\n        gdf = gdf.explode(\\'assets\\')\\n        gdf[\\'band\\'] = gdf.assets.map(lambda x: list(x.keys())[0]) \\n        gdf[\\'url\\'] = gdf.assets.map(lambda x: list(x.values())[0]) \\n        del gdf[\\'assets\\']\\n        if columns:\\n            columns+=[\\'band\\',\\'url\\']\\n    if columns==None:\\n        print(gdf.columns)\\n        return gdf\\n    else:\\n        gdf=gdf[list(set(columns).intersection(set(gdf.columns)))]\\n        print(gdf.columns)\\n        return gdf\\n\\n@fused.cache\\ndef get_url_aws_stac(bounds, collections=[\"cop-dem-glo-30\"]):\\n    import pystac_client\\n    catalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\\n    items = catalog.search(\\n        collections=collections,\\n        bbox=bounds.total_bounds,\\n    ).item_collection()\\n    url_list=[i[\\'assets\\'][\\'data\\'][\\'href\\'] for i in items.to_dict()[\\'features\\']]\\n    return url_list\\n        \\ndef get_collection_bbox(collection):\\n    import geopandas as gpd\\n    import planetary_computer\\n    import pystac_client\\n\\n    catalog = pystac_client.Client.open(\\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\\n        modifier=planetary_computer.sign_inplace,\\n    )\\n    asset = catalog.get_collection(collection).assets[\"geoparquet-items\"]\\n    df = gpd.read_parquet(\\n        asset.href, storage_options=asset.extra_fields[\"table:storage_options\"]\\n    )\\n    return df[[\"assets\", \"datetime\", \"geometry\"]]\\n\\n\\ndef get_pc_token(url):\\n    from urllib.parse import urlparse\\n\\n    import requests\\n\\n    parsed_url = urlparse(url.rstrip(\"/\"))\\n    account_name = parsed_url.netloc.split(\".\")[0]\\n    path_blob = parsed_url.path.lstrip(\"/\").split(\"/\", 1)\\n    container_name = path_blob[-2]\\n    url = f\"https://planetarycomputer.microsoft.com/api/sas/v1/token/{account_name}/{container_name}\"\\n    response = requests.get(url)\\n    return response.json()\\n\\n@fused.cache(path=\"table_to_tile\")\\ndef table_to_tile(\\n    bounds,\\n    table=\"s3://fused-asset/imagery/naip/\",\\n    min_zoom=12,\\n    centorid_zoom_offset=0,\\n    use_columns=[\"geometry\"],\\n    clip=False,\\n    print_xyz=False,\\n):\\n    import fused\\n    import geopandas as gpd\\n    import pandas as pd\\n\\n    version = \"0.2.3\"\\n\\n    try:\\n        x, y, z = bounds[[\"x\", \"y\", \"z\"]].iloc[0]\\n        if print_xyz:\\n            print(x, y, z)\\n    except:\\n        z = min_zoom\\n    df = fused.get_chunks_metadata(table)\\n    if isinstance(bounds, (list, tuple, np.ndarray)):\\n        bounds=to_gdf(bounds)\\n    elif len(bounds) > 1: \\n        bounds = bounds.dissolve().reset_index(drop=True)\\n    else:\\n        bounds = bounds.reset_index(drop=True)\\n    df = df[df.intersects(bounds.geometry[0])]\\n    if z >= min_zoom:\\n        List = df[[\"file_id\", \"chunk_id\"]].values\\n        if not len(List):\\n            # No result at this area\\n            return gpd.GeoDataFrame(geometry=[])\\n        if use_columns:\\n            if \"geometry\" not in use_columns:\\n                use_columns += [\"geometry\"]\\n            rows_df = pd.concat(\\n                [\\n                    fused.get_chunk_from_table(table, fc[0], fc[1], columns=use_columns)\\n                    for fc in List\\n                ]\\n            )\\n        else:\\n            rows_df = pd.concat(\\n                [fused.get_chunk_from_table(table, fc[0], fc[1]) for fc in List]\\n            )\\n            print(\"available columns:\", list(rows_df.columns))\\n        try:\\n            df = rows_df[rows_df.intersects(bounds.geometry[0])]\\n        except:\\n            df = rows_df\\n        df.crs = bounds.crs\\n        if (\\n            z < min_zoom + centorid_zoom_offset\\n        ):  # switch to centroid for the last one zoom level before showing metadata\\n            df.geometry = df.geometry.centroid\\n        if clip:\\n            return df.clip(bounds).explode()\\n        else:\\n            return df\\n    else:\\n        df.crs = bounds.crs\\n        if clip:\\n            return df.clip(bounds).explode()\\n        else:\\n            return df\\n\\n\\ndef rasterize_geometry(\\n    geom: Dict, shape: Tuple[int, int], affine, all_touched: bool = False\\n) -> NDArray[np.uint8]:\\n    \"\"\"Return an image array with input geometries burned in.\\n\\n    Args:\\n        geom: GeoJSON geometry\\n        shape: desired 2-d shape\\n        affine: transform for the image\\n        all_touched: rasterization strategy. Defaults to False.\\n\\n    Returns:\\n        numpy array with input geometries burned in.\\n    \"\"\"\\n    from rasterio import features\\n\\n    geoms = [(geom, 1)]\\n    rv_array = features.rasterize(\\n        geoms,\\n        out_shape=shape,\\n        transform=affine,\\n        fill=0,\\n        dtype=\"uint8\",\\n        all_touched=all_touched,\\n    )\\n\\n    return rv_array.astype(bool)\\n\\n\\n@fused.cache(path=\"geom_stats\")\\ndef geom_stats(gdf, arr, output_shape=(255, 255)):\\n    import numpy as np\\n\\n    df_3857 = gdf.to_crs(3857)\\n    df_tile = df_3857.dissolve()\\n    minx, miny, maxx, maxy = df_tile.total_bounds\\n    dx = (maxx - minx) / output_shape[-1]\\n    dy = (maxy - miny) / output_shape[-2]\\n    transform = [dx, 0.0, minx, 0.0, -dy, maxy, 0.0, 0.0, 1.0]\\n    geom_masks = [\\n        rasterize_geometry(geom, arr.shape[-2:], transform) for geom in df_3857.geometry\\n    ]\\n    if isinstance(arr, np.ma.MaskedArray):\\n        arr = arr.data\\n    gdf[\"stats\"] = [np.nanmean(arr[geom_mask]) for geom_mask in geom_masks]\\n    gdf[\"count\"] = [geom_mask.sum() for geom_mask in geom_masks]\\n    return gdf\\n\\n\\ndef earth_session(cred):\\n    from job2.credentials import get_session\\n    from rasterio.session import AWSSession\\n\\n    aws_session = get_session(\\n        cred[\"env\"],\\n        earthdatalogin_username=cred[\"username\"],\\n        earthdatalogin_password=cred[\"password\"],\\n    )\\n    return AWSSession(aws_session, requester_pays=False)\\n\\n\\n@fused.cache(path=\"read_tiff\")\\ndef read_tiff(\\n    bounds,\\n    input_tiff_path,\\n    filter_list=None,\\n    output_shape=(256, 256),\\n    overview_level=None,\\n    return_colormap=False,\\n    return_transform=False,\\n    return_crs=False,\\n    return_bounds=False,\\n    return_meta=False,\\n    cred=None,\\n):\\n    import os\\n    from contextlib import ExitStack\\n\\n    import numpy as np\\n    import rasterio\\n    from rasterio.coords import BoundingBox, disjoint_bounds\\n    from rasterio.warp import Resampling, reproject\\n    from scipy.ndimage import zoom\\n\\n    version = \"0.2.0\"\\n\\n    if not cred:\\n        context = rasterio.Env()\\n    else:\\n        aws_session = earth_session(cred=cred)\\n        context = rasterio.Env(\\n            aws_session,\\n            GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\\n            GDAL_HTTP_COOKIEFILE=os.path.expanduser(\"/tmp/cookies.txt\"),\\n            GDAL_HTTP_COOKIEJAR=os.path.expanduser(\"/tmp/cookies.txt\"),\\n        )\\n    with ExitStack() as stack:\\n        stack.enter_context(context)\\n        try:\\n            with rasterio.open(input_tiff_path, OVERVIEW_LEVEL=overview_level) as src:\\n                # with rasterio.Env():\\n                if src.crs:\\n                    src_crs = src.crs\\n                else:\\n                    src_crs=4326\\n                src_bbox = bounds.to_crs(src_crs)\\n\\n                if disjoint_bounds(src.bounds, BoundingBox(*src_bbox.total_bounds)):\\n                    return None\\n\\n                window = src.window(*src_bbox.total_bounds)\\n\\n                factor = 1\\n                if output_shape is not None:\\n                    # determine a factor to downsample based on the overviews of the first band\\n                    for f in src.overviews(1):\\n                        if (window.height / f) < output_shape[0]:\\n                            break\\n                        else:\\n                            factor = f\\n\\n                    n_pixels_window = window.height * window.width / (factor ** 2)\\n                    if n_pixels_window > (output_shape[-2] * output_shape[-1] * 4):\\n                        # if we would still be reading too much data with the current\\n                        # window and factor (cutoff at 4x the desired output size)\\n                        # -> increase factor to avoid memory blowup\\n                        new_factor =  min(\\n                            window.height / (output_shape[-2]*2),\\n                            window.width / (output_shape[-1]*2)\\n                        )\\n                        factor = int(new_factor // factor) * factor\\n\\n                # # transform_bounds = rasterio.warp.transform_bounds(3857, src_crs, *bounds[\"geometry\"].bounds.iloc[0])\\n                # window = src.window(*bounds.to_crs(src_crs).total_bounds)\\n                # original_window = src.window(*bounds.to_crs(src_crs).total_bounds)\\n                # gridded_window = rasterio.windows.round_window_to_full_blocks(\\n                #     original_window, [(1, 1)]\\n                # )\\n                # window = gridded_window  # Expand window to nearest full pixels\\n                source_data = src.read(\\n                    window=window,\\n                    out_shape=(src.count, int(window.height / factor), int(window.width / factor)),\\n                    resampling=Resampling.bilinear,\\n                    boundless=True,\\n                    masked=True,\\n                )\\n\\n                window_transform = src.window_transform(window)\\n                src_transform = window_transform * window_transform.scale(\\n                    (window.width / source_data.shape[-1]),\\n                    (window.height / source_data.shape[-2])\\n                )\\n                src_dtype = src.dtypes[0]\\n                src_meta = src.meta\\n                nodata_value = src.nodatavals[0]\\n\\n                if filter_list:\\n                    mask = np.isin(source_data, filter_list, invert=True)\\n                    source_data[mask] = 0\\n                if return_colormap:\\n                    colormap = src.colormap(1)\\n        except rasterio.RasterioIOError as err:\\n            print(f\"Caught RasterioIOError {err=}, {type(err)=}\")\\n            return  # Return without data\\n        except Exception as err:\\n            print(f\"Unexpected {err=}, {type(err)=}\")\\n            raise\\n        if output_shape:\\n            # reproject\\n            bbox_web = bounds.to_crs(\"EPSG:3857\")\\n            minx, miny, maxx, maxy = bbox_web.total_bounds\\n            dx = (maxx - minx) / output_shape[-1]\\n            dy = (maxy - miny) / output_shape[-2]\\n            dst_transform = [dx, 0.0, minx, 0.0, -dy, maxy, 0.0, 0.0, 1.0]\\n            if len(source_data.shape) == 3 and source_data.shape[0] > 1:\\n                dst_shape = (source_data.shape[0], output_shape[-2], output_shape[-1])\\n            else:\\n                dst_shape = output_shape\\n            dst_crs = bbox_web.crs\\n\\n            destination_data = np.zeros(dst_shape, src_dtype)\\n            reproject(\\n                source_data,\\n                destination_data,\\n                src_transform=src_transform,\\n                src_crs=src_crs,\\n                dst_transform=dst_transform,\\n                dst_crs=dst_crs,\\n                # TODO: rather than nearest, get all the values and then get pct\\n                resampling=Resampling.bilinear,\\n            )\\n\\n            destination_mask = np.zeros(dst_shape, dtype=\"int8\")\\n            reproject(\\n                source_data.mask.astype(\"uint8\"),\\n                destination_mask,\\n                src_transform=src_transform,\\n                src_crs=src_crs,\\n                dst_transform=dst_transform,\\n                dst_crs=dst_crs,\\n                resampling=Resampling.nearest,\\n            )\\n            destination_data = np.ma.masked_array(\\n                destination_data, destination_mask\\n            )\\n        else:\\n            dst_transform = src_transform\\n            dst_crs = src_crs\\n            destination_data = source_data\\n            destination_data = np.ma.masked_array(\\n                source_data, source_data == nodata_value\\n            )\\n    if return_colormap or return_transform or return_crs or return_bounds or return_meta:\\n        ### TODO: NOT backward comp -- fix colormap / transform \\n        metadata={}\\n        if return_colormap:\\n            # todo: only set transparency to zero\\n            colormap[0] = [0, 0, 0, 0]\\n            metadata[\\'colormap\\']=colormap\\n        if return_crs:  \\n            metadata[\\'crs\\']=dst_crs\\n        if return_transform:  # Note: usually you do not need this since it can be calculated using crs=4326 and bounds\\n            metadata[\\'transform\\']=dst_transform\\n        if return_bounds:\\n            metadata[\\'bounds\\']=rasterio.transform.array_bounds(destination_data.shape[-2] , destination_data.shape[-1], dst_transform)\\n        if return_meta:\\n            metadata[\\'meta\\']=src_meta\\n        return destination_data, metadata\\n        \\n    else:\\n        return destination_data\\n\\n\\ndef get_bounds_tiff(tiff_path):\\n    import rasterio\\n    with rasterio.open(tiff_path) as src:\\n        bounds = src.bounds\\n        import shapely \\n        import geopandas as gpd\\n        bounds = gpd.GeoDataFrame({}, geometry=[shapely.box(*bounds)],crs=src.crs)\\n        bounds = bounds.to_crs(4326)\\n        return bounds\\n\\ndef gdf_to_mask_arr(gdf, shape, first_n=None):\\n    from rasterio.features import geometry_mask\\n\\n    xmin, ymin, xmax, ymax = gdf.total_bounds\\n    w = (xmax - xmin) / shape[-1]\\n    h = (ymax - ymin) / shape[-2]\\n    if first_n:\\n        geom = gdf.geometry.iloc[:first_n]\\n    else:\\n        geom = gdf.geometry\\n    return ~geometry_mask(\\n        geom,\\n        transform=(w, 0, xmin, 0, -h, ymax, 0, 0, 0),\\n        invert=True,\\n        out_shape=shape[-2:],\\n    )\\n\\n\\ndef mosaic_tiff(\\n    bounds,\\n    tiff_list,\\n    reduce_function=None,\\n    filter_list=None,\\n    output_shape=(256, 256),\\n    overview_level=None,\\n    cred=None,\\n):\\n    import numpy as np\\n\\n    if not reduce_function:\\n        reduce_function = lambda x: np.max(x, axis=0)\\n    a = []\\n    for input_tiff_path in tiff_list:\\n        if not input_tiff_path:\\n            continue\\n        new_tiff = read_tiff(\\n            bounds=bounds,\\n            input_tiff_path=input_tiff_path,\\n            filter_list=filter_list,\\n            output_shape=output_shape,\\n            overview_level=overview_level,\\n            cred=cred,\\n        )\\n        if new_tiff is not None:\\n            a.append(new_tiff)\\n\\n    if len(a) == 0:\\n        return\\n    elif len(a) == 1:\\n        data = a[0]\\n    else:\\n        data = reduce_function(a)\\n    return data  # .squeeze()[:-2,:-2]\\n\\n\\ndef arr_resample(arr, dst_shape=(512, 512), order=0):\\n    import numpy as np\\n    from scipy.ndimage import zoom\\n\\n    zoom_factors = np.array(dst_shape) / np.array(arr.shape[-2:])\\n    if len(arr.shape) == 2:\\n        return zoom(arr, zoom_factors, order=order)\\n    elif len(arr.shape) == 3:\\n        return np.asanyarray([zoom(i, zoom_factors, order=order) for i in arr])\\n\\n\\ndef arr_to_cog(\\n    arr,\\n    bounds=(-180, -90, 180, 90),\\n    crs=4326,\\n    output_path=\"output_cog.tif\",\\n    blockxsize=256,\\n    blockysize=256,\\n    overviews=[2, 4, 8, 16],\\n):\\n    import numpy as np\\n    import rasterio\\n    from rasterio.crs import CRS\\n    from rasterio.enums import Resampling\\n    from rasterio.transform import from_bounds\\n\\n    data = arr.squeeze()\\n    # Define the CRS (Coordinate Reference System)\\n    crs = CRS.from_epsg(crs)\\n\\n    # Calculate transform\\n    transform = from_bounds(*bounds, data.shape[-1], data.shape[-2])\\n    if len(data.shape) == 2:\\n        data = np.stack([data])\\n        count = 1\\n    elif len(data.shape) == 3:\\n        if data.shape[0] == 3:\\n            count = 3\\n        elif data.shape[0] == 4:\\n            count = 4\\n        else:\\n            print(data.shape)\\n            return f\"Wrong number of bands {data.shape[0]}. The options are: 1(gray) | 3 (RGB) | 4 (RGBA)\"\\n    else:\\n        return f\"wrong shape {data.shape}. Data shape options are: (ny,nx) | (1,ny,nx) | (3,ny,nx) | (4,ny,nx)\"\\n    # Write the numpy array to a Cloud-Optimized GeoTIFF file\\n    with rasterio.open(\\n        output_path,\\n        \"w\",\\n        driver=\"GTiff\",\\n        height=data.shape[-2],\\n        width=data.shape[-1],\\n        count=count,\\n        dtype=data.dtype,\\n        crs=crs,\\n        transform=transform,\\n        tiled=True,  # Enable tiling\\n        blockxsize=blockxsize,  # Set block size\\n        blockysize=blockysize,  # Set block size\\n        compress=\"deflate\",  # Use compression\\n        interleave=\"band\",  # Interleave bands\\n    ) as dst:\\n        dst.write(data)\\n        # Build overviews (pyramid layers)\\n        dst.build_overviews(overviews, Resampling.nearest)\\n        # Update tags to comply with COG standards\\n        dst.update_tags(ns=\"rio_overview\", resampling=\"nearest\")\\n    return output_path\\n\\n\\ndef arr_to_color(arr, colormap, out_dtype=\"uint8\"):\\n    import numpy as np\\n\\n    mapped_colors = np.array([colormap[val] for val in arr.flat])\\n    return (\\n        mapped_colors.reshape(arr.shape[-2:] + (len(colormap[0]),))\\n        .astype(out_dtype)\\n        .transpose(2, 0, 1)\\n    )\\n\\n\\ndef arr_to_plasma(\\n    data, min_max=(0, 255), colormap=\"plasma\", include_opacity=False, reverse=True\\n):\\n    import numpy as np\\n\\n    data = data.astype(float)\\n    if min_max:\\n        norm_data = (data - min_max[0]) / (min_max[1] - min_max[0])\\n        norm_data = np.clip(norm_data, 0, 1)\\n    else:\\n        print(f\"min_max:({round(np.nanmin(data),3)},{round(np.nanmax(data),3)})\")\\n        norm_data = (data - np.nanmin(data)) / (np.nanmax(data) - np.nanmin(data))\\n    norm_data255 = (norm_data * 255).astype(\"uint8\")\\n    if colormap:\\n        # ref: https://matplotlib.org/stable/users/explain/colors/colormaps.html\\n        from matplotlib import colormaps\\n\\n        if include_opacity:\\n            colormap = [\\n                (\\n                    np.array(\\n                        [\\n                            colormaps[colormap](i)[0],\\n                            colormaps[colormap](i)[1],\\n                            colormaps[colormap](i)[2],\\n                            i,\\n                        ]\\n                    )\\n                    * 255\\n                ).astype(\"uint8\")\\n                for i in range(257)\\n            ]\\n            if reverse:\\n                colormap = colormap[::-1]\\n            mapped_colors = np.array([colormap[val] for val in norm_data255.flat])\\n            return (\\n                mapped_colors.reshape(data.shape + (4,))\\n                .astype(\"uint8\")\\n                .transpose(2, 0, 1)\\n            )\\n        else:\\n            colormap = [\\n                (np.array(colormaps[colormap](i)[:3]) * 255).astype(\"uint8\")\\n                for i in range(256)\\n            ]\\n            if reverse:\\n                colormap = colormap[::-1]\\n            mapped_colors = np.array([colormap[val] for val in norm_data255.flat])\\n            return (\\n                mapped_colors.reshape(data.shape + (3,))\\n                .astype(\"uint8\")\\n                .transpose(2, 0, 1)\\n            )\\n    else:\\n        return norm_data255\\n\\ndef run_cmd(cmd, cwd=\".\", shell=False, communicate=False):\\n    import shlex\\n    import subprocess\\n\\n    if type(cmd) == str:\\n        cmd = shlex.split(cmd)\\n    proc = subprocess.Popen(\\n        cmd, shell=shell, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\\n    )\\n    if communicate:\\n        return proc.communicate()\\n    else:\\n        return proc\\n\\n\\ndef download_file(url, destination):\\n    import requests\\n\\n    try:\\n        response = requests.get(url)\\n        with open(destination, \"wb\") as file:\\n            file.write(response.content)\\n        return f\"File downloaded to \\'{destination}\\'.\"\\n    except requests.exceptions.RequestException as e:\\n        return f\"Error downloading file: {e}\"\\n\\n\\ndef fs_list_hls(\\n    path=\"lp-prod-protected/HLSL30.020/HLS.L30.T10SEG.2023086T184554.v2.0/\",\\n    env=\"earthdata\",\\n    earthdatalogin_username=\"\",\\n    earthdatalogin_password=\"\",\\n):\\n    import s3fs\\n    from job2.credentials import get_credentials\\n\\n    aws_session = get_credentials(\\n        env,\\n        earthdatalogin_username=earthdatalogin_username,\\n        earthdatalogin_password=earthdatalogin_password,\\n    )\\n    fs = s3fs.S3FileSystem(\\n        key=aws_session[\"aws_access_key_id\"],\\n        secret=aws_session[\"aws_secret_access_key\"],\\n        token=aws_session[\"aws_session_token\"],\\n    )\\n    return fs.ls(path)\\n\\n\\ndef get_s3_list(path, suffix=None):\\n    import s3fs\\n\\n    fs = s3fs.S3FileSystem()\\n    if suffix:\\n        return [\"s3://\" + i for i in fs.ls(path) if i[-len(suffix) :] == suffix]\\n    else:\\n        return [\"s3://\" + i for i in fs.ls(path)]\\n\\n\\n@fused.cache\\ndef get_s3_list_walk(path):\\n    #version 2 (recursive)\\n    import s3fs\\n    s3 = s3fs.S3FileSystem()\\n    # Recursively list all files in the specified S3 path\\n    flist=[]\\n    for dirpath, dirnames, filenames in s3.walk(path):\\n        for filename in filenames:\\n            flist.append(f\"s3://{dirpath}/{filename}\")\\n    return flist\\n\\n\\ndef run_async(fn, arr_args, delay=0, max_workers=32):\\n    import asyncio\\n    import concurrent.futures\\n\\n    import nest_asyncio\\n    import numpy as np\\n\\n    nest_asyncio.apply()\\n\\n    loop = asyncio.get_event_loop()\\n    pool = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\\n\\n    async def fn_async(pool, fn, *args):\\n        try:\\n            result = await loop.run_in_executor(pool, fn, *args)\\n            return result\\n        except OSError as error:\\n            print(f\"Error: {error}\")\\n            return None\\n\\n    async def fn_async_exec(fn, arr, delay):\\n        tasks = []\\n        await asyncio.sleep(delay * np.random.random())\\n        if type(arr[0]) == list or type(arr[0]) == tuple:\\n            pass\\n        else:\\n            arr = [[i] for i in arr]\\n        for i in arr:\\n            tasks.append(fn_async(pool, fn, *i))\\n        return await asyncio.gather(*tasks)\\n\\n    return loop.run_until_complete(fn_async_exec(fn, arr_args, delay))\\n\\n\\ndef run_pool(fn, arg_list, max_workers=36):\\n    import concurrent.futures\\n\\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as pool:\\n        return list(pool.map(fn, arg_list))\\n\\n\\ndef import_env(\\n    env=\"testxenv\",\\n    mnt_path=\"/mnt/cache/envs/\",\\n    packages_path=\"/lib/python3.11/site-packages\",\\n):\\n    import sys\\n\\n    sys.path.append(f\"{mnt_path}{env}{packages_path}\")\\n\\n\\ndef install_module(\\n    name,\\n    env=\"testxenv\",\\n    mnt_path=\"/mnt/cache/envs/\",\\n    packages_path=\"/lib/python3.11/site-packages\",\\n):\\n    import_env(env, mnt_path, packages_path)\\n    import os\\n    import sys\\n\\n    path = f\"{mnt_path}{env}{packages_path}\"\\n    sys.path.append(path)\\n    if not os.path.exists(path):\\n        run_cmd(f\"python -m venv  {mnt_path}{env}\", communicate=True)\\n    return run_cmd(\\n        f\"{mnt_path}{env}/bin/python -m pip install {name}\", communicate=True\\n    )\\n\\n\\ndef read_module(url, remove_strings=[]):\\n    import requests\\n\\n    content_string = requests.get(url).text\\n    if len(remove_strings) > 0:\\n        for i in remove_strings:\\n            content_string = content_string.replace(i, \"\")\\n    module = {}\\n    exec(content_string, module)\\n    return module\\n\\n\\ndef get_geo_cols(data) -> List[str]:\\n    \"\"\"Get the names of the geometry columns.\\n\\n    The first item in the result is the name of the primary geometry column. Following\\n    items are other columns with a type of GeoSeries.\\n    \"\"\"\\n    import geopandas as gpd\\n    main_col = data.geometry.name\\n    cols = [\\n        i for i in data.columns if (type(data[i]) == gpd.GeoSeries) & (i != main_col)\\n    ]\\n    return [main_col] + cols\\n\\n\\ndef crs_display(crs):\\n    \"\"\"Convert a CRS object into its human-readable EPSG code if possible.\\n\\n    If the CRS object does not have a corresponding EPSG code, the CRS object itself is\\n    returned.\\n    \"\"\"\\n    try:\\n        epsg_code = crs.to_epsg()\\n        if epsg_code is not None:\\n            return epsg_code\\n        else:\\n            return crs\\n    except Exception:\\n        return crs\\n\\n\\ndef resolve_crs(gdf,\\n                crs,\\n                verbose= False\\n):\\n    \"\"\"Reproject a GeoDataFrame to the given CRS\\n\\n    Args:\\n        gdf: The GeoDataFrame to reproject.\\n        crs: The CRS to use as destination CRS.\\n        verbose: Whether to print log statements while running. Defaults to False.\\n\\n    Returns:\\n        _description_\\n    \"\"\"\\n    if str(crs).lower() == \"utm\":\\n        if gdf.crs is None:\\n            gdf = gdf.set_crs(4326)\\n            if verbose:\\n                logger.debug(\"No crs exists on `gdf`. Assuming it\\'s WGS84 (epsg:4326).\")\\n\\n        utm_crs = gdf.estimate_utm_crs()\\n        if gdf.crs == utm_crs:\\n            if verbose:\\n                logger.debug(f\"CRS is already {crs_display(utm_crs)}.\")\\n            return gdf\\n\\n        else:\\n            if verbose:\\n                logger.debug(\\n                    f\"Converting from {crs_display(gdf.crs)} to {crs_display(utm_crs)}.\"\\n                )\\n            return gdf.to_crs(utm_crs)\\n\\n    elif (gdf.crs is not None) & (gdf.crs != crs):\\n        old_crs = gdf.crs\\n        if verbose:\\n            logger.debug(\\n                f\"Converting from {crs_display(old_crs)} to {crs_display(crs)}.\"\\n            )\\n        return gdf.to_crs(crs)\\n    elif gdf.crs is None:\\n        raise ValueError(\"gdf.crs is None and reprojection could not be performed.\")\\n    else:\\n        if verbose:\\n            logger.debug(f\"crs is already {crs_display(crs)}.\")\\n\\n        return gdf\\n\\n\\ndef infer_lonlat(columns: Sequence[str]) -> Optional[Tuple[str, str]]:\\n    \"\"\"Infer longitude and latitude columns from the column names of the DataFrame\\n\\n    Args:\\n        columns: the column names in the DataFrame\\n\\n    Returns:\\n        The pair of (longitude, latitude) column names, if found. Otherwise None.\\n    \"\"\"\\n    columns_set = set(columns)\\n    allowed_column_pairs = [\\n        (\"longitude\", \"latitude\"),\\n        (\"lon\", \"lat\"),\\n        (\"lng\", \"lat\"),\\n        (\"fused_centroid_x\", \"fused_centroid_y\"),\\n        (\"fused_centroid_x_left\", \"fused_centroid_y_left\"),\\n        (\"fused_centroid_x_right\", \"fused_centroid_x_right\"),\\n    ]\\n    for allowed_column_pair in allowed_column_pairs:\\n        if (\\n            allowed_column_pair[0] in columns_set\\n            and allowed_column_pair[1] in columns_set\\n        ):\\n            return allowed_column_pair\\n    return None\\n\\n\\ndef df_to_gdf(df, cols_lonlat=None, verbose=False):\\n    import json\\n\\n    import pyarrow as pa\\n    import shapely\\n    from geopandas.io.arrow import _arrow_to_geopandas\\n\\n    geo_metadata = {\\n        \"primary_column\": \"geometry\",\\n        \"columns\": {\"geometry\": {\"encoding\": \"WKB\", \"crs\": 4326}},\\n        \"version\": \"1.0.0-beta.1\",\\n    }\\n    arrow_geo_metadata = {b\"geo\": json.dumps(geo_metadata).encode()}\\n    if not cols_lonlat:\\n        cols_lonlat = infer_lonlat(list(df.columns))\\n        if not cols_lonlat:\\n            raise ValueError(\"no latitude and longitude columns were found.\")\\n\\n        assert (\\n            cols_lonlat[0] in df.columns\\n        ), f\"column name {cols_lonlat[0]} was not found.\"\\n        assert (\\n            cols_lonlat[1] in df.columns\\n        ), f\"column name {cols_lonlat[1]} was not found.\"\\n\\n        if verbose:\\n            logger.debug(\\n                f\"Converting {cols_lonlat} to points({cols_lonlat[0]},{cols_lonlat[0]}).\"\\n            )\\n    geoms = shapely.points(df[cols_lonlat[0]], df[cols_lonlat[1]])\\n    table = pa.Table.from_pandas(df)\\n    table = table.append_column(\"geometry\", pa.array(shapely.to_wkb(geoms)))\\n    table = table.replace_schema_metadata(arrow_geo_metadata)\\n    try:\\n        df = _arrow_to_geopandas(table)\\n    except:\\n        df = _arrow_to_geopandas(table.drop([\"__index_level_0__\"]))\\n    return df\\n\\n\\ndef to_gdf(\\n    data,\\n    crs=None,\\n    cols_lonlat=None,\\n    col_geom=\"geometry\",\\n    verbose: bool = False,\\n):\\n    \"\"\"Convert input data into a GeoPandas GeoDataFrame.\"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    import pandas as pd\\n    import mercantile\\n    \\n    # Convert xyz dict to xyz array\\n    if isinstance(data, dict) and set(data.keys()) == {\\'x\\', \\'y\\', \\'z\\'}:\\n        try:\\n            data = [int(data[\\'x\\']), int(data[\\'y\\']), int(data[\\'z\\'])]\\n        except (ValueError, TypeError):\\n            pass     \\n            \\n    \\n    if data is None or (isinstance(data, (list, tuple, np.ndarray))):\\n        \\n        data = [327, 791, 11] if data is None else data #if no data, get a tile in SF\\n        \\n        if len(data) == 3: # Handle xyz tile coordinates\\n            x, y, z = data\\n            tile = mercantile.Tile(x, y, z)\\n            bounds = mercantile.bounds(tile)\\n            gdf = gpd.GeoDataFrame(\\n                {\"x\": [x], \"y\": [y], \"z\": [z]},\\n                geometry=[shapely.box(bounds.west, bounds.south, bounds.east, bounds.north)],\\n                crs=4326\\n            )\\n            return gdf[[\\'x\\', \\'y\\', \\'z\\', \\'geometry\\']]\\n         \\n        elif len(data) == 4: # Handle the bounds case specifically        \\n            return gpd.GeoDataFrame({}, geometry=[shapely.box(*data)], crs=crs or 4326)        \\n        \\n    if cols_lonlat:\\n        if isinstance(data, pd.Series):\\n            raise ValueError(\\n                \"Cannot pass a pandas Series or a geopandas GeoSeries in conjunction \"\\n                \"with cols_lonlat.\"\\n            )\\n        gdf = df_to_gdf(data, cols_lonlat, verbose=verbose)\\n        if verbose:\\n            logger.debug(\\n                \"cols_lonlat was passed so original CRS was assumed to be EPSG:4326.\"\\n            )\\n        if crs:\\n            gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        return gdf\\n    if isinstance(data, gpd.GeoDataFrame):\\n        gdf = data\\n        if crs:\\n            gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        elif gdf.crs is None:\\n            raise ValueError(\"Please provide crs. usually crs=4326.\")\\n        return gdf\\n    elif isinstance(data, gpd.GeoSeries):\\n        gdf = gpd.GeoDataFrame(data=data)\\n        if crs:\\n            gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        elif gdf.crs is None:\\n            raise ValueError(\"Please provide crs. usually crs=4326.\")\\n        return gdf\\n    elif type(data) in (pd.DataFrame, pd.Series):\\n        if type(data) is pd.Series:\\n            data = pd.DataFrame(data)\\n            if col_geom in data.index:\\n                data = data.T\\n        if (col_geom in data.columns) and (not cols_lonlat):\\n            if type(data[col_geom][0]) == str:\\n                gdf = gpd.GeoDataFrame(\\n                    data.drop(columns=[col_geom]),\\n                    geometry=shapely.from_wkt(data[col_geom]),\\n                )\\n            else:\\n                gdf = gpd.GeoDataFrame(data)\\n            if gdf.crs is None:\\n                if crs:\\n                    gdf = gdf.set_crs(crs)\\n                else:\\n                    raise ValueError(\"Please provide crs. usually crs=4326.\")\\n            elif crs:\\n                gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        elif not cols_lonlat:\\n            cols_lonlat = infer_lonlat(data.columns)\\n            if not cols_lonlat:\\n                raise ValueError(\"no latitude and longitude columns were found.\")\\n            if crs:\\n                if verbose:\\n                    logger.debug(\\n                        f\"cols_lonlat was passed so crs was set to wgs84(4326) and {crs=} was ignored.\"\\n                    )\\n            # This is needed for Python 3.8 specifically, because otherwise creating the GeoDataFrame modifies the input DataFrame\\n            data = data.copy()\\n            gdf = df_to_gdf(data, cols_lonlat, verbose=verbose)\\n        return gdf\\n    elif (\\n        isinstance(data, shapely.geometry.base.BaseGeometry)\\n        or isinstance(data, shapely.geometry.base.BaseMultipartGeometry)\\n        or isinstance(data, shapely.geometry.base.EmptyGeometry)\\n    ):\\n        if not crs:\\n            raise ValueError(\"Please provide crs. usually crs=4326.\")\\n        return gpd.GeoDataFrame(geometry=[data], crs=crs)\\n    else:\\n        raise ValueError(\\n            f\"Cannot convert data of type {type(data)} to GeoDataFrame. Please pass a GeoDataFrame, GeoSeries, DataFrame, Series, or shapely geometry.\"\\n        )\\n\\ndef geo_buffer(\\n    data,\\n    buffer_distance=1000,\\n    utm_crs=\"utm\",\\n    dst_crs=\"original\",\\n    col_geom_buff=\"geom_buff\",\\n    verbose: bool=False,\\n):\\n    \"\"\"Buffer the geometry column in a GeoDataFrame in UTM projection.\\n\\n    Args:\\n        data: The GeoDataFrame to use as input.\\n        buffer_distance: The distance in meters to use for buffering geometries. Defaults to 1000.\\n        utm_crs: The CRS to use for the buffering operation. Geometries will be reprojected to this CRS before the buffering operation is applied. Defaults to \"utm\", which finds the most appropriate UTM zone based on the data.\\n        dst_crs: The CRS to use for the output geometries. Defaults to \"original\", which matches the CRS defined in the input data.\\n        col_geom_buff: The name of the column that should store the buffered geometries. Defaults to \"geom_buff\".\\n        verbose: Whether to print logging output. Defaults to False.\\n\\n    Returns:\\n        A new GeoDataFrame with a new column with buffered geometries.\\n    \"\"\"\\n    data = data.copy()\\n    assert data.crs not in (\\n        None,\\n        \"\",\\n    ), \"no crs was not found. use to_gdf to add crs\"\\n    if str(dst_crs).lower().replace(\"_\", \"\").replace(\" \", \"\").replace(\"-\", \"\") in [\\n        \"original\",\\n        \"originalcrs\",\\n        \"origcrs\",\\n        \"orig\",\\n        \"source\",\\n        \"sourcecrs\",\\n        \"srccrs\",\\n        \"src\",\\n    ]:\\n        dst_crs = data.crs\\n    if utm_crs:\\n        data[col_geom_buff] = resolve_crs(data, utm_crs, verbose=verbose).buffer(\\n            buffer_distance\\n        )\\n        data = data.set_geometry(col_geom_buff)\\n    else:\\n        data[col_geom_buff] = data.buffer(buffer_distance)\\n        data = data.set_geometry(col_geom_buff)\\n    if dst_crs:\\n        return resolve_crs(data, dst_crs, verbose=verbose)\\n    else:\\n        return data\\n\\n\\ndef geo_bbox(\\n    data,\\n    dst_crs=None,\\n    verbose: bool = False,\\n):\\n    \"\"\"Generate a GeoDataFrame that has the bounds of the current data frame.\\n\\n    Args:\\n        data: the GeoDataFrame to use as input.\\n        dst_crs: Destination CRS. Defaults to None.\\n        verbose: Provide extra logging output. Defaults to False.\\n\\n    Returns:\\n        A GeoDataFrame with one row, containing a geometry that has the bounds of this\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    import pyproj\\n    src_crs = data.crs\\n    if not dst_crs:\\n        return to_gdf(\\n            shapely.geometry.box(*data.total_bounds), crs=src_crs, verbose=verbose\\n        )\\n    elif str(dst_crs).lower() == \"utm\":\\n        dst_crs = data.estimate_utm_crs()\\n        logger.debug(f\"estimated dst_crs={crs_display(dst_crs)}\")\\n    transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=True)\\n    dst_bounds = transformer.transform_bounds(*data.total_bounds)\\n    return to_gdf(\\n        shapely.geometry.box(*dst_bounds, ccw=True), crs=dst_crs, verbose=verbose\\n    )\\n\\n\\ndef clip_bbox_gdfs(\\n    left,\\n    right,\\n    buffer_distance: Union[int, float] = 1000,\\n    join_type: Literal[\"left\", \"right\"] = \"left\",\\n    verbose: bool = True,\\n):\\n    \"\"\"Clip a DataFrame by a bounding box and then join to another DataFrame\\n\\n    Args:\\n        left: The left GeoDataFrame to use for the join.\\n        right: The right GeoDataFrame to use for the join.\\n        buffer_distance: The distance in meters to use for buffering before joining geometries. Defaults to 1000.\\n        join_type: _description_. Defaults to \"left\".\\n        verbose: Provide extra logging output. Defaults to False.\\n    \"\"\"\\n\\n    def fn(df1, df2, buffer_distance=buffer_distance):\\n        if buffer_distance:\\n            utm_crs = df1.estimate_utm_crs()\\n            # transform bounds to utm & buffer & then to df2_crs\\n            bbox_utm = geo_bbox(df1, dst_crs=utm_crs, verbose=verbose)\\n            bbox_utm_buff = geo_buffer(\\n                bbox_utm, buffer_distance, utm_crs=None, dst_crs=None, verbose=verbose\\n            )\\n            bbox_utm_buff_df2_crs = geo_bbox(\\n                bbox_utm_buff, dst_crs=df2.crs, verbose=verbose\\n            )\\n            return df1, df2.sjoin(bbox_utm_buff_df2_crs).drop(columns=\"index_right\")\\n        else:\\n            return df1, df2.sjoin(geo_bbox(df1, dst_crs=df2.crs, verbose=verbose)).drop(\\n                columns=\"index_right\"\\n            )\\n\\n    if join_type.lower() == \"left\":\\n        left, right = fn(left, right)\\n    elif join_type.lower() == \"right\":\\n        right, left = fn(right, left)\\n    else:\\n        assert False, \"join_type should be left or right\"\\n\\n    return left, right\\n\\n\\ndef geo_join(\\n    left,\\n    right,\\n    buffer_distance: Union[int, float, None] = None,\\n    utm_crs=\"utm\",\\n    clip_bbox=\"left\",\\n    drop_extra_geoms: bool = True,\\n    verbose: bool = False,\\n):\\n    \"\"\"Join two GeoDataFrames\\n\\n    Args:\\n        left: The left GeoDataFrame to use for the join.\\n        right: The right GeoDataFrame to use for the join.\\n        buffer_distance: The distance in meters to use for buffering before joining geometries. Defaults to None.\\n        utm_crs: The CRS used for UTM computations. Defaults to \"utm\", which infers a suitable UTM zone.\\n        clip_bbox: A bounding box used for clipping in the join step. Defaults to \"left\".\\n        drop_extra_geoms: Keep only the first geometry column. Defaults to True.\\n        verbose: Provide extra logging output. Defaults to False.\\n\\n    Returns:\\n        Joined GeoDataFrame.\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    if type(left) != gpd.GeoDataFrame:\\n        left = to_gdf(left, verbose=verbose)\\n    if type(right) != gpd.GeoDataFrame:\\n        right = to_gdf(right, verbose=verbose)\\n    left_geom_cols = get_geo_cols(left)\\n    right_geom_cols = get_geo_cols(right)\\n    if verbose:\\n        logger.debug(\\n            f\"primary geometry columns -- input left: {left_geom_cols[0]} | input right: {right_geom_cols[0]}\"\\n        )\\n    if clip_bbox:\\n        assert clip_bbox in (\\n            \"left\",\\n            \"right\",\\n        ), f\\'{clip_bbox} not in (\"left\", \"right\", \"None\").\\'\\n        left, right = clip_bbox_gdfs(\\n            left,\\n            right,\\n            buffer_distance=buffer_distance,\\n            join_type=clip_bbox,\\n            verbose=verbose,\\n        )\\n    if drop_extra_geoms:\\n        left = left.drop(columns=left_geom_cols[1:])\\n        right = right.drop(columns=right_geom_cols[1:])\\n    conflict_list = [\\n        col\\n        for col in right.columns\\n        if (col in left.columns) & (col not in (right_geom_cols[0]))\\n    ]\\n    for col in conflict_list:\\n        right[f\"{col}_right\"] = right[col]\\n    right = right.drop(columns=conflict_list)\\n    if not drop_extra_geoms:\\n        right[right_geom_cols[0] + \"_right\"] = right[right_geom_cols[0]]\\n    if buffer_distance:\\n        if not utm_crs:\\n            utm_crs = left.crs\\n            if verbose:\\n                logger.debug(\\n                    f\"No crs transform before applying buffer (left crs:{crs_display(utm_crs)}).\"\\n                )\\n        df_joined = geo_buffer(\\n            left,\\n            buffer_distance,\\n            utm_crs=utm_crs,\\n            dst_crs=right.crs,\\n            col_geom_buff=\"_fused_geom_buff_\",\\n            verbose=verbose,\\n        ).sjoin(right)\\n        df_joined = df_joined.set_geometry(left_geom_cols[0]).drop(\\n            columns=[\"_fused_geom_buff_\"]\\n        )\\n        if left.crs == right.crs:\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n        else:\\n            df_joined = df_joined.to_crs(left.crs)\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n    else:\\n        if left.crs != right.crs:\\n            df_joined = left.to_crs(right.crs).sjoin(right).to_crs(left.crs)\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n        else:\\n            df_joined = left.sjoin(right)\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n\\n\\ndef geo_distance(\\n    left,\\n    right,\\n    search_distance: Union[int, float] = 1000,\\n    utm_crs=\"utm\",\\n    clip_bbox=\"left\",\\n    col_distance: str = \"distance\",\\n    k_nearest: int = 1,\\n    cols_agg: Sequence[str] = (\"fused_index\",),\\n    cols_right: Sequence[str] = (),\\n    drop_extra_geoms: bool = True,\\n    verbose: bool = False,\\n):\\n    \"\"\"Compute the distance from rows in one dataframe to another.\\n\\n    First this performs an geo_join, and then finds the nearest rows in right to left.\\n\\n    Args:\\n        left: left GeoDataFrame\\n        right: right GeoDataFrame\\n        search_distance: Distance in meters used for buffering in the geo_join step. Defaults to 1000.\\n        utm_crs: The CRS used for UTM computations. Defaults to \"utm\", which infers a suitable UTM zone.\\n        clip_bbox: A bounding box used for clipping in the join step. Defaults to \"left\".\\n        col_distance: The column named for saving the output of the distance step. Defaults to \"distance\".\\n        k_nearest: The number of nearest values to keep.. Defaults to 1.\\n        cols_agg: Columns used for the aggregation before the join. Defaults to (\"fused_index\",).\\n        cols_right: Columns from the right dataframe to keep. Defaults to ().\\n        drop_extra_geoms: Keep only the first geometry column. Defaults to True.\\n        verbose: Provide extra logging output. Defaults to False.\\n\\n    Returns:\\n        _description_\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    if type(left) != gpd.GeoDataFrame:\\n        left = to_gdf(left, verbose=verbose)\\n    if type(right) != gpd.GeoDataFrame:\\n        right = to_gdf(right, verbose=verbose)\\n    left_geom_cols = get_geo_cols(left)\\n    right_geom_cols = get_geo_cols(right)\\n    cols_right = list(cols_right)\\n    if drop_extra_geoms:\\n        left = left.drop(columns=left_geom_cols[1:])\\n        right = right.drop(columns=right_geom_cols[1:])\\n        cols_right = [i for i in cols_right if i in right.columns]\\n    if right_geom_cols[0] not in cols_right:\\n        cols_right += [right_geom_cols[0]]\\n    if cols_agg:\\n        cols_agg = list(cols_agg)\\n        all_cols = set(list(left.columns) + list(cols_right))\\n        assert (\\n            len(set(cols_agg) - all_cols) == 0\\n        ), f\"{cols_agg=} not in the table. Please pass valid list or cols_agg=None if you want distance over entire datafame\"\\n    dfj = geo_join(\\n        left,\\n        right[cols_right],\\n        buffer_distance=search_distance,\\n        utm_crs=utm_crs,\\n        clip_bbox=clip_bbox,\\n        drop_extra_geoms=False,\\n        verbose=verbose,\\n    )\\n    if len(dfj) > 0:\\n        utm_crs = dfj[left_geom_cols[0]].estimate_utm_crs()\\n        dfj[col_distance] = (\\n            dfj[[left_geom_cols[0]]]\\n            .to_crs(utm_crs)\\n            .distance(dfj[f\"{right_geom_cols[0]}_right\"].to_crs(utm_crs))\\n        )\\n    else:\\n        dfj[col_distance] = 0\\n        if verbose:\\n            print(\"geo_join returned empty dataframe.\")\\n\\n    if drop_extra_geoms:\\n        dfj = dfj.drop(columns=get_geo_cols(dfj)[1:])\\n    if cols_agg:\\n        dfj = dfj.sort_values(col_distance).groupby(cols_agg).head(k_nearest)\\n    else:\\n        dfj = dfj.sort_values(col_distance).head(k_nearest)\\n    return dfj\\n\\n\\ndef geo_samples(\\n    n_samples: int, min_x: float, max_x: float, min_y: float, max_y: float\\n):\\n    \"\"\"\\n    Generate sample points in a bounding box, uniformly.\\n\\n    Args:\\n        n_samples: Number of sample points to generate\\n        min_x: Minimum x coordinate\\n        max_x: Maximum x coordinate\\n        min_y: Minimum y coordinate\\n        max_y: Maximum y coordinate\\n\\n    Returns:\\n        A GeoDataFrame with point geometry.\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    import random\\n    points = [\\n        (random.uniform(min_x, max_x), random.uniform(min_y, max_y))\\n        for _ in range(n_samples)\\n    ]\\n    return to_gdf(pd.DataFrame(points, columns=[\"lng\", \"lat\"]))[[\"geometry\"]]\\n\\n\\ndef bbox_stac_items(bounds, table):\\n    import fused\\n    import geopandas as gpd\\n    import pandas as pd\\n    import pyarrow.parquet as pq\\n    import shapely\\n\\n    df = fused.get_chunks_metadata(table)\\n    df = df[df.intersects(bounds)]\\n    if len(df) > 10 or len(df) == 0:\\n        return None  # fault\\n    matching_images = []\\n    for idx, row in df.iterrows():\\n        file_url = table + row[\"file_id\"] + \".parquet\"\\n        chunk_table = pq.ParquetFile(file_url).read_row_group(row[\"chunk_id\"])\\n        chunk_gdf = gpd.GeoDataFrame(chunk_table.to_pandas())\\n        if \"geometry\" in chunk_gdf:\\n            chunk_gdf.geometry = shapely.from_wkb(chunk_gdf[\"geometry\"])\\n        matching_images.append(chunk_gdf)\\n\\n    ret_gdf = pd.concat(matching_images)\\n    ret_gdf = ret_gdf[ret_gdf.intersects(bounds)]\\n    return ret_gdf\\n\\n\\n# todo: switch to read_tiff with requester_pays option\\ndef read_tiff_naip(\\n    bounds, input_tiff_path, crs, buffer_degree, output_shape, resample_order=0\\n):\\n    from io import BytesIO\\n\\n    import rasterio\\n    from rasterio.session import AWSSession\\n    from scipy.ndimage import zoom\\n\\n    out_buf = BytesIO()\\n    with rasterio.Env(AWSSession(requester_pays=True)):\\n        with rasterio.open(input_tiff_path) as src:\\n            if buffer_degree != 0:\\n                bounds.geometry = bounds.geometry.buffer(buffer_degree)\\n            bbox_projected = bounds.to_crs(crs)\\n            window = src.window(*bbox_projected.total_bounds)\\n            data = src.read(window=window, boundless=True)\\n            zoom_factors = np.array(output_shape) / np.array(data[0].shape)\\n            rgb = np.array(\\n                [zoom(arr, zoom_factors, order=resample_order) for arr in data]\\n            )\\n        return rgb\\n\\n\\ndef image_server_bbox(\\n    image_url,\\n    bounds=None,\\n    time=None,\\n    size=512,\\n    bbox_crs=4326,\\n    image_crs=3857,\\n    image_format=\"tiff\",\\n    return_colormap=False,\\n):\\n    if bbox_crs and bbox_crs != image_crs:\\n        import geopandas as gpd\\n        import shapely\\n\\n        gdf = gpd.GeoDataFrame(geometry=[shapely.box(*bounds)], crs=bbox_crs).to_crs(\\n            image_crs\\n        )\\n        print(gdf)\\n        minx, miny, maxx, maxy = gdf.total_bounds\\n    else:\\n        minx, miny, maxx, maxy = list(bounds)\\n    image_url = image_url.strip(\"/\")\\n    url_template = f\"{image_url}?f=image\"\\n    url_template += f\"&bounds={minx},{miny},{maxx},{maxy}\"\\n    if time:\\n        url_template += f\"&time={time}\"\\n    if image_crs:\\n        url_template += f\"&imageSR={image_crs}&bboxSR={image_crs}\"\\n    if size:\\n        url_template += f\"&size={size},{size*(miny-maxy)/(minx-maxx)}\"\\n    url_template += f\"&format={image_format}\"\\n    return url_to_arr(url_template, return_colormap=return_colormap)\\n\\n\\ndef arr_to_stats(arr, gdf, type=\"nominal\"):\\n    import numpy as np\\n\\n    minx, miny, maxx, maxy = gdf.total_bounds\\n    dx = (maxx - minx) / arr.shape[-1]\\n    dy = (maxy - miny) / arr.shape[-2]\\n    transform = [dx, 0.0, minx, 0.0, -dy, maxy, 0.0, 0.0, 1.0]\\n    geom_masks = [\\n        rasterize_geometry(geom, arr.shape[-2:], transform) for geom in gdf.geometry\\n    ]\\n    if type == \"nominal\":\\n        counts_per_mask = []\\n        for geom_mask in geom_masks:\\n            masked_arr = arr[geom_mask]\\n            unique_values, counts = np.unique(masked_arr, return_counts=True)\\n            counts_dict = {value: count for value, count in zip(unique_values, counts)}\\n            counts_per_mask.append(counts_dict)\\n        gdf[\"stats\"] = counts_per_mask\\n        return gdf\\n    elif type == \"numerical\":\\n        stats_per_mask = []\\n        for geom_mask in geom_masks:\\n            masked_arr = arr[geom_mask]\\n            stats_per_mask.append(\\n                {\\n                    \"min\": np.nanmin(masked_arr),\\n                    \"max\": np.nanmax(masked_arr),\\n                    \"mean\": np.nanmean(masked_arr),\\n                    \"median\": np.nanmedian(masked_arr),\\n                    \"std\": np.nanstd(masked_arr),\\n                }\\n            )\\n        gdf[\"stats\"] = stats_per_mask\\n        return gdf\\n    else:\\n        raise ValueError(\\n            f\\'{type} is not supported. Type options are \"nominal\" and \"numerical\"\\'\\n        )\\n\\n\\ndef ask_openai(prompt, openai_api_key, role=\"user\", model=\"gpt-4-turbo-preview\"):\\n    # ref: https://github.com/openai/openai-python\\n    # ref: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\\n    from openai import OpenAI\\n\\n    client = OpenAI(\\n        api_key=openai_api_key,\\n    )\\n    messages = [\\n        {\\n            \"role\": role,\\n            \"content\": prompt,\\n        }\\n    ]\\n    chat_completion = client.chat.completions.create(\\n        messages=messages,\\n        model=model,\\n    )\\n    return [i.message.content for i in chat_completion.choices]\\n\\n\\ndef ee_initialize(service_account_name=\"\", key_path=\"\"):\\n    \"\"\"\\n    Authenticate with Google Earth Engine using service account credentials.\\n\\n    This function initializes the Earth Engine API by authenticating with the\\n    provided service account credentials. The authenticated session allows for\\n    accessing and manipulating data within Google Earth Engine.\\n\\n    Args:\\n        service_account_name (str): The email address of the service account.\\n        key_path (str): The path to the private key file for the service account.\\n\\n    See documentation: https://docs.fused.io/basics/in/gee/\\n\\n    Example:\\n        ee_initialize(\\'your-service-account@your-project.iam.gserviceaccount.com\\', \\'path/to/your-private-key.json\\')\\n    \"\"\"\\n    import ee\\n\\n    credentials = ee.ServiceAccountCredentials(service_account_name, key_path)\\n    ee.Initialize(\\n        opt_url=\"https://earthengine-highvolume.googleapis.com\", credentials=credentials\\n    )\\n\\n\\n@fused.cache\\ndef run_pool_tiffs(bounds, df_tiffs, output_shape):\\n    import numpy as np\\n\\n    columns = df_tiffs.columns\\n\\n    @fused.cache\\n    def fn_read_tiff(tiff_url, bounds=bounds, output_shape=output_shape):\\n        read_tiff = fused.load(\\n            \"https://github.com/fusedio/udfs/tree/3c4bc47/public/common/\"\\n        ).utils.read_tiff\\n        return read_tiff(bounds, tiff_url, output_shape=output_shape)\\n\\n    tiff_list = []\\n    for band in columns:\\n        for i in range(len(df_tiffs)):\\n            tiff_list.append(df_tiffs[band].iloc[i])\\n\\n    arrs_tmp = run_pool(fn_read_tiff, tiff_list)\\n    arrs_out = np.stack(arrs_tmp)\\n    arrs_out = arrs_out.reshape(\\n        len(columns), len(df_tiffs), output_shape[0], output_shape[1]\\n    )\\n    return arrs_out\\n\\n\\ndef search_pc_catalog(\\n    bounds,\\n    time_of_interest,\\n    query={\"eo:cloud_cover\": {\"lt\": 5}},\\n    collection=\"sentinel-2-l2a\",\\n):\\n    import planetary_computer\\n    import pystac_client\\n\\n    # Instantiate PC client\\n    catalog = pystac_client.Client.open(\\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\\n        modifier=planetary_computer.sign_inplace,\\n    )\\n\\n    # Search catalog\\n    items = catalog.search(\\n        collections=[collection],\\n        bbox=bounds.total_bounds,\\n        datetime=time_of_interest,\\n        query=query,\\n    ).item_collection()\\n\\n    if len(items) == 0:\\n        print(f\"empty for {time_of_interest}\")\\n        return\\n\\n    return items\\n\\n\\ndef create_tiffs_catalog(stac_items, band_list):\\n    import pandas as pd\\n\\n    input_paths = []\\n    for selected_item in stac_items:\\n        max_key_length = len(max(selected_item.assets, key=len))\\n        input_paths.append([selected_item.assets[band].href for band in band_list])\\n    return pd.DataFrame(input_paths, columns=band_list)\\n\\ndef create_chunk_metadata(df, chunk_size=10_000):\\n    total_rows = len(df)\\n    num_chunks = (total_rows + chunk_size - 1) // chunk_size\\n    meta = []\\n    for idx in range(num_chunks):\\n        chunk = df.iloc[idx * chunk_size:min((idx + 1) * chunk_size, total_rows)]\\n        meta.append({\\n            \"bbox_minx\": chunk.lng.min(),\\n            \"bbox_maxx\": chunk.lng.max(),\\n            \"bbox_miny\": chunk.lat.min(),\\n            \"bbox_maxy\": chunk.lat.max(),\\n            \"chunk_id\": idx,\\n            \"num_rows\": len(chunk)  # Additional stat showing number of rows in chunk\\n        })\\n    import pandas as pd\\n    return pd.DataFrame.from_records(meta)\\n\\ndef chunked_tiff_to_points(tiff_path, i: int = 0, x_chunks: int = 2, y_chunks: int = 2):\\n    import numpy as np\\n    import pandas as pd\\n    import rasterio\\n\\n    with rasterio.open(tiff_path) as src:\\n        x_list, y_list = shape_transform_to_xycoor(src.shape[-2:], src.transform)\\n        x_slice, y_slice = get_chunk_slices_from_shape(\\n            src.shape[-2:], x_chunks, y_chunks, i\\n        )\\n        x_list = x_list[x_slice]\\n        y_list = y_list[y_slice]\\n        X, Y = np.meshgrid(x_list, y_list)\\n        arr = src.read(window=(y_slice, x_slice)).flatten()\\n        df = pd.DataFrame(X.flatten(), columns=[\"lng\"])\\n        df[\"lat\"] = Y.flatten()\\n        df[\"data\"] = arr\\n    return df\\n\\n\\ndef shape_transform_to_xycoor(shape, transform):\\n    import numpy as np\\n\\n    n_y = shape[-2]\\n    n_x = shape[-1]\\n    w, _, x, _, h, y, _, _, _ = transform\\n    x_list = np.arange(x + w / 2, x + n_x * w + w / 2, w)[:n_x]\\n    y_list = np.arange(y + h / 2, y + n_y * h + h / 2, h)[:n_y]\\n    return x_list, y_list\\n\\n\\ndef get_chunk_slices_from_shape(array_shape, x_chunks, y_chunks, i):\\n    # Unpack the dimensions of the array shape\\n    rows, cols = array_shape\\n\\n    # Calculate the size of each chunk\\n    chunk_height = rows // y_chunks\\n    chunk_width = cols // x_chunks\\n\\n    # Calculate row and column index for the i-th chunk\\n    row_start = (i // x_chunks) * chunk_height\\n    col_start = (i % x_chunks) * chunk_width\\n\\n    # Create slice objects for the i-th chunk\\n    y_slice = slice(row_start, row_start + chunk_height)\\n    x_slice = slice(col_start, col_start + chunk_width)\\n\\n    return x_slice, y_slice\\n\\ndef arr_to_latlng(arr, bounds):\\n    import numpy as np\\n    import pandas as pd\\n    from rasterio.transform import from_bounds\\n    transform = from_bounds(*bounds, arr.shape[-1], arr.shape[-2])\\n    x_list, y_list = shape_transform_to_xycoor(arr.shape[-2:], transform)\\n    X, Y = np.meshgrid(x_list, y_list)\\n    df = pd.DataFrame(X.flatten(), columns=[\"lng\"])\\n    df[\"lat\"] = Y.flatten()\\n    df[\"data\"] = arr.flatten()\\n    return df\\n\\n# @fused.cache\\ndef df_to_h3(df, res, latlng_cols=(\"lat\", \"lng\"), ordered=False):\\n    qr = f\"\"\"\\n            SELECT h3_latlng_to_cell({latlng_cols[0]}, {latlng_cols[1]}, {res}) AS hex, ARRAY_AGG(data) as agg_data\\n            FROM df\\n            group by 1\\n        \"\"\"\\n    if ordered:\\n        qr+=\"  order by 1\"\\n    con = duckdb_connect()\\n    return con.query(qr).df()\\n\\ndef arr_to_h3(arr, bounds, res, ordered=False):\\n    return df_to_h3(arr_to_latlng(arr, bounds), res=res, ordered=ordered)\\n\\ndef duckdb_connect(home_directory=\\'/tmp/\\'):\\n    import duckdb \\n    con = duckdb.connect()\\n    con.sql(\\n    f\"\"\"SET home_directory=\\'{home_directory}\\';\\n    INSTALL h3 FROM community;\\n    LOAD h3;\\n    install \\'httpfs\\';\\n    load \\'httpfs\\';\\n    INSTALL spatial;\\n    LOAD spatial;\\n    \"\"\")\\n    print(\"duckdb version:\", duckdb.__version__)\\n    return con\\n\\n\\n# @fused.cache\\ndef run_query(query, return_arrow=False):\\n    con = duckdb_connect()\\n    if return_arrow:\\n        return con.sql(query).fetch_arrow_table()\\n    else:\\n        return con.sql(query).df()\\n\\n\\ndef ds_to_tile(ds, variable, bounds, na_values=0, cols_lonlat=(\\'x\\', \\'y\\')):\\n    da = ds[variable]\\n    x_slice, y_slice = bbox_to_xy_slice(\\n        bounds.total_bounds, ds.rio.shape, ds.rio.transform()\\n    )\\n    window = bbox_to_window(bounds.total_bounds, ds.rio.shape, ds.rio.transform())\\n    py0 = py1 = px0 = px1 = 0\\n    if window.col_off < 0:\\n        px0 = -window.col_off\\n    if window.col_off + window.width > da.shape[-2]:\\n        px1 = window.col_off + window.width - da.shape[-2]\\n    if window.row_off < 0:\\n        py0 = -window.row_off\\n    if window.row_off + window.height > da.shape[-1]:\\n        py1 = window.row_off + window.height - da.shape[-1]\\n    # data = da.isel(x=x_slice, y=y_slice, time=0).fillna(0)\\n    data = da.isel({cols_lonlat[0]:x_slice, cols_lonlat[1]:y_slice}).fillna(0)\\n    data = data.pad({cols_lonlat[0]:(px0, px1), cols_lonlat[1]:(py0, py1)},\\n                     mode=\"constant\", constant_values=na_values\\n    )\\n    return data\\n\\n# @fused.cache(cache_max_age=\\'24h\\')\\ndef tiff_to_xyz(input_tiff, output_xyz, xoff, x_block_size, yoff, y_block_size):\\n    cmd = f\"gdal_translate -srcwin {xoff*x_block_size} {yoff*y_block_size} {x_block_size} {y_block_size} -of XYZ {input_tiff} {output_xyz}\"\\n    r = run_cmd(cmd, communicate=True)\\n    # assert r[1] == b\"\"   # cehck if there is an error (r[1] != b\"\")\\n    return r\\n\\ndef xy_transform(df, src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\", \\n                         cols_src_xy=[\\'x\\',\\'y\\'], cols_dst_xy=[\\'lng\\', \\'lat\\']):\\n    from pyproj import Proj, transform\\n\\n    # Define the source (original CRS) and target (EPSG:4326) CRSs\\n    source_proj = Proj(init=src_crs)  # Replace with original CRS\\n    target_proj = Proj(init=dst_crs)  # EPSG:4326 for WGS84\\n\\n    # Transform the X, Y coordinates to EPSG:4326 (WGS84)\\n    x_coords = df[cols_src_xy[0]].values\\n    y_coords = df[cols_src_xy[1]].values\\n    lon, lat = transform(source_proj, target_proj, x_coords, y_coords)\\n    df[cols_dst_xy[0]] = lon\\n    df[cols_dst_xy[1]] = lat\\n    return df\\n\\ndef bbox_to_xy_slice(bounds, shape, transform):\\n    import rasterio\\n    from affine import Affine\\n\\n    if transform[4] < 0:  # if pixel_height is negative\\n        original_window = rasterio.windows.from_bounds(*bounds, transform=transform)\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        y_slice, x_slice = gridded_window.toslices()\\n        return x_slice, y_slice\\n    else:  # if pixel_height is not negative\\n        original_window = rasterio.windows.from_bounds(\\n            *bounds,\\n            transform=Affine(\\n                transform[0],\\n                transform[1],\\n                transform[2],\\n                transform[3],\\n                -transform[4],\\n                -transform[5],\\n            ),\\n        )\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        y_slice, x_slice = gridded_window.toslices()\\n        y_slice = slice(shape[0] - y_slice.stop, shape[0] - y_slice.start + 0)\\n        return x_slice, y_slice\\n\\n\\ndef bbox_to_window(bounds, shape, transform):\\n    import rasterio\\n    from affine import Affine\\n\\n    if transform[4] < 0:  # if pixel_height is negative\\n        original_window = rasterio.windows.from_bounds(*bounds, transform=transform)\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        return gridded_window\\n    else:  # if pixel_height is not negative\\n        original_window = rasterio.windows.from_bounds(\\n            *bounds,\\n            transform=Affine(\\n                transform[0],\\n                transform[1],\\n                transform[2],\\n                transform[3],\\n                -transform[4],\\n                -transform[5],\\n            ),\\n        )\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        return gridded_window\\n\\n\\ndef bounds_to_gdf(bounds_list, crs=4326):\\n    import geopandas as gpd\\n    import shapely\\n\\n    box = shapely.box(*bounds_list)\\n    return gpd.GeoDataFrame(geometry=[box], crs=crs)\\n\\n\\ndef mercantile_polyfill(geom, zooms=[15], compact=True, k=None):\\n    import geopandas as gpd\\n    import mercantile\\n    import shapely\\n\\n    gdf = to_gdf(geom , crs = 4326)\\n    geometry = gdf.geometry[0]\\n\\n    tile_list = list(mercantile.tiles(*geometry.bounds, zooms=zooms))\\n    gdf_tiles = gpd.GeoDataFrame(\\n        tile_list,\\n        geometry=[shapely.box(*mercantile.bounds(i)) for i in tile_list],\\n        crs=4326,\\n    )\\n    gdf_tiles_intersecting = gdf_tiles[gdf_tiles.intersects(geometry)]\\n\\n    if k:\\n        temp_list = gdf_tiles_intersecting.apply(\\n            lambda row: mercantile.Tile(row.x, row.y, row.z), 1\\n        )\\n        clip_list = mercantile_kring_list(temp_list, k)\\n        if not compact:\\n            gdf = gpd.GeoDataFrame(\\n                clip_list,\\n                geometry=[shapely.box(*mercantile.bounds(i)) for i in clip_list],\\n                crs=4326,\\n            )\\n            return gdf\\n    else:\\n        if not compact:\\n            return gdf_tiles_intersecting\\n        clip_list = gdf_tiles_intersecting.apply(\\n            lambda row: mercantile.Tile(row.x, row.y, row.z), 1\\n        )\\n    simple_list = mercantile.simplify(clip_list)\\n    gdf = gpd.GeoDataFrame(\\n        simple_list,\\n        geometry=[shapely.box(*mercantile.bounds(i)) for i in simple_list],\\n        crs=4326,\\n    )\\n    return gdf  # .reset_index(drop=True)\\n\\n\\ndef mercantile_kring(tile, k):\\n    # ToDo: Remove invalid tiles in the globe boundries (e.g. negative values)\\n    import mercantile\\n\\n    result = []\\n    for x in range(tile.x - k, tile.x + k + 1):\\n        for y in range(tile.y - k, tile.y + k + 1):\\n            result.append(mercantile.Tile(x, y, tile.z))\\n    return result\\n\\n\\ndef mercantile_kring_list(tiles, k):\\n    a = []\\n    for tile in tiles:\\n        a.extend(mercantile_kring(tile, k))\\n    return list(set(a))\\n\\n\\ndef make_tiles_gdf(bounds, zoom=14, k=0, compact=0):\\n    import shapely\\n\\n    df_tiles = mercantile_polyfill(\\n        shapely.box(*bounds), zooms=[zoom], compact=compact, k=k\\n    )\\n    df_tiles[\"bounds\"] = df_tiles[\"geometry\"].apply(lambda x: x.bounds, 1)\\n    return df_tiles\\n\\n\\ndef get_masked_array(gdf_aoi, arr_aoi):\\n    import numpy as np\\n    from rasterio.features import geometry_mask\\n    from rasterio.transform import from_bounds\\n\\n    transform = from_bounds(*gdf_aoi.total_bounds, arr_aoi.shape[-1], arr_aoi.shape[-2])\\n    geom_mask = geometry_mask(\\n        gdf_aoi.geometry,\\n        transform=transform,\\n        invert=True,\\n        out_shape=arr_aoi.shape[-2:],\\n    )\\n    masked_value = np.ma.MaskedArray(data=arr_aoi, mask=[~geom_mask])\\n    return masked_value\\n\\n\\ndef get_da(path, coarsen_factor=1, variable_index=0, xy_cols=[\"longitude\", \"latitude\"]):\\n    # Load data\\n    import xarray\\n\\n    path = fused.download(path, path.split(\\'/\\')[-1]) \\n    ds = xarray.open_dataset(path, engine=\\'h5netcdf\\')\\n    try:\\n        var = list(ds.data_vars)[variable_index]\\n        print(var)\\n        if coarsen_factor > 1:\\n            da = ds.coarsen(\\n                {xy_cols[0]: coarsen_factor, xy_cols[1]: coarsen_factor},\\n                boundary=\"trim\",\\n            ).max()[var]\\n        else:\\n            da = ds[var]\\n        print(\"done\")\\n        return da\\n    except Exception as e:\\n        print(e)\\n        ValueError()\\n\\n\\ndef get_da_bounds(da, xy_cols=(\"longitude\", \"latitude\"), pixel_position=\\'center\\'):\\n    x_list = da[xy_cols[0]].values\\n    y_list = da[xy_cols[1]].values\\n    if pixel_position==\\'center\\':\\n        pixel_width = x_list[1] - x_list[0]\\n        pixel_height = y_list[1] - y_list[0]\\n        minx = x_list[0] - pixel_width / 2\\n        miny = y_list[-1] + pixel_height / 2\\n        maxx = x_list[-1] + pixel_width / 2\\n        maxy = y_list[0] - pixel_height / 2\\n        return (minx, miny, maxx, maxy)\\n    else:\\n        return (x_list[0], y_list[-1], x_list[-1], y_list[0])\\n        \\n        \\ndef da_fit_to_resolution(da, target_shape):\\n    import numpy as np\\n    dims = da.dims\\n    new_coords = {\\n        dim: np.linspace(da[dim].min(), da[dim].max(), target_shape[i])\\n        for i, dim in enumerate(dims)\\n    }\\n    return da.interp(new_coords)    \\n\\n\\ndef clip_arr(arr, bounds_aoi, bounds_total=(-180, -90, 180, 90)):\\n    # ToDo: fix antimeridian issue by spliting and merging arr around lng=360\\n    from rasterio.transform import from_bounds\\n\\n    transform = from_bounds(*bounds_total, arr.shape[-1], arr.shape[-2])\\n    if bounds_total[2] > 180 and bounds_total[0] >= 0:\\n        print(\"Normalized longitude for bounds_aoi to (0,360) to match bounds_total\")\\n        bounds_aoi = (\\n            (bounds_aoi[0] + 360) % 360,\\n            bounds_aoi[1],\\n            (bounds_aoi[2] + 360) % 360,\\n            bounds_aoi[3],\\n        )\\n    x_slice, y_slice = bbox_to_xy_slice(bounds_aoi, arr.shape, transform)\\n    arr_aoi = arr[y_slice, x_slice]\\n    if bounds_total[1] > bounds_total[3]:\\n        if len(arr_aoi.shape) == 3:\\n            arr_aoi = arr_aoi[:, ::-1]\\n        else:\\n            arr_aoi = arr_aoi[::-1]\\n    return arr_aoi\\n\\n\\ndef visualize(\\n    data,\\n    mask: float | np.ndarray = None,\\n    min: float = 0,\\n    max: float = 1,\\n    opacity: float = 1,\\n    colormap = None,\\n):\\n    \"\"\"Convert objects into visualization tiles.\"\"\"\\n    import xarray as xr\\n    import palettable\\n    from matplotlib.colors import LinearSegmentedColormap\\n    from matplotlib.colors import Normalize   \\n    \\n    if data is None:\\n        return\\n    \\n    if colormap is None:\\n        # Set a default colormap\\n        colormap = palettable.colorbrewer.sequential.Greys_9_r\\n        cm = colormap.mpl_colormap\\n    elif isinstance(colormap, palettable.palette.Palette):\\n        cm = colormap.mpl_colormap\\n    elif isinstance(colormap, (list, tuple)):\\n        cm = LinearSegmentedColormap.from_list(\\'custom\\', colormap)\\n    else:\\n        print(\\'visualize: no type match for colormap\\')\\n\\n    if isinstance(data, xr.DataArray):\\n        # Convert from an Xarray DataArray to a Numpy ND Array\\n        data = data.values\\n\\n    if isinstance(data, np.ndarray):\\n\\n        if isinstance(data, np.ma.MaskedArray):\\n            boolean_mask = data.mask\\n            if mask is None:\\n                mask = boolean_mask\\n            else:\\n                # Combine the two masks.\\n                mask = mask * boolean_mask\\n        \\n        norm_data = Normalize(vmin=min, vmax=max, clip=False)(data)\\n        mapped_colors = cm(norm_data)\\n        if isinstance(mask, (float, np.ndarray)):\\n            mapped_colors[:,:,3] = mask * opacity\\n        else:\\n            mapped_colors[:,:,3] = opacity\\n        \\n        # Convert to unsigned 8-bit ints for visualization.\\n        vis_dtype = np.uint8\\n        max_color_value = np.iinfo(vis_dtype).max\\n        norm_data255 = (mapped_colors * max_color_value).astype(vis_dtype)\\n        \\n        # Reshape array to 4 x nRow x nCol.\\n        shaped = norm_data255.transpose(2,0,1)\\n    \\n        return shaped\\n    else:\\n        print(\\'visualize: data instance type not recognized\\')\\n\\n    \\nclass AsyncRunner:\\n    \\'\\'\\'\\n    ## Usage example:\\n    async def fn(n): return n**2\\n    runner = AsyncRunner(fn, range(10))\\n    runner.get_result_now()\\n    \\'\\'\\'\\n    def __init__(self, func, args_list, delay_second=0, verbose=True):\\n        import asyncio\\n        if isinstance(args_list, pd.DataFrame):\\n            self.args_list=args_list.T.to_dict().values()\\n        elif isinstance(args_list, list) or isinstance(args_list, range):     \\n            self.args_list=args_list\\n        else:\\n            raise ValueError(\\'args_list need to be list, pd.DataFrame, or range\\')\\n        self.func = func\\n        self.verbose = verbose\\n        self.delay_second = delay_second\\n        self.loop = asyncio.get_running_loop()\\n        self.run_async()\\n    \\n    def create_task(self, args):\\n        import time\\n        import json\\n        time.sleep(self.delay_second)\\n        if type(args)==str:\\n            args=json.loads(args)\\n        if isinstance(args, dict):\\n            task = self.loop.create_task(self.func(**args))\\n        else:\\n            task = self.loop.create_task(self.func(args))\\n        task.set_name(json.dumps(args))\\n        return task\\n        \\n    def run_async(self):\\n        tasks = []\\n        for args in self.args_list:\\n            tasks.append(self.create_task(args))\\n        self.tasks=tasks\\n    \\n    def is_done(self):\\n        return [task.done() for task in self.tasks]\\n    \\n    def get_task_result(self, r):\\n        if r.done():\\n            import pandas as pd\\n            try:\\n                return r.result()\\n            except Exception as e:\\n                return str(e)\\n        else:\\n            return \\'pending\\'\\n        \\n    def get_result_now(self, retry=True):\\n        if retry:\\n            self.retry()\\n        if self.verbose:\\n            print(f\"{sum(self.is_done())} out of {len(self.is_done())} are done!\")\\n        import json\\n        import pandas as pd\\n        df = pd.DataFrame([json.loads(task.get_name()) for task in self.tasks])\\n        df[\\'result\\']= [self.get_task_result(task) for task in self.tasks]\\n        def fn(r):\\n            if type(r)==str:\\n                if r==\\'pending\\':\\n                    return \\'running\\'\\n                else:\\n                    return \\'faild\\'\\n            else:\\n                return \\'done\\'\\n        df[\\'status\\']=df[\\'result\\'].map(fn)\\n        return df            \\n    \\n    def retry(self):\\n        def _retry_task(task, verbose):\\n            if task.done():\\n                task_exception = task.exception()\\n                if task_exception:\\n                    if verbose: print(task_exception)\\n                    return self.create_task(task.get_name()) \\n                else:\\n                    return task\\n            else:\\n                return task            \\n        self.tasks = [_retry_task(task, self.verbose) for task in self.tasks]\\n    \\n    async def get_result_async(self):\\n        import asyncio\\n        return await asyncio.gather(*self.tasks)\\n\\n    def __repr__(self):\\n        if self.verbose:\\n            print(f\\'tasks_done={self.is_done()}\\')\\n        if (sum(self.is_done())/len(self.is_done()))==1:\\n            return f\"done!\"\\n        else:\\n            return \"running...\"\\n    \\nclass PoolRunner:\\n    \\'\\'\\'\\n    ## Usage example:\\n    def fn(n): return n**2\\n    runner = PoolRunner(fn, range(10))\\n    runner.get_result_now()\\n    runner.get_result_all()\\n    \\'\\'\\'\\n    def __init__(self, func, args_list, delay_second=0.01, verbose=True, max_retry=3):\\n        import asyncio\\n        import pandas as pd\\n        import concurrent.futures\\n        if isinstance(args_list, pd.DataFrame):\\n            self.args_list=args_list.T.to_dict().values()\\n        elif isinstance(args_list, list) or isinstance(args_list, range):     \\n            self.args_list=args_list\\n        else:\\n            raise ValueError(\\'args_list need to be list, pd.DataFrame, or range\\')\\n        self.func = func\\n        self.delay_second = delay_second\\n        self.verbose = verbose\\n        self.max_retry=max_retry\\n        self.pool = concurrent.futures.ThreadPoolExecutor(max_workers=1024)\\n        self.run_pool()\\n        self.result=[]\\n\\n    def create_task(self, args, n_retry):\\n        import time\\n        time.sleep(self.delay_second)\\n        if isinstance(args, dict):\\n            task = self.pool.submit(self.func, **args)\\n        else:\\n            task = self.pool.submit(self.func, args)\\n        return [task, args, n_retry]\\n        \\n    def run_pool(self):\\n        tasks = []\\n        for args in self.args_list:\\n            tasks.append(self.create_task(args, n_retry=0))\\n        self.tasks=tasks\\n    \\n    def is_success(self):\\n        return [task[0].done() for task in self.tasks]\\n    \\n    def get_task_result(self, task):\\n        if task[0].done() or task[2]>self.max_retry:\\n            try:\\n                return task[0].result()\\n            except Exception as e:\\n                if task[2]<self.max_retry: \\n                    return str(e)\\n                else:\\n                    return f\\'Exceeded max_retry ({task[2]-1}|{self.max_retry}): \\'+str(e)\\n        else:\\n            # if task[2]<self.max_retry: \\n                return f\\'pending\\'\\n            # else:\\n            #     return f\\'Exceeded max_retry\\'\\n        \\n    def get_result_now(self, retry=True, sleep_second=1):\\n        if retry:\\n            self.retry()\\n        else:\\n            import time\\n            time.sleep(sleep_second)\\n        if self.verbose:\\n            n1=sum(self.is_success())\\n            n2=len(self.is_success())\\n            print(f\"\\\\r{n1/n2*100:.1f}% ({n1}|{n2}) complete\", end=\\'\\\\n\\')\\n        import json\\n        import pandas as pd\\n        df = pd.DataFrame([task[1] for task in self.tasks])\\n        self.result=[self.get_task_result(task) for task in self.tasks]\\n        df[\\'result\\']=self.result\\n        def fn(r):\\n            if type(r)==str:\\n                if str.startswith(r, \\'Exceeded max_retry\\'):\\n                    return \\'error\\'\\n                else:#elif r==\\'pending\\':\\n                    return \\'running\\'#\\'error_retry\\'\\n            else:\\n                return \\'success\\'\\n        df[\\'status\\']=df[\\'result\\'].map(fn)\\n        return df            \\n    \\n    def retry(self):\\n        def _retry_task(task, verbose):\\n            if task[0].done():\\n                task_exception = task[0].exception()\\n                if task_exception:\\n                    if verbose: \\n                        print(task_exception)\\n                    if (task[2]<self.max_retry):\\n                        if verbose: \\n                            print(f\\'Retry {task[2]+1}|{self.max_retry} for {task[1]}.\\')\\n                        return self.create_task(task[1], n_retry=task[2]+1) \\n                    else:\\n                        if verbose: \\n                            print(f\\'Exceeded Max retry {self.max_retry} for {task[1]}.\\')\\n                        return [task[0], task[1], task[2]+1]\\n                else:\\n                    return task\\n            else:\\n                return task            \\n        self.tasks = [_retry_task(task, self.verbose) for task in self.tasks]\\n    \\n    def get_result_all(self, timeout=120):\\n        import time\\n        for i in range(timeout):\\n            df=self.get_result_now(retry=True)\\n            # if (df.status==\\'success\\').mean()==1:\\n            if (df.status==\\'running\\').mean()==0:\\n                break\\n            else:\\n                time.sleep(1)\\n        if self.verbose:\\n            print(f\"Done!\")\\n        return df\\n\\n    def get_error(self, sleep_second=3):\\n        df=self.get_result_now(retry=False, sleep_second=sleep_second)\\n        if self.verbose:\\n            print(\\'Status Summary:\\', df.status.value_counts().to_dict())\\n            error_mask=df.status==\\'error\\'\\n            if sum(error_mask)==0:\\n                print(\\'No error.\\')\\n            else:\\n                df_error = df[error_mask]\\n                for i in range(len(df_error)): \\n                    print(f\\'\\\\nROW: {i} | ERROR {\"=\"*30}\\')\\n                    for col in df_error.columns:  \\n                        print(f\\'{str(col).upper()}: {df_error.iloc[i][col]}\\')\\n        return df\\n    def get_concat(self, sleep_second=0, verbose=None):\\n        if verbose != None:\\n            self.verbose=verbose\\n        import pandas as pd\\n        df = self.get_error(sleep_second=sleep_second)\\n        mask=df.status==\\'success\\'\\n        if mask.sum()==0:\\n            return \\n        else:\\n            results=[i for i in df[mask].result if i is not None]\\n            if self.verbose:\\n                print(f\"{100*mask.mean().round(3)} percent success.\")\\n                if len(results)!=mask.sum():\\n                    print(f\"Warnning: {mask.sum()-len(results)}/{mask.sum()} are None values.\")\\n        return pd.concat(results)\\n    def __repr__(self):\\n        if self.verbose:\\n            print(f\\'tasks_success={self.is_success()}\\')\\n        if (sum(self.is_success())/len(self.is_success()))==1:\\n            return f\"success!\"\\n        else:\\n            return \"running...\"\\n            \\n@fused.cache\\ndef get_parquet_stats(path):\\n    import pyarrow.parquet as pq\\n    import pandas as pd\\n    # Load Parquet file\\n    parquet_file = pq.ParquetFile(path)\\n    \\n    # List to store the metadata for each row group\\n    stats_list = []\\n\\n    # Iterate through row groups\\n    for i in range(parquet_file.num_row_groups):\\n        row_group = parquet_file.metadata.row_group(i)\\n        \\n        # Dictionary to store row group\\'s statistics\\n        row_stats = {\\'row_group\\': i, \\'num_rows\\': row_group.num_rows}\\n        \\n        # Iterate through columns and gather statistics\\n        for j in range(row_group.num_columns):\\n            column = row_group.column(j)\\n            stats = column.statistics\\n            \\n            if stats:\\n                col_name = column.path_in_schema\\n                row_stats[f\"{col_name}_min\"] = stats.min\\n                row_stats[f\"{col_name}_max\"] = stats.max\\n                row_stats[f\"{col_name}_null_count\"] = stats.null_count\\n        \\n        # Append the row group stats to the list\\n        stats_list.append(row_stats)\\n    \\n    # Convert the list to a DataFrame\\n    df_stats = pd.DataFrame(stats_list)\\n    \\n    return df_stats\\n\\n@fused.cache\\ndef get_row_groups(key, value, file_path):\\n    version=\\'1.0\\'\\n    df = get_parquet_stats(file_path)[[\\'row_group\\', key+\\'_min\\', key+\\'_max\\']]\\n    con = duckdb_connect()\\n    df = con.query(f\\'select * from df where {value} between {key}_min and {key}_max\\').df()\\n    return df.row_group.values\\n\\ndef read_row_groups(file_path, chunk_ids, columns=None):\\n    import pyarrow.parquet as pq   \\n    table=pq.ParquetFile(file_path)   \\n    if columns:\\n        return table.read_row_groups(chunk_ids, columns=columns).to_pandas()   \\n    else: \\n        print(\\'available columns:\\', table.schema.names)\\n        return table.read_row_groups(chunk_ids).to_pandas() \\n\\n\\ndef tiff_bbox(url):\\n    import rasterio\\n    import shapely\\n    import geopandas as gpd\\n    with rasterio.open(url) as dataset:\\n        gpd.GeoDataFrame(geometry=[shapely.box(*dataset.bounds)],crs=dataset.crs)\\n        return list(dataset.bounds)\\n    \\ndef s3_to_https(path):\\n    arr = path[5:].split(\\'/\\')\\n    out = \\'https://\\'+arr[0]+\\'.s3.amazonaws.com/\\'+\\'/\\'.join(arr[1:])\\n    return out\\n\\ndef get_ip():\\n    import socket\\n    hostname=socket.gethostname()\\n    IPAddr=socket.gethostbyname(hostname)\\n    return IPAddr\\n    \\n\\ndef scipy_voronoi(gdf):\\n    \"\"\"\\n    Scipy based Voronoi function. Built because fused version at time is on geopandas 0.14.4 which \\n    doesnt\\' have `gdf.geometry.voronoi_polygons()`\\n    Probably not the most optimised funciton but it gets the job done. \\n    Irrelevant once we move to geopandas 1.0+\\n    \"\"\"\\n    from shapely.geometry import Polygon, Point\\n    from scipy.spatial import Voronoi\\n\\n    points = np.array([(geom.x, geom.y) for geom in gdf.geometry])\\n    vor = Voronoi(points)\\n    \\n    polygons = []\\n    for region in vor.regions:\\n        if not region or -1 in region:  # Ignore regions with open boundaries\\n            continue\\n        \\n        # Get the vertices for the region and construct a polygon\\n        polygon = Polygon([vor.vertices[i] for i in region])\\n        polygons.append(polygon)\\n\\n    voronoi_gdf = gpd.GeoDataFrame(geometry=polygons, crs=gdf.crs)\\n    return voronoi_gdf\\n\\n\\n\\n\\n\\ndef estimate_zoom(bounds, target_num_tiles=1):\\n    \"\"\"\\n    Estimate the zoom level for a given bounding box.\\n\\n    This method returns the zoom level at which a tile exists that, potentially\\n    shifted slightly, fully covers the bounding box.\\n    \\n    Args:\\n        bounds: A list of 4 coordinates (minx, miny, maxx, maxy), a\\n                GeoDataFrame or Shapely geometry, or a mercantile Tile.\\n        target_num_tiles: Target number of tiles to cover the bounds (default=1).\\n                      If 1, finds the zoom where a single tile covers the bounds.\\n                      If >1, estimates zoom to achieve approximately this many tiles.\\n    \\n    Returns:\\n        The estimated zoom level (0-20).\\n    \"\"\"\\n    from fused._optional_deps import (\\n        GPD_GEODATAFRAME,\\n        HAS_GEOPANDAS,\\n        HAS_MERCANTILE,\\n        HAS_SHAPELY,\\n        MERCANTILE_TILE,\\n        SHAPELY_GEOMETRY,\\n    )\\n\\n    # Process input bounds to get standard format\\n    if HAS_GEOPANDAS and isinstance(bounds, GPD_GEODATAFRAME):\\n        bounds = bounds.total_bounds\\n    elif HAS_SHAPELY and isinstance(bounds, SHAPELY_GEOMETRY):\\n        bounds = bounds.bounds\\n    elif HAS_MERCANTILE and isinstance(bounds, MERCANTILE_TILE):\\n        return bounds.z\\n    elif not isinstance(bounds, list):\\n        raise TypeError(f\"Invalid bounds type: {type(bounds)}\")\\n\\n    if not HAS_MERCANTILE:\\n        raise ImportError(\"This function requires the mercantile package.\")\\n    \\n    import mercantile\\n    import math\\n    \\n\\n    if target_num_tiles == 1:\\n        minx, miny, maxx, maxy = bounds\\n        centroid = (minx + maxx) / 2, (miny + maxy) / 2\\n        width = (maxx - minx) - 1e-11\\n        height = (maxy - miny) - 1e-11\\n        \\n        for z in range(20, 0, -1):\\n            tile = mercantile.tile(*centroid, zoom=z)\\n            west, south, east, north = mercantile.bounds(tile)\\n            if width <= (east - west) and height <= (north - south):\\n                break\\n        return z\\n    \\n\\n    else:\\n        minx, miny, maxx, maxy = bounds\\n        miny = max(miny,-89.9999993) #there is a bug in the mercentile that adds an epsilon to lat lngs and causes issue\\n        maxy = min(maxy,89.9999993) #there is a bug in the mercentile that adds an epsilon to lat lngs and causes issue\\n        max_zoom = 20\\n        x_min, y_min, _ = mercantile.tile(minx, maxy, max_zoom)\\n        x_max, y_max, _ = mercantile.tile(maxx, miny, max_zoom)\\n        delta_x = x_max - x_min + 1\\n        delta_y = y_max - y_min + 1\\n\\n        zoom = math.log2(math.sqrt(target_num_tiles) / max(delta_x, delta_y)) + max_zoom\\n        zoom = int(math.floor(zoom)) \\n        current_num_tiles = len(mercantile_polyfill(bounds, zooms=[zoom], compact=False))\\n        if current_num_tiles>=target_num_tiles:\\n            return zoom\\n        else:\\n            return zoom+1\\n\\n\\ndef get_tiles(\\n    bounds=None, target_num_tiles=1, zoom=None, max_tile_recursion=6, as_gdf=True\\n):\\n    bounds = to_gdf(bounds)\\n    import mercantile\\n\\n    if zoom is not None:\\n        print(\"zoom is provided; target_num_tiles will be ignored.\")\\n        target_num_tiles = None\\n\\n    if target_num_tiles is not None and target_num_tiles < 1:\\n        raise ValueError(\"target_num_tiles should be more than zero.\")\\n\\n    if target_num_tiles == 1:\\n        \\n        tile = mercantile.bounding_tile(*bounds.total_bounds)\\n        print(to_gdf((0,0,0)))\\n        gdf = to_gdf((tile.x, tile.y, tile.z))\\n    else:\\n        zoom_level = (\\n            zoom\\n            if zoom is not None\\n            else estimate_zoom(bounds, target_num_tiles=target_num_tiles)\\n        )\\n        base_zoom = estimate_zoom(bounds, target_num_tiles=1)\\n        if zoom_level > (base_zoom + max_tile_recursion + 1):\\n            zoom_level = base_zoom + max_tile_recursion + 1\\n            if zoom:\\n                print(\\n                    f\"Warning: Maximum number of tiles is reached ({zoom=} > {base_zoom+max_tile_recursion+1=} tiles). Increase {max_tile_recursion=} to allow for deeper tile recursion\"\\n                )\\n            else:\\n                print(\\n                    f\"Warning: Maximum number of tiles is reached ({target_num_tiles} > {4**max_tile_recursion-1} tiles). Increase {max_tile_recursion=} to allow for deeper tile recursion\"\\n                )\\n\\n        gdf = mercantile_polyfill(bounds, zooms=[zoom_level], compact=False)\\n        print(f\"Generated {len(gdf)} tiles at zoom level {zoom_level}\")\\n\\n    return gdf if as_gdf else gdf[[\"x\", \"y\", \"z\"]].values\\n\\ndef get_utm_epsg(geometry):\\n    utm_zone = int((geometry.centroid.x + 180) / 6) + 1\\n    return 32600 + utm_zone if geometry.centroid.y >= 0 else 32700 + utm_zone  # 326XX for Northern Hemisphere, 327XX for Southern\\n\\n\\ndef add_utm_area(gdf, utm_col=\\'utm_epsg\\', utm_area_col=\\'utm_area_sqm\\'):\\n    import geopandas as gpd\\n\\n    # Step 1: Compute UTM zones\\n    gdf[utm_col] = gdf.geometry.apply(get_utm_epsg)\\n\\n    # Step 2: Compute areas in batches while preserving order\\n    areas_dict = {}\\n\\n    for utm_zone, group in gdf.groupby(utm_col, group_keys=False):\\n        utm_crs = f\"EPSG:{utm_zone}\"\\n        reprojected = group.to_crs(utm_crs)  # Reproject all geometries in batch\\n        areas_dict.update(dict(zip(group.index, reprojected.area)))  # Store areas by index\\n\\n    # Step 3: Assign areas back to original gdf order\\n    gdf[utm_area_col] = gdf.index.map(areas_dict)\\n    return gdf\\n\\n\\ndef run_submit_with_defaults(udf_token: str, cache_length: str = \"9999d\", default_params_token: Optional[str] = None):\\n    \"\"\"\\n    Uses fused.submit() to run a UDF over:\\n    - A UDF that returns a pd.DataFrame of test arguments (`default_params_token`)\\n    - Or default params (expectes udf.utils.submit_default_params to return a pd.DataFrame)\\n    \"\"\"\\n    \\n    # Assume people know what they\\'re doing \\n    try:\\n        # arg_token is a UDF that returns a pd.DataFrame of test arguments\\n        arg_list = fused.run(default_params_token)\\n\\n        if \\'bounds\\' in arg_list.columns:\\n            # This is a hacky workaround for now as we can\\'t pass np.float bounds to `fused.run(udf, bounds) so need to convert them to float\\n            # but fused.run() returns bounds as `np.float` for whatever reason\\n            arg_list[\\'bounds\\'] = arg_list[\\'bounds\\'].apply(lambda bounds_list: [float(x) for x in bounds_list])\\n            \\n        print(f\"Loaded default params from UDF {default_params_token}... Running UDF over these\")\\n    except Exception as e:\\n        print(f\"Couldn\\'t load UDF {udf_token} with arg_token {default_params_token}, trying to load default params...\")\\n        \\n        try:\\n            udf = fused.load(udf_token)\\n            \\n            # Assume we have a funciton called \\'submit_default_params` inside the main UDF which returns a pd.DataFrame of test arguments\\n            # TODO: edit this to directly use `udf.submit_default_params()` once we remove utils\\n            if hasattr(udf.utils, \"submit_default_params\"):\\n                print(\"Found default params for UDF, using them...\")\\n                arg_list = udf.utils.submit_default_params()\\n            else:\\n                raise ValueError(\"No default params found for UDF, can\\'t run this UDF\")\\n\\n        except Exception as e:\\n            raise ValueError(\"Couldn\\'t load UDF, can\\'t run this UDF. Try with another UDF\")\\n        \\n        #TODO: Add support for using the default view state\\n\\n    return fused.submit(\\n        udf_token,\\n        arg_list,\\n        cache_max_age=cache_length,\\n        wait_on_results=True,\\n    )\\n\\ndef test_udf(udf_token: str, cache_length: str = \"9999d\", arg_token: Optional[str] = None):\\n    \"\"\"\\n    Testing a UDF:\\n    1. Does it run and return successful result for all its default parameters?\\n    2. Are the results identical to the cached results?\\n\\n    Returns:\\n    - all_passing: True if the UDF runs and returns successful result for all its default parameters\\n    - all_equal: True if the results are identical to the cached results\\n    - prev_run: Cached UDF output\\n    - current_run: New UDF output\\n    \"\"\"\\n    import pickle\\n\\n    cached_run = run_submit_with_defaults(udf_token, cache_length, arg_token)\\n    current_run = run_submit_with_defaults(udf_token, \"0s\", arg_token)\\n\\n    # Check if results are valid\\n    all_passing = (current_run[\"status\"] == \"success\").all()\\n    # Check if result matches cached result\\n    all_equal = pickle.dumps(cached_run) == pickle.dumps(current_run)\\n    return (bool(all_passing), all_equal, cached_run, current_run)\\n\\n  \\ndef save_to_agent(\\n    agent_json_path: str, udf: AnyBaseUdf, agent_name: str, udf_name: str, mcp_metadata: dict[str, Any], overwrite: bool = True,\\n):\\n    \"\"\"\\n    Save UDF to agent of udf_ai directory\\n    Args:\\n        agent_json_path (str): Absolute path to the agent.json file\\n        agent_name (str): Name of the agent\\n        udf (AnyBaseUdf): UDF to save\\n        udf_name (str): Name of the UDF\\n        mcp_metadata (dict[str, Any]): MCP metadata\\n        overwrite (bool): If True, overwrites any existing UDF directory with current `udf`\\n    \"\"\"\\n    # load agent.json\\n    if os.path.exists(agent_json_path):\\n        agent_json = json.load(open(agent_json_path))\\n    else:\\n        agent_json = {\"agents\": []}\\n    repo_dir = os.path.dirname(agent_json_path)\\n\\n    # save udf to repo\\n    udf.metadata = {}\\n    udf.name = udf_name\\n    if not mcp_metadata.get(\"description\") or not mcp_metadata.get(\"parameters\"):\\n        raise ValueError(\"mcp_metadata must have description and parameters\")\\n    udf.metadata[\"fused:mcp\"] = mcp_metadata\\n    udf.to_directory(f\"{repo_dir}/udfs/{udf_name}\", overwrite=overwrite)\\n\\n    if agent_name in [agent[\"name\"] for agent in agent_json[\"agents\"]]:\\n        for agent in agent_json[\"agents\"]:\\n            if agent[\"name\"] == agent_name:\\n                # Only append udf_name if it doesn\\'t already exist in the agent\\'s udfs list\\n                if udf_name not in agent[\"udfs\"]:\\n                    agent[\"udfs\"].append(udf_name)\\n                break\\n    else:\\n        agent_json[\"agents\"].append({\"name\": agent_name, \"udfs\": [udf_name]})\\n\\n    # save agent.json\\n    json.dump(agent_json, open(agent_json_path, \"w\"), indent=4)\\n\\ndef generate_local_mcp_config(config_path: str, agents_list: list[str], repo_path: str, uv_path: str = \\'uv\\', script_path: str = \\'main.py\\'):\\n    \"\"\"\\n    Generate MCP configuration file based on list of agents from the udf_ai directory\\n    Args:\\n        config_path (str): Absolute path to the MCP configuration file.\\n        agents_list (list[str]): List of agent names to be included in the configuration.\\n        repo_path (str): Absolute path to the locally cloned udf_ai repo directory.\\n        uv_path (str): Path to `uv`. Defaults to `uv` but might require your local path to `uv`.\\n        script_path (str): Path to the script to run. Defaults to `run.py`.\\n    \"\"\"\\n    if not os.path.exists(repo_path):\\n        raise ValueError(f\"Repository path {repo_path} does not exist\")\\n\\n    # load agent.json containing agent to udfs mapping\\n    agent_json = json.load(open(f\"{repo_path}/agents.json\", \"rt\"))\\n\\n    # create config json for all agents\\n    config_json = {\"mcpServers\": {}}\\n\\n    for agent_name in agents_list:\\n        agent = next(\\n            (agent for agent in agent_json[\"agents\"] if agent[\"name\"] == agent_name),\\n            None,\\n        )\\n        if not agent or not agent[\"udfs\"]:\\n            raise ValueError(f\"No UDFs found for agent {agent_name}\")\\n\\n        agent_config = {\\n            \"command\": uv_path,\\n            \"args\": [\\n                \"run\",\\n                \"--directory\",\\n                f\"{repo_path}\",\\n                f\"{script_path}\",\\n                \"--runtime=local\",\\n                f\"--udf-names={\\',\\'.join(agent[\\'udfs\\'])}\",\\n                f\"--name={agent_name}\",\\n            ],\\n        }\\n        config_json[\"mcpServers\"][agent_name] = agent_config\\n\\n    # save config json\\n    json.dump(config_json, open(config_path, \"w\"), indent=4)\\n\\n' source_file='utils.py'\n",
      "module_name='utils' source_code='# To use these functions, add the following command in your UDF:\\n# `common = fused.utils.common`\\nfrom __future__ import annotations\\nimport fused\\nimport pandas as pd\\nimport numpy as np\\nimport json\\nimport os\\nfrom numpy.typing import NDArray\\nfrom typing import Dict, List, Literal, Optional, Sequence, Tuple, Union, Any\\nfrom loguru import logger\\nfrom fused.api.api import AnyBaseUdf\\n\\ndef to_pickle(obj):\\n    \"\"\"Encode an object to a pickle byte stream and store in DataFrame.\"\"\"\\n    import pickle\\n    import pandas as pd\\n    return pd.DataFrame(\\n        {\"data_type\": [type(obj).__name__], \"data_content\": [pickle.dumps(obj)]}\\n    )\\n\\ndef from_pickle(df):\\n    \"\"\"Decode an object from a DataFrame containing pickle byte stream.\"\"\"\\n    import pickle\\n    return pickle.loads(df[\"data_content\"].iloc[0])\\n\\ndef df_summary(df, description=\"\", n_head=5, n_tail=5, n_sample=5, n_unique=100, add_details=True):\\n    val = description+\"\\\\n\\\\n\"\\n    val += \"These are stats for df (pd.DataFrame):\\\\n\"\\n    val += f\"{list(df.columns)=} \\\\n\\\\n\"\\n    val += f\"{df.isnull().sum()=} \\\\n\\\\n\"\\n    val += f\"{df.describe().to_json()=} \\\\n\\\\n\"\\n    val += f\"{df.head(n_head).to_json()=} \\\\n\\\\n\"\\n    val += f\"{df.tail(n_tail).to_json()=} \\\\n\\\\n\"\\n    if len(df) > n_sample:\\n        val += f\"{df.sample(n_sample).to_json()=} \\\\n\\\\n\"\\n    if add_details:\\n        if len(df) <= n_unique:\\n            val += f\"{df.to_json()} \\\\n\\\\n\"\\n        else:\\n            for c in df.columns:\\n                value_counts = df[c].value_counts()\\n                df[c].value_counts().head()\\n                val += f\"df[{c}].value_counts()\\\\n{value_counts} \\\\n\\\\n\"\\n                val += f\"{df[c].unique()[:n_unique]} \\\\n\\\\n\"\\n    return val\\n\\ndef get_diff_text(text1: str, text2: str, as_html: bool=True, only_diff: bool=False) -> str:\\n    import difflib\\n    import html\\n    \\n    diff = difflib.ndiff(text1.splitlines(keepends=True), text2.splitlines(keepends=True))\\n    processed_diff = []\\n    \\n    if not as_html:\\n        for line in diff:\\n            if line.startswith(\"+\"):\\n                processed_diff.append(f\"ADD: {line}\")  # Additions\\n            elif line.startswith(\"-\"):\\n                processed_diff.append(f\"DEL: {line}\")  # Deletions\\n            else:\\n                if not only_diff:\\n                    processed_diff.append(f\"  {line}\")  # Unchanged lines\\n        return \"\\\\n\".join(processed_diff)            \\n    \\n    for line in diff:\\n        escaped_line = html.escape(line)  # Escape HTML to preserve special characters\\n        \\n        if line.startswith(\"+\"):\\n            processed_diff.append(f\"<span style=\\'color:green; line-height:normal;\\'> {escaped_line} </span><br>\")  # Green for additions\\n        elif line.startswith(\"-\"):\\n            processed_diff.append(f\"<span style=\\'color:red; line-height:normal;\\'> {escaped_line} </span><br>\")  # Red for deletions\\n        else:\\n            if not only_diff:\\n                processed_diff.append(f\"<span style=\\'color:gray; line-height:normal;\\'> {escaped_line} </span><br>\")  # Gray for unchanged lines\\n    \\n    # HTML structure with a dropdown for selecting background color\\n    html_output = \"\"\"\\n    <div>\\n        <label for=\"backgroundColor\" style=\"color:gray;\">Choose Background Color: </label>\\n        <select id=\"backgroundColor\" onchange=\"document.getElementById(\\'diff-container\\').style.backgroundColor = this.value;\">\\n            <option value=\"#111111\">Dark Gray</option>\\n            <option value=\"#f0f0f0\">Light Gray</option>\\n            <option value=\"#ffffff\">White</option>\\n            <option value=\"#e0f7fa\">Cyan</option>\\n            <option value=\"#ffebee\">Pink</option>\\n            <option value=\"#c8e6c9\">Green</option>\\n        </select>\\n    </div>\\n    <div id=\"diff-container\" style=\"background-color:#111111; padding:10px; font-family:monospace; white-space:pre; line-height:normal;\">\\n        {}</div>\\n    \"\"\".format(\"\".join(processed_diff))\\n    return html_output\\n\\n\\ndef json_path_from_secret(var=\\'gcs_fused\\'):\\n    import json\\n    import tempfile\\n    with tempfile.NamedTemporaryFile(suffix=\".json\", delete=False, mode=\"w\") as tmp_file:\\n        json.dump(json.loads(fused.secrets[var]), tmp_file)\\n        tmp_file_path = tmp_file.name\\n    return tmp_file_path\\n    \\ndef gcs_credentials_from_secret(var=\\'gcs_fused\\'):\\n    \"\"\"\\n    example: \\n    \"\"\"\\n    import gcsfs\\n    import json\\n    from google.oauth2 import service_account\\n    \\n    gcs_secret = json.loads(fused.secrets[var])\\n    credentials = service_account.Credentials.from_service_account_info(gcs_secret)\\n    return credentials\\n\\n@fused.cache\\ndef simplify_gdf(gdf, pct=1, args=\\'-o force -clean\\'):\\n    #ref https://github.com/mbloch/mapshaper/blob/master/REFERENCE.md\\n    import geopandas as gpd\\n    import uuid\\n    import json\\n    file_path = f\\'/mount/cache_data/temp/{uuid.uuid4()}.json\\'\\n    print(file_path)\\n    with open(file_path, \"w\") as f:\\n        json.dump(gdf.__geo_interface__, f)\\n    # print(fused.utils.common.run_cmd(\\'npm install -g mapshaper\\',communicate=True)[1].decode())    \\n    print(run_cmd(f\\'/mount/npm/bin/mapshaper {file_path} -simplify {pct}% {args} -o {file_path.replace(\".json\",\"2.json\")}\\',communicate=True))\\n    return gpd.read_file(file_path.replace(\".json\",\"2.json\"))\\n\\ndef html_to_obj(html_str):\\n    from fastapi import Response\\n    return Response(html_str.encode(\\'utf-8\\'), media_type=\"text/html\")\\n\\ndef pydeck_to_obj(map, as_string=False):    \\n        html_str = map.to_html(as_string=True)\\n        if as_string:\\n            return html_str\\n        else: \\n            return html_to_obj(html_str)\\n\\ndef altair_to_obj(chart, as_string=False):    \\n    import io\\n    html_buffer = io.StringIO()\\n    chart.save(html_buffer, format=\"html\")\\n    html_str = html_buffer.getvalue()\\n    html_buffer.close()\\n    if as_string:\\n        return html_str\\n    else: \\n        return html_to_obj(html_str)\\n\\ndef html_params(html_template, params={}, **kw):\\n    \\'\\'\\'Exampl: html_params(\\'<div>{{v1}}{{v2}}</div>\\',{\\'v1\\':\\'hello \\'}, v2=\\'world!\\')\\'\\'\\'\\n    from jinja2 import Template\\n    template = Template(html_template)\\n    return template.render(params, **kw)\\n\\n\\ndef url_redirect(url):\\n    from fastapi import Response\\n    return Response(f\\'<meta http-equiv=\"refresh\" content=\"0; url={url}\">\\'.encode(\\'utf-8\\'), media_type=\"text/html\")\\n    \\n@fused.cache\\ndef read_shapefile(url):\\n    import geopandas as gpd\\n    try:\\n        return gpd.read_file(url)\\n    except:\\n        path = fused.download(url, url)\\n        name=str(path).split(\\'/\\')[-1].split(\\'.\\')[0]\\n        print(name)\\n        try:\\n            return gpd.read_file(f\"zip://{path}!{name}/{name}.shp\")\\n        except:\\n            return gpd.read_file(f\"zip://{path}!{name}.shp\")\\n\\ndef point_to_line(start_lon=-122.4194, start_lat=37.7749, end_lon=-122.2712, end_lat=37.8044, value=0):\\n    import geopandas as gpd\\n    import shapely\\n    from shapely.geometry import LineString    \\n    line = LineString([(start_lon, start_lat), (end_lon, end_lat)])    \\n    return gpd.GeoDataFrame({\"value\": [value]}, geometry=[line], crs=4326)\\n\\ndef interpolate_points(line_shape, point_distance):\\n    points = []\\n    num_points = int(line_shape.length // point_distance)\\n    for i in range(num_points + 1):\\n        point = line_shape.interpolate(i * point_distance)\\n        points.append(point)\\n    return points\\n\\ndef pointify(lines, point_distance, segment_col=\\'segment_id\\'):\\n    import geopandas as gpd\\n    crs_orig = lines.crs\\n    crs_utm=lines.estimate_utm_crs()\\n    lines = lines.to_crs(crs_utm)\\n    from shapely.geometry import Point\\n    import pandas as pd\\n    points_list = []\\n    for _, row in lines.iterrows():\\n        line = row[\\'geometry\\']\\n        points = interpolate_points(line, point_distance)\\n        for point in points:\\n            points_list.append({segment_col: row[segment_col], \\'geometry\\': point})\\n    print(\\'number of points:\\', len(points_list))\\n    points_gdf = gpd.GeoDataFrame(points_list, crs=lines.crs)\\n    return points_gdf.to_crs(crs_orig)\\n\\ndef chunkify(lst, chunk_size):\\n    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\\n\\n@fused.cache\\ndef get_meta_chunk_datestr(base_path, total_row_groups=52, start_year=2020, end_year=2024, n_chunks_row_group=2, n_chunks_datestr=90,):\\n    import pandas as pd\\n    date_list = pd.date_range(start=f\\'{start_year}-01-01\\', end=f\\'{end_year+1}-01-01\\').strftime(\\'%Y-%m-%d\\').tolist()[:-1]\\n    df = pd.DataFrame([[i[0],i[-1]] for i in chunkify(date_list,n_chunks_datestr)], columns=[\\'start_datestr\\',\\'end_datestr\\'])\\n    df[\\'row_group_ids\\']=[chunkify(list(range(total_row_groups)),n_chunks_row_group)]*len(df)\\n    df = df.explode(\\'row_group_ids\\').reset_index(drop=True)\\n    df[\\'path\\'] = df.apply(lambda row:f\"{base_path.strip(\\'/\\')}/file_{row.start_datestr.replace(\\'-\\',\\'\\')}_{row.end_datestr.replace(\\'-\\',\\'\\')}_{row.row_group_ids[0]}_{row.row_group_ids[-1]}.parquet\", axis=1)\\n    df[\\'idx\\'] = df.index\\n    df[\\'row_group\\'] = df[\\'row_group_ids\\']\\n    df = df.explode(\\'row_group\\').drop_duplicates(\\'idx\\').sort_values(\\'row_group\\')\\n    del df[\\'row_group\\']\\n    return df\\n\\ndef write_log(msg=\"Your message.\", name=\\'//default\\', log_type=\\'info\\', rotation=\"10 MB\"):\\n    from loguru import logger\\n    path = fused.file_path(\\'logs/\\' + name + \\'.log\\')\\n    logger.add(path, rotation=rotation)\\n    if log_type==\\'warning\\':\\n        logger.warning(msg)\\n    else:\\n        logger.info(msg)  # Write the log message\\n    logger.remove()  # Remove the log handler\\n    from datetime import datetime\\n    timestamp = datetime.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')\\n    print(f\"{timestamp} | {path} |msg: {msg}\" )\\n\\ndef read_log(n=None, name=\\'default\\', return_log=False):\\n    path = fused.file_path(\\'logs/\\' + name + \\'.log\\')\\n    try:\\n        with open(path, \\'r\\') as file:\\n            log_content = file.readlines()\\n            if n:\\n                log_content = \\'\\'.join(log_content[-n:])  # Return last \\'tail_lines\\' entries\\n            else:\\n                log_content = \\'\\'.join(log_content)\\n            if return_log:\\n                return log_content\\n            else:\\n                print(log_content)\\n    except FileNotFoundError:\\n        if return_log:\\n            return \"Log file not found.\"\\n        else:\\n            print(\"Log file not found.\")\\n\\ndef dilate_bbox(bounds, chip_len, border_pixel):\\n    bbox_crs = bounds.crs\\n    clipped_chip=chip_len-(border_pixel*2)\\n    bounds = bounds.to_crs(bounds.estimate_utm_crs())\\n    length = bounds.area[0]**0.5\\n    buffer_ratio = (chip_len-clipped_chip)/clipped_chip\\n    buffer_distance=length*buffer_ratio/2\\n    bounds.geometry = bounds.buffer(buffer_distance)\\n    bounds = bounds.to_crs(bbox_crs)\\n    return bounds\\n\\ndef read_gdf_file(path):\\n    import geopandas as gpd\\n\\n    extension = path.rsplit(\".\", maxsplit=1)[-1].lower()\\n    if extension in [\"gpkg\", \"shp\", \"geojson\"]:\\n        driver = (\\n            \"GPKG\"\\n            if extension == \"gpkg\"\\n            else (\"ESRI Shapefile\" if extension == \"shp\" else \"GeoJSON\")\\n        )\\n        return gpd.read_file(path, driver=driver)\\n    elif extension == \"zip\":\\n        return gpd.read_file(f\"zip+{path}\")\\n    elif extension in [\"parquet\", \"pq\"]:\\n        return gpd.read_parquet(path)\\n\\n\\ndef url_to_arr(url, return_colormap=False):\\n    from io import BytesIO\\n\\n    import rasterio\\n    import requests\\n    from rasterio.plot import show\\n\\n    response = requests.get(url)\\n    print(response.status_code)\\n    with rasterio.open(BytesIO(response.content)) as dataset:\\n        if return_colormap:\\n            colormap = dataset.colormap\\n            return dataset.read(), dataset.colormap(1)\\n        else:\\n            return dataset.read()\\n\\n\\ndef read_shape_zip(url, file_index=0, name_prefix=\"\"):\\n    \"\"\"This function opens any zipped shapefile\"\"\"\\n    import zipfile\\n\\n    import geopandas as gpd\\n\\n    path = fused.core.download(url, name_prefix + url.split(\"/\")[-1])\\n    fnames = [\\n        i.filename for i in zipfile.ZipFile(path).filelist if i.filename[-4:] == \".shp\"\\n    ]\\n    df = gpd.read_file(f\"{path}!{fnames[file_index]}\")\\n    return df\\n\\n@fused.cache\\ndef stac_to_gdf(bounds, datetime=\\'2024\\', collections=[\"sentinel-2-l2a\"], columns=[\\'id\\', \\'geometry\\', \\'bounds\\', \\'assets\\', \\'datetime\\', \\'eo:cloud_cover\\'], query={\"eo:cloud_cover\": {\"lt\": 20}}, catalog=\\'mspc\\', explode_assets=False, version=0):\\n    import pystac_client\\n    import stac_geoparquet\\n    if catalog.lower()==\\'aws\\':\\n        catalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\\n    elif catalog.lower()==\\'mspc\\':\\n        import planetary_computer\\n        # catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\\n        catalog = pystac_client.Client.open(\\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\\n            modifier=planetary_computer.sign_inplace,\\n        )\\n    else: \\n        catalog = pystac_client.Client.open(catalog)\\n    items = catalog.search(\\n        collections=collections,\\n        bbox=bounds.total_bounds,\\n        datetime=datetime,\\n        query=query,\\n        ).item_collection()\\n    gdf=stac_geoparquet.to_geodataframe([item.to_dict() for item in items])\\n    if explode_assets:\\n        gdf[\\'assets\\'] = gdf.assets.map(lambda x: [{k:x[k][\\'href\\']} for k in x]) \\n        gdf = gdf.explode(\\'assets\\')\\n        gdf[\\'band\\'] = gdf.assets.map(lambda x: list(x.keys())[0]) \\n        gdf[\\'url\\'] = gdf.assets.map(lambda x: list(x.values())[0]) \\n        del gdf[\\'assets\\']\\n        if columns:\\n            columns+=[\\'band\\',\\'url\\']\\n    if columns==None:\\n        print(gdf.columns)\\n        return gdf\\n    else:\\n        gdf=gdf[list(set(columns).intersection(set(gdf.columns)))]\\n        print(gdf.columns)\\n        return gdf\\n\\n@fused.cache\\ndef get_url_aws_stac(bounds, collections=[\"cop-dem-glo-30\"]):\\n    import pystac_client\\n    catalog = pystac_client.Client.open(\"https://earth-search.aws.element84.com/v1\")\\n    items = catalog.search(\\n        collections=collections,\\n        bbox=bounds.total_bounds,\\n    ).item_collection()\\n    url_list=[i[\\'assets\\'][\\'data\\'][\\'href\\'] for i in items.to_dict()[\\'features\\']]\\n    return url_list\\n        \\ndef get_collection_bbox(collection):\\n    import geopandas as gpd\\n    import planetary_computer\\n    import pystac_client\\n\\n    catalog = pystac_client.Client.open(\\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\\n        modifier=planetary_computer.sign_inplace,\\n    )\\n    asset = catalog.get_collection(collection).assets[\"geoparquet-items\"]\\n    df = gpd.read_parquet(\\n        asset.href, storage_options=asset.extra_fields[\"table:storage_options\"]\\n    )\\n    return df[[\"assets\", \"datetime\", \"geometry\"]]\\n\\n\\ndef get_pc_token(url):\\n    from urllib.parse import urlparse\\n\\n    import requests\\n\\n    parsed_url = urlparse(url.rstrip(\"/\"))\\n    account_name = parsed_url.netloc.split(\".\")[0]\\n    path_blob = parsed_url.path.lstrip(\"/\").split(\"/\", 1)\\n    container_name = path_blob[-2]\\n    url = f\"https://planetarycomputer.microsoft.com/api/sas/v1/token/{account_name}/{container_name}\"\\n    response = requests.get(url)\\n    return response.json()\\n\\n@fused.cache(path=\"table_to_tile\")\\ndef table_to_tile(\\n    bounds,\\n    table=\"s3://fused-asset/imagery/naip/\",\\n    min_zoom=12,\\n    centorid_zoom_offset=0,\\n    use_columns=[\"geometry\"],\\n    clip=False,\\n    print_xyz=False,\\n):\\n    import fused\\n    import geopandas as gpd\\n    import pandas as pd\\n\\n    version = \"0.2.3\"\\n\\n    try:\\n        x, y, z = bounds[[\"x\", \"y\", \"z\"]].iloc[0]\\n        if print_xyz:\\n            print(x, y, z)\\n    except:\\n        z = min_zoom\\n    df = fused.get_chunks_metadata(table)\\n    if isinstance(bounds, (list, tuple, np.ndarray)):\\n        bounds=to_gdf(bounds)\\n    elif len(bounds) > 1: \\n        bounds = bounds.dissolve().reset_index(drop=True)\\n    else:\\n        bounds = bounds.reset_index(drop=True)\\n    df = df[df.intersects(bounds.geometry[0])]\\n    if z >= min_zoom:\\n        List = df[[\"file_id\", \"chunk_id\"]].values\\n        if not len(List):\\n            # No result at this area\\n            return gpd.GeoDataFrame(geometry=[])\\n        if use_columns:\\n            if \"geometry\" not in use_columns:\\n                use_columns += [\"geometry\"]\\n            rows_df = pd.concat(\\n                [\\n                    fused.get_chunk_from_table(table, fc[0], fc[1], columns=use_columns)\\n                    for fc in List\\n                ]\\n            )\\n        else:\\n            rows_df = pd.concat(\\n                [fused.get_chunk_from_table(table, fc[0], fc[1]) for fc in List]\\n            )\\n            print(\"available columns:\", list(rows_df.columns))\\n        try:\\n            df = rows_df[rows_df.intersects(bounds.geometry[0])]\\n        except:\\n            df = rows_df\\n        df.crs = bounds.crs\\n        if (\\n            z < min_zoom + centorid_zoom_offset\\n        ):  # switch to centroid for the last one zoom level before showing metadata\\n            df.geometry = df.geometry.centroid\\n        if clip:\\n            return df.clip(bounds).explode()\\n        else:\\n            return df\\n    else:\\n        df.crs = bounds.crs\\n        if clip:\\n            return df.clip(bounds).explode()\\n        else:\\n            return df\\n\\n\\ndef rasterize_geometry(\\n    geom: Dict, shape: Tuple[int, int], affine, all_touched: bool = False\\n) -> NDArray[np.uint8]:\\n    \"\"\"Return an image array with input geometries burned in.\\n\\n    Args:\\n        geom: GeoJSON geometry\\n        shape: desired 2-d shape\\n        affine: transform for the image\\n        all_touched: rasterization strategy. Defaults to False.\\n\\n    Returns:\\n        numpy array with input geometries burned in.\\n    \"\"\"\\n    from rasterio import features\\n\\n    geoms = [(geom, 1)]\\n    rv_array = features.rasterize(\\n        geoms,\\n        out_shape=shape,\\n        transform=affine,\\n        fill=0,\\n        dtype=\"uint8\",\\n        all_touched=all_touched,\\n    )\\n\\n    return rv_array.astype(bool)\\n\\n\\n@fused.cache(path=\"geom_stats\")\\ndef geom_stats(gdf, arr, output_shape=(255, 255)):\\n    import numpy as np\\n\\n    df_3857 = gdf.to_crs(3857)\\n    df_tile = df_3857.dissolve()\\n    minx, miny, maxx, maxy = df_tile.total_bounds\\n    dx = (maxx - minx) / output_shape[-1]\\n    dy = (maxy - miny) / output_shape[-2]\\n    transform = [dx, 0.0, minx, 0.0, -dy, maxy, 0.0, 0.0, 1.0]\\n    geom_masks = [\\n        rasterize_geometry(geom, arr.shape[-2:], transform) for geom in df_3857.geometry\\n    ]\\n    if isinstance(arr, np.ma.MaskedArray):\\n        arr = arr.data\\n    gdf[\"stats\"] = [np.nanmean(arr[geom_mask]) for geom_mask in geom_masks]\\n    gdf[\"count\"] = [geom_mask.sum() for geom_mask in geom_masks]\\n    return gdf\\n\\n\\ndef earth_session(cred):\\n    from job2.credentials import get_session\\n    from rasterio.session import AWSSession\\n\\n    aws_session = get_session(\\n        cred[\"env\"],\\n        earthdatalogin_username=cred[\"username\"],\\n        earthdatalogin_password=cred[\"password\"],\\n    )\\n    return AWSSession(aws_session, requester_pays=False)\\n\\n\\n@fused.cache(path=\"read_tiff\")\\ndef read_tiff(\\n    bounds,\\n    input_tiff_path,\\n    filter_list=None,\\n    output_shape=(256, 256),\\n    overview_level=None,\\n    return_colormap=False,\\n    return_transform=False,\\n    return_crs=False,\\n    return_bounds=False,\\n    return_meta=False,\\n    cred=None,\\n):\\n    import os\\n    from contextlib import ExitStack\\n\\n    import numpy as np\\n    import rasterio\\n    from rasterio.coords import BoundingBox, disjoint_bounds\\n    from rasterio.warp import Resampling, reproject\\n    from scipy.ndimage import zoom\\n\\n    version = \"0.2.0\"\\n\\n    if not cred:\\n        context = rasterio.Env()\\n    else:\\n        aws_session = earth_session(cred=cred)\\n        context = rasterio.Env(\\n            aws_session,\\n            GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\\n            GDAL_HTTP_COOKIEFILE=os.path.expanduser(\"/tmp/cookies.txt\"),\\n            GDAL_HTTP_COOKIEJAR=os.path.expanduser(\"/tmp/cookies.txt\"),\\n        )\\n    with ExitStack() as stack:\\n        stack.enter_context(context)\\n        try:\\n            with rasterio.open(input_tiff_path, OVERVIEW_LEVEL=overview_level) as src:\\n                # with rasterio.Env():\\n                if src.crs:\\n                    src_crs = src.crs\\n                else:\\n                    src_crs=4326\\n                src_bbox = bounds.to_crs(src_crs)\\n\\n                if disjoint_bounds(src.bounds, BoundingBox(*src_bbox.total_bounds)):\\n                    return None\\n\\n                window = src.window(*src_bbox.total_bounds)\\n\\n                factor = 1\\n                if output_shape is not None:\\n                    # determine a factor to downsample based on the overviews of the first band\\n                    for f in src.overviews(1):\\n                        if (window.height / f) < output_shape[0]:\\n                            break\\n                        else:\\n                            factor = f\\n\\n                    n_pixels_window = window.height * window.width / (factor ** 2)\\n                    if n_pixels_window > (output_shape[-2] * output_shape[-1] * 4):\\n                        # if we would still be reading too much data with the current\\n                        # window and factor (cutoff at 4x the desired output size)\\n                        # -> increase factor to avoid memory blowup\\n                        new_factor =  min(\\n                            window.height / (output_shape[-2]*2),\\n                            window.width / (output_shape[-1]*2)\\n                        )\\n                        factor = int(new_factor // factor) * factor\\n\\n                # # transform_bounds = rasterio.warp.transform_bounds(3857, src_crs, *bounds[\"geometry\"].bounds.iloc[0])\\n                # window = src.window(*bounds.to_crs(src_crs).total_bounds)\\n                # original_window = src.window(*bounds.to_crs(src_crs).total_bounds)\\n                # gridded_window = rasterio.windows.round_window_to_full_blocks(\\n                #     original_window, [(1, 1)]\\n                # )\\n                # window = gridded_window  # Expand window to nearest full pixels\\n                source_data = src.read(\\n                    window=window,\\n                    out_shape=(src.count, int(window.height / factor), int(window.width / factor)),\\n                    resampling=Resampling.bilinear,\\n                    boundless=True,\\n                    masked=True,\\n                )\\n\\n                window_transform = src.window_transform(window)\\n                src_transform = window_transform * window_transform.scale(\\n                    (window.width / source_data.shape[-1]),\\n                    (window.height / source_data.shape[-2])\\n                )\\n                src_dtype = src.dtypes[0]\\n                src_meta = src.meta\\n                nodata_value = src.nodatavals[0]\\n\\n                if filter_list:\\n                    mask = np.isin(source_data, filter_list, invert=True)\\n                    source_data[mask] = 0\\n                if return_colormap:\\n                    colormap = src.colormap(1)\\n        except rasterio.RasterioIOError as err:\\n            print(f\"Caught RasterioIOError {err=}, {type(err)=}\")\\n            return  # Return without data\\n        except Exception as err:\\n            print(f\"Unexpected {err=}, {type(err)=}\")\\n            raise\\n        if output_shape:\\n            # reproject\\n            bbox_web = bounds.to_crs(\"EPSG:3857\")\\n            minx, miny, maxx, maxy = bbox_web.total_bounds\\n            dx = (maxx - minx) / output_shape[-1]\\n            dy = (maxy - miny) / output_shape[-2]\\n            dst_transform = [dx, 0.0, minx, 0.0, -dy, maxy, 0.0, 0.0, 1.0]\\n            if len(source_data.shape) == 3 and source_data.shape[0] > 1:\\n                dst_shape = (source_data.shape[0], output_shape[-2], output_shape[-1])\\n            else:\\n                dst_shape = output_shape\\n            dst_crs = bbox_web.crs\\n\\n            destination_data = np.zeros(dst_shape, src_dtype)\\n            reproject(\\n                source_data,\\n                destination_data,\\n                src_transform=src_transform,\\n                src_crs=src_crs,\\n                dst_transform=dst_transform,\\n                dst_crs=dst_crs,\\n                # TODO: rather than nearest, get all the values and then get pct\\n                resampling=Resampling.bilinear,\\n            )\\n\\n            destination_mask = np.zeros(dst_shape, dtype=\"int8\")\\n            reproject(\\n                source_data.mask.astype(\"uint8\"),\\n                destination_mask,\\n                src_transform=src_transform,\\n                src_crs=src_crs,\\n                dst_transform=dst_transform,\\n                dst_crs=dst_crs,\\n                resampling=Resampling.nearest,\\n            )\\n            destination_data = np.ma.masked_array(\\n                destination_data, destination_mask\\n            )\\n        else:\\n            dst_transform = src_transform\\n            dst_crs = src_crs\\n            destination_data = source_data\\n            destination_data = np.ma.masked_array(\\n                source_data, source_data == nodata_value\\n            )\\n    if return_colormap or return_transform or return_crs or return_bounds or return_meta:\\n        ### TODO: NOT backward comp -- fix colormap / transform \\n        metadata={}\\n        if return_colormap:\\n            # todo: only set transparency to zero\\n            colormap[0] = [0, 0, 0, 0]\\n            metadata[\\'colormap\\']=colormap\\n        if return_crs:  \\n            metadata[\\'crs\\']=dst_crs\\n        if return_transform:  # Note: usually you do not need this since it can be calculated using crs=4326 and bounds\\n            metadata[\\'transform\\']=dst_transform\\n        if return_bounds:\\n            metadata[\\'bounds\\']=rasterio.transform.array_bounds(destination_data.shape[-2] , destination_data.shape[-1], dst_transform)\\n        if return_meta:\\n            metadata[\\'meta\\']=src_meta\\n        return destination_data, metadata\\n        \\n    else:\\n        return destination_data\\n\\n\\ndef get_bounds_tiff(tiff_path):\\n    import rasterio\\n    with rasterio.open(tiff_path) as src:\\n        bounds = src.bounds\\n        import shapely \\n        import geopandas as gpd\\n        bounds = gpd.GeoDataFrame({}, geometry=[shapely.box(*bounds)],crs=src.crs)\\n        bounds = bounds.to_crs(4326)\\n        return bounds\\n\\ndef gdf_to_mask_arr(gdf, shape, first_n=None):\\n    from rasterio.features import geometry_mask\\n\\n    xmin, ymin, xmax, ymax = gdf.total_bounds\\n    w = (xmax - xmin) / shape[-1]\\n    h = (ymax - ymin) / shape[-2]\\n    if first_n:\\n        geom = gdf.geometry.iloc[:first_n]\\n    else:\\n        geom = gdf.geometry\\n    return ~geometry_mask(\\n        geom,\\n        transform=(w, 0, xmin, 0, -h, ymax, 0, 0, 0),\\n        invert=True,\\n        out_shape=shape[-2:],\\n    )\\n\\n\\ndef mosaic_tiff(\\n    bounds,\\n    tiff_list,\\n    reduce_function=None,\\n    filter_list=None,\\n    output_shape=(256, 256),\\n    overview_level=None,\\n    cred=None,\\n):\\n    import numpy as np\\n\\n    if not reduce_function:\\n        reduce_function = lambda x: np.max(x, axis=0)\\n    a = []\\n    for input_tiff_path in tiff_list:\\n        if not input_tiff_path:\\n            continue\\n        new_tiff = read_tiff(\\n            bounds=bounds,\\n            input_tiff_path=input_tiff_path,\\n            filter_list=filter_list,\\n            output_shape=output_shape,\\n            overview_level=overview_level,\\n            cred=cred,\\n        )\\n        if new_tiff is not None:\\n            a.append(new_tiff)\\n\\n    if len(a) == 0:\\n        return\\n    elif len(a) == 1:\\n        data = a[0]\\n    else:\\n        data = reduce_function(a)\\n    return data  # .squeeze()[:-2,:-2]\\n\\n\\ndef arr_resample(arr, dst_shape=(512, 512), order=0):\\n    import numpy as np\\n    from scipy.ndimage import zoom\\n\\n    zoom_factors = np.array(dst_shape) / np.array(arr.shape[-2:])\\n    if len(arr.shape) == 2:\\n        return zoom(arr, zoom_factors, order=order)\\n    elif len(arr.shape) == 3:\\n        return np.asanyarray([zoom(i, zoom_factors, order=order) for i in arr])\\n\\n\\ndef arr_to_cog(\\n    arr,\\n    bounds=(-180, -90, 180, 90),\\n    crs=4326,\\n    output_path=\"output_cog.tif\",\\n    blockxsize=256,\\n    blockysize=256,\\n    overviews=[2, 4, 8, 16],\\n):\\n    import numpy as np\\n    import rasterio\\n    from rasterio.crs import CRS\\n    from rasterio.enums import Resampling\\n    from rasterio.transform import from_bounds\\n\\n    data = arr.squeeze()\\n    # Define the CRS (Coordinate Reference System)\\n    crs = CRS.from_epsg(crs)\\n\\n    # Calculate transform\\n    transform = from_bounds(*bounds, data.shape[-1], data.shape[-2])\\n    if len(data.shape) == 2:\\n        data = np.stack([data])\\n        count = 1\\n    elif len(data.shape) == 3:\\n        if data.shape[0] == 3:\\n            count = 3\\n        elif data.shape[0] == 4:\\n            count = 4\\n        else:\\n            print(data.shape)\\n            return f\"Wrong number of bands {data.shape[0]}. The options are: 1(gray) | 3 (RGB) | 4 (RGBA)\"\\n    else:\\n        return f\"wrong shape {data.shape}. Data shape options are: (ny,nx) | (1,ny,nx) | (3,ny,nx) | (4,ny,nx)\"\\n    # Write the numpy array to a Cloud-Optimized GeoTIFF file\\n    with rasterio.open(\\n        output_path,\\n        \"w\",\\n        driver=\"GTiff\",\\n        height=data.shape[-2],\\n        width=data.shape[-1],\\n        count=count,\\n        dtype=data.dtype,\\n        crs=crs,\\n        transform=transform,\\n        tiled=True,  # Enable tiling\\n        blockxsize=blockxsize,  # Set block size\\n        blockysize=blockysize,  # Set block size\\n        compress=\"deflate\",  # Use compression\\n        interleave=\"band\",  # Interleave bands\\n    ) as dst:\\n        dst.write(data)\\n        # Build overviews (pyramid layers)\\n        dst.build_overviews(overviews, Resampling.nearest)\\n        # Update tags to comply with COG standards\\n        dst.update_tags(ns=\"rio_overview\", resampling=\"nearest\")\\n    return output_path\\n\\n\\ndef arr_to_color(arr, colormap, out_dtype=\"uint8\"):\\n    import numpy as np\\n\\n    mapped_colors = np.array([colormap[val] for val in arr.flat])\\n    return (\\n        mapped_colors.reshape(arr.shape[-2:] + (len(colormap[0]),))\\n        .astype(out_dtype)\\n        .transpose(2, 0, 1)\\n    )\\n\\n\\ndef arr_to_plasma(\\n    data, min_max=(0, 255), colormap=\"plasma\", include_opacity=False, reverse=True\\n):\\n    import numpy as np\\n\\n    data = data.astype(float)\\n    if min_max:\\n        norm_data = (data - min_max[0]) / (min_max[1] - min_max[0])\\n        norm_data = np.clip(norm_data, 0, 1)\\n    else:\\n        print(f\"min_max:({round(np.nanmin(data),3)},{round(np.nanmax(data),3)})\")\\n        norm_data = (data - np.nanmin(data)) / (np.nanmax(data) - np.nanmin(data))\\n    norm_data255 = (norm_data * 255).astype(\"uint8\")\\n    if colormap:\\n        # ref: https://matplotlib.org/stable/users/explain/colors/colormaps.html\\n        from matplotlib import colormaps\\n\\n        if include_opacity:\\n            colormap = [\\n                (\\n                    np.array(\\n                        [\\n                            colormaps[colormap](i)[0],\\n                            colormaps[colormap](i)[1],\\n                            colormaps[colormap](i)[2],\\n                            i,\\n                        ]\\n                    )\\n                    * 255\\n                ).astype(\"uint8\")\\n                for i in range(257)\\n            ]\\n            if reverse:\\n                colormap = colormap[::-1]\\n            mapped_colors = np.array([colormap[val] for val in norm_data255.flat])\\n            return (\\n                mapped_colors.reshape(data.shape + (4,))\\n                .astype(\"uint8\")\\n                .transpose(2, 0, 1)\\n            )\\n        else:\\n            colormap = [\\n                (np.array(colormaps[colormap](i)[:3]) * 255).astype(\"uint8\")\\n                for i in range(256)\\n            ]\\n            if reverse:\\n                colormap = colormap[::-1]\\n            mapped_colors = np.array([colormap[val] for val in norm_data255.flat])\\n            return (\\n                mapped_colors.reshape(data.shape + (3,))\\n                .astype(\"uint8\")\\n                .transpose(2, 0, 1)\\n            )\\n    else:\\n        return norm_data255\\n\\ndef run_cmd(cmd, cwd=\".\", shell=False, communicate=False):\\n    import shlex\\n    import subprocess\\n\\n    if type(cmd) == str:\\n        cmd = shlex.split(cmd)\\n    proc = subprocess.Popen(\\n        cmd, shell=shell, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\\n    )\\n    if communicate:\\n        return proc.communicate()\\n    else:\\n        return proc\\n\\n\\ndef download_file(url, destination):\\n    import requests\\n\\n    try:\\n        response = requests.get(url)\\n        with open(destination, \"wb\") as file:\\n            file.write(response.content)\\n        return f\"File downloaded to \\'{destination}\\'.\"\\n    except requests.exceptions.RequestException as e:\\n        return f\"Error downloading file: {e}\"\\n\\n\\ndef fs_list_hls(\\n    path=\"lp-prod-protected/HLSL30.020/HLS.L30.T10SEG.2023086T184554.v2.0/\",\\n    env=\"earthdata\",\\n    earthdatalogin_username=\"\",\\n    earthdatalogin_password=\"\",\\n):\\n    import s3fs\\n    from job2.credentials import get_credentials\\n\\n    aws_session = get_credentials(\\n        env,\\n        earthdatalogin_username=earthdatalogin_username,\\n        earthdatalogin_password=earthdatalogin_password,\\n    )\\n    fs = s3fs.S3FileSystem(\\n        key=aws_session[\"aws_access_key_id\"],\\n        secret=aws_session[\"aws_secret_access_key\"],\\n        token=aws_session[\"aws_session_token\"],\\n    )\\n    return fs.ls(path)\\n\\n\\ndef get_s3_list(path, suffix=None):\\n    import s3fs\\n\\n    fs = s3fs.S3FileSystem()\\n    if suffix:\\n        return [\"s3://\" + i for i in fs.ls(path) if i[-len(suffix) :] == suffix]\\n    else:\\n        return [\"s3://\" + i for i in fs.ls(path)]\\n\\n\\n@fused.cache\\ndef get_s3_list_walk(path):\\n    #version 2 (recursive)\\n    import s3fs\\n    s3 = s3fs.S3FileSystem()\\n    # Recursively list all files in the specified S3 path\\n    flist=[]\\n    for dirpath, dirnames, filenames in s3.walk(path):\\n        for filename in filenames:\\n            flist.append(f\"s3://{dirpath}/{filename}\")\\n    return flist\\n\\n\\ndef run_async(fn, arr_args, delay=0, max_workers=32):\\n    import asyncio\\n    import concurrent.futures\\n\\n    import nest_asyncio\\n    import numpy as np\\n\\n    nest_asyncio.apply()\\n\\n    loop = asyncio.get_event_loop()\\n    pool = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\\n\\n    async def fn_async(pool, fn, *args):\\n        try:\\n            result = await loop.run_in_executor(pool, fn, *args)\\n            return result\\n        except OSError as error:\\n            print(f\"Error: {error}\")\\n            return None\\n\\n    async def fn_async_exec(fn, arr, delay):\\n        tasks = []\\n        await asyncio.sleep(delay * np.random.random())\\n        if type(arr[0]) == list or type(arr[0]) == tuple:\\n            pass\\n        else:\\n            arr = [[i] for i in arr]\\n        for i in arr:\\n            tasks.append(fn_async(pool, fn, *i))\\n        return await asyncio.gather(*tasks)\\n\\n    return loop.run_until_complete(fn_async_exec(fn, arr_args, delay))\\n\\n\\ndef run_pool(fn, arg_list, max_workers=36):\\n    import concurrent.futures\\n\\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as pool:\\n        return list(pool.map(fn, arg_list))\\n\\n\\ndef import_env(\\n    env=\"testxenv\",\\n    mnt_path=\"/mnt/cache/envs/\",\\n    packages_path=\"/lib/python3.11/site-packages\",\\n):\\n    import sys\\n\\n    sys.path.append(f\"{mnt_path}{env}{packages_path}\")\\n\\n\\ndef install_module(\\n    name,\\n    env=\"testxenv\",\\n    mnt_path=\"/mnt/cache/envs/\",\\n    packages_path=\"/lib/python3.11/site-packages\",\\n):\\n    import_env(env, mnt_path, packages_path)\\n    import os\\n    import sys\\n\\n    path = f\"{mnt_path}{env}{packages_path}\"\\n    sys.path.append(path)\\n    if not os.path.exists(path):\\n        run_cmd(f\"python -m venv  {mnt_path}{env}\", communicate=True)\\n    return run_cmd(\\n        f\"{mnt_path}{env}/bin/python -m pip install {name}\", communicate=True\\n    )\\n\\n\\ndef read_module(url, remove_strings=[]):\\n    import requests\\n\\n    content_string = requests.get(url).text\\n    if len(remove_strings) > 0:\\n        for i in remove_strings:\\n            content_string = content_string.replace(i, \"\")\\n    module = {}\\n    exec(content_string, module)\\n    return module\\n\\n\\ndef get_geo_cols(data) -> List[str]:\\n    \"\"\"Get the names of the geometry columns.\\n\\n    The first item in the result is the name of the primary geometry column. Following\\n    items are other columns with a type of GeoSeries.\\n    \"\"\"\\n    import geopandas as gpd\\n    main_col = data.geometry.name\\n    cols = [\\n        i for i in data.columns if (type(data[i]) == gpd.GeoSeries) & (i != main_col)\\n    ]\\n    return [main_col] + cols\\n\\n\\ndef crs_display(crs):\\n    \"\"\"Convert a CRS object into its human-readable EPSG code if possible.\\n\\n    If the CRS object does not have a corresponding EPSG code, the CRS object itself is\\n    returned.\\n    \"\"\"\\n    try:\\n        epsg_code = crs.to_epsg()\\n        if epsg_code is not None:\\n            return epsg_code\\n        else:\\n            return crs\\n    except Exception:\\n        return crs\\n\\n\\ndef resolve_crs(gdf,\\n                crs,\\n                verbose= False\\n):\\n    \"\"\"Reproject a GeoDataFrame to the given CRS\\n\\n    Args:\\n        gdf: The GeoDataFrame to reproject.\\n        crs: The CRS to use as destination CRS.\\n        verbose: Whether to print log statements while running. Defaults to False.\\n\\n    Returns:\\n        _description_\\n    \"\"\"\\n    if str(crs).lower() == \"utm\":\\n        if gdf.crs is None:\\n            gdf = gdf.set_crs(4326)\\n            if verbose:\\n                logger.debug(\"No crs exists on `gdf`. Assuming it\\'s WGS84 (epsg:4326).\")\\n\\n        utm_crs = gdf.estimate_utm_crs()\\n        if gdf.crs == utm_crs:\\n            if verbose:\\n                logger.debug(f\"CRS is already {crs_display(utm_crs)}.\")\\n            return gdf\\n\\n        else:\\n            if verbose:\\n                logger.debug(\\n                    f\"Converting from {crs_display(gdf.crs)} to {crs_display(utm_crs)}.\"\\n                )\\n            return gdf.to_crs(utm_crs)\\n\\n    elif (gdf.crs is not None) & (gdf.crs != crs):\\n        old_crs = gdf.crs\\n        if verbose:\\n            logger.debug(\\n                f\"Converting from {crs_display(old_crs)} to {crs_display(crs)}.\"\\n            )\\n        return gdf.to_crs(crs)\\n    elif gdf.crs is None:\\n        raise ValueError(\"gdf.crs is None and reprojection could not be performed.\")\\n    else:\\n        if verbose:\\n            logger.debug(f\"crs is already {crs_display(crs)}.\")\\n\\n        return gdf\\n\\n\\ndef infer_lonlat(columns: Sequence[str]) -> Optional[Tuple[str, str]]:\\n    \"\"\"Infer longitude and latitude columns from the column names of the DataFrame\\n\\n    Args:\\n        columns: the column names in the DataFrame\\n\\n    Returns:\\n        The pair of (longitude, latitude) column names, if found. Otherwise None.\\n    \"\"\"\\n    columns_set = set(columns)\\n    allowed_column_pairs = [\\n        (\"longitude\", \"latitude\"),\\n        (\"lon\", \"lat\"),\\n        (\"lng\", \"lat\"),\\n        (\"fused_centroid_x\", \"fused_centroid_y\"),\\n        (\"fused_centroid_x_left\", \"fused_centroid_y_left\"),\\n        (\"fused_centroid_x_right\", \"fused_centroid_x_right\"),\\n    ]\\n    for allowed_column_pair in allowed_column_pairs:\\n        if (\\n            allowed_column_pair[0] in columns_set\\n            and allowed_column_pair[1] in columns_set\\n        ):\\n            return allowed_column_pair\\n    return None\\n\\n\\ndef df_to_gdf(df, cols_lonlat=None, verbose=False):\\n    import json\\n\\n    import pyarrow as pa\\n    import shapely\\n    from geopandas.io.arrow import _arrow_to_geopandas\\n\\n    geo_metadata = {\\n        \"primary_column\": \"geometry\",\\n        \"columns\": {\"geometry\": {\"encoding\": \"WKB\", \"crs\": 4326}},\\n        \"version\": \"1.0.0-beta.1\",\\n    }\\n    arrow_geo_metadata = {b\"geo\": json.dumps(geo_metadata).encode()}\\n    if not cols_lonlat:\\n        cols_lonlat = infer_lonlat(list(df.columns))\\n        if not cols_lonlat:\\n            raise ValueError(\"no latitude and longitude columns were found.\")\\n\\n        assert (\\n            cols_lonlat[0] in df.columns\\n        ), f\"column name {cols_lonlat[0]} was not found.\"\\n        assert (\\n            cols_lonlat[1] in df.columns\\n        ), f\"column name {cols_lonlat[1]} was not found.\"\\n\\n        if verbose:\\n            logger.debug(\\n                f\"Converting {cols_lonlat} to points({cols_lonlat[0]},{cols_lonlat[0]}).\"\\n            )\\n    geoms = shapely.points(df[cols_lonlat[0]], df[cols_lonlat[1]])\\n    table = pa.Table.from_pandas(df)\\n    table = table.append_column(\"geometry\", pa.array(shapely.to_wkb(geoms)))\\n    table = table.replace_schema_metadata(arrow_geo_metadata)\\n    try:\\n        df = _arrow_to_geopandas(table)\\n    except:\\n        df = _arrow_to_geopandas(table.drop([\"__index_level_0__\"]))\\n    return df\\n\\n\\ndef to_gdf(\\n    data,\\n    crs=None,\\n    cols_lonlat=None,\\n    col_geom=\"geometry\",\\n    verbose: bool = False,\\n):\\n    \"\"\"Convert input data into a GeoPandas GeoDataFrame.\"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    import pandas as pd\\n    import mercantile\\n    \\n    # Convert xyz dict to xyz array\\n    if isinstance(data, dict) and set(data.keys()) == {\\'x\\', \\'y\\', \\'z\\'}:\\n        try:\\n            data = [int(data[\\'x\\']), int(data[\\'y\\']), int(data[\\'z\\'])]\\n        except (ValueError, TypeError):\\n            pass     \\n            \\n    \\n    if data is None or (isinstance(data, (list, tuple, np.ndarray))):\\n        \\n        data = [327, 791, 11] if data is None else data #if no data, get a tile in SF\\n        \\n        if len(data) == 3: # Handle xyz tile coordinates\\n            x, y, z = data\\n            tile = mercantile.Tile(x, y, z)\\n            bounds = mercantile.bounds(tile)\\n            gdf = gpd.GeoDataFrame(\\n                {\"x\": [x], \"y\": [y], \"z\": [z]},\\n                geometry=[shapely.box(bounds.west, bounds.south, bounds.east, bounds.north)],\\n                crs=4326\\n            )\\n            return gdf[[\\'x\\', \\'y\\', \\'z\\', \\'geometry\\']]\\n         \\n        elif len(data) == 4: # Handle the bounds case specifically        \\n            return gpd.GeoDataFrame({}, geometry=[shapely.box(*data)], crs=crs or 4326)        \\n        \\n    if cols_lonlat:\\n        if isinstance(data, pd.Series):\\n            raise ValueError(\\n                \"Cannot pass a pandas Series or a geopandas GeoSeries in conjunction \"\\n                \"with cols_lonlat.\"\\n            )\\n        gdf = df_to_gdf(data, cols_lonlat, verbose=verbose)\\n        if verbose:\\n            logger.debug(\\n                \"cols_lonlat was passed so original CRS was assumed to be EPSG:4326.\"\\n            )\\n        if crs:\\n            gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        return gdf\\n    if isinstance(data, gpd.GeoDataFrame):\\n        gdf = data\\n        if crs:\\n            gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        elif gdf.crs is None:\\n            raise ValueError(\"Please provide crs. usually crs=4326.\")\\n        return gdf\\n    elif isinstance(data, gpd.GeoSeries):\\n        gdf = gpd.GeoDataFrame(data=data)\\n        if crs:\\n            gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        elif gdf.crs is None:\\n            raise ValueError(\"Please provide crs. usually crs=4326.\")\\n        return gdf\\n    elif type(data) in (pd.DataFrame, pd.Series):\\n        if type(data) is pd.Series:\\n            data = pd.DataFrame(data)\\n            if col_geom in data.index:\\n                data = data.T\\n        if (col_geom in data.columns) and (not cols_lonlat):\\n            if type(data[col_geom][0]) == str:\\n                gdf = gpd.GeoDataFrame(\\n                    data.drop(columns=[col_geom]),\\n                    geometry=shapely.from_wkt(data[col_geom]),\\n                )\\n            else:\\n                gdf = gpd.GeoDataFrame(data)\\n            if gdf.crs is None:\\n                if crs:\\n                    gdf = gdf.set_crs(crs)\\n                else:\\n                    raise ValueError(\"Please provide crs. usually crs=4326.\")\\n            elif crs:\\n                gdf = resolve_crs(gdf, crs, verbose=verbose)\\n        elif not cols_lonlat:\\n            cols_lonlat = infer_lonlat(data.columns)\\n            if not cols_lonlat:\\n                raise ValueError(\"no latitude and longitude columns were found.\")\\n            if crs:\\n                if verbose:\\n                    logger.debug(\\n                        f\"cols_lonlat was passed so crs was set to wgs84(4326) and {crs=} was ignored.\"\\n                    )\\n            # This is needed for Python 3.8 specifically, because otherwise creating the GeoDataFrame modifies the input DataFrame\\n            data = data.copy()\\n            gdf = df_to_gdf(data, cols_lonlat, verbose=verbose)\\n        return gdf\\n    elif (\\n        isinstance(data, shapely.geometry.base.BaseGeometry)\\n        or isinstance(data, shapely.geometry.base.BaseMultipartGeometry)\\n        or isinstance(data, shapely.geometry.base.EmptyGeometry)\\n    ):\\n        if not crs:\\n            raise ValueError(\"Please provide crs. usually crs=4326.\")\\n        return gpd.GeoDataFrame(geometry=[data], crs=crs)\\n    else:\\n        raise ValueError(\\n            f\"Cannot convert data of type {type(data)} to GeoDataFrame. Please pass a GeoDataFrame, GeoSeries, DataFrame, Series, or shapely geometry.\"\\n        )\\n\\ndef geo_buffer(\\n    data,\\n    buffer_distance=1000,\\n    utm_crs=\"utm\",\\n    dst_crs=\"original\",\\n    col_geom_buff=\"geom_buff\",\\n    verbose: bool=False,\\n):\\n    \"\"\"Buffer the geometry column in a GeoDataFrame in UTM projection.\\n\\n    Args:\\n        data: The GeoDataFrame to use as input.\\n        buffer_distance: The distance in meters to use for buffering geometries. Defaults to 1000.\\n        utm_crs: The CRS to use for the buffering operation. Geometries will be reprojected to this CRS before the buffering operation is applied. Defaults to \"utm\", which finds the most appropriate UTM zone based on the data.\\n        dst_crs: The CRS to use for the output geometries. Defaults to \"original\", which matches the CRS defined in the input data.\\n        col_geom_buff: The name of the column that should store the buffered geometries. Defaults to \"geom_buff\".\\n        verbose: Whether to print logging output. Defaults to False.\\n\\n    Returns:\\n        A new GeoDataFrame with a new column with buffered geometries.\\n    \"\"\"\\n    data = data.copy()\\n    assert data.crs not in (\\n        None,\\n        \"\",\\n    ), \"no crs was not found. use to_gdf to add crs\"\\n    if str(dst_crs).lower().replace(\"_\", \"\").replace(\" \", \"\").replace(\"-\", \"\") in [\\n        \"original\",\\n        \"originalcrs\",\\n        \"origcrs\",\\n        \"orig\",\\n        \"source\",\\n        \"sourcecrs\",\\n        \"srccrs\",\\n        \"src\",\\n    ]:\\n        dst_crs = data.crs\\n    if utm_crs:\\n        data[col_geom_buff] = resolve_crs(data, utm_crs, verbose=verbose).buffer(\\n            buffer_distance\\n        )\\n        data = data.set_geometry(col_geom_buff)\\n    else:\\n        data[col_geom_buff] = data.buffer(buffer_distance)\\n        data = data.set_geometry(col_geom_buff)\\n    if dst_crs:\\n        return resolve_crs(data, dst_crs, verbose=verbose)\\n    else:\\n        return data\\n\\n\\ndef geo_bbox(\\n    data,\\n    dst_crs=None,\\n    verbose: bool = False,\\n):\\n    \"\"\"Generate a GeoDataFrame that has the bounds of the current data frame.\\n\\n    Args:\\n        data: the GeoDataFrame to use as input.\\n        dst_crs: Destination CRS. Defaults to None.\\n        verbose: Provide extra logging output. Defaults to False.\\n\\n    Returns:\\n        A GeoDataFrame with one row, containing a geometry that has the bounds of this\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    import pyproj\\n    src_crs = data.crs\\n    if not dst_crs:\\n        return to_gdf(\\n            shapely.geometry.box(*data.total_bounds), crs=src_crs, verbose=verbose\\n        )\\n    elif str(dst_crs).lower() == \"utm\":\\n        dst_crs = data.estimate_utm_crs()\\n        logger.debug(f\"estimated dst_crs={crs_display(dst_crs)}\")\\n    transformer = pyproj.Transformer.from_crs(src_crs, dst_crs, always_xy=True)\\n    dst_bounds = transformer.transform_bounds(*data.total_bounds)\\n    return to_gdf(\\n        shapely.geometry.box(*dst_bounds, ccw=True), crs=dst_crs, verbose=verbose\\n    )\\n\\n\\ndef clip_bbox_gdfs(\\n    left,\\n    right,\\n    buffer_distance: Union[int, float] = 1000,\\n    join_type: Literal[\"left\", \"right\"] = \"left\",\\n    verbose: bool = True,\\n):\\n    \"\"\"Clip a DataFrame by a bounding box and then join to another DataFrame\\n\\n    Args:\\n        left: The left GeoDataFrame to use for the join.\\n        right: The right GeoDataFrame to use for the join.\\n        buffer_distance: The distance in meters to use for buffering before joining geometries. Defaults to 1000.\\n        join_type: _description_. Defaults to \"left\".\\n        verbose: Provide extra logging output. Defaults to False.\\n    \"\"\"\\n\\n    def fn(df1, df2, buffer_distance=buffer_distance):\\n        if buffer_distance:\\n            utm_crs = df1.estimate_utm_crs()\\n            # transform bounds to utm & buffer & then to df2_crs\\n            bbox_utm = geo_bbox(df1, dst_crs=utm_crs, verbose=verbose)\\n            bbox_utm_buff = geo_buffer(\\n                bbox_utm, buffer_distance, utm_crs=None, dst_crs=None, verbose=verbose\\n            )\\n            bbox_utm_buff_df2_crs = geo_bbox(\\n                bbox_utm_buff, dst_crs=df2.crs, verbose=verbose\\n            )\\n            return df1, df2.sjoin(bbox_utm_buff_df2_crs).drop(columns=\"index_right\")\\n        else:\\n            return df1, df2.sjoin(geo_bbox(df1, dst_crs=df2.crs, verbose=verbose)).drop(\\n                columns=\"index_right\"\\n            )\\n\\n    if join_type.lower() == \"left\":\\n        left, right = fn(left, right)\\n    elif join_type.lower() == \"right\":\\n        right, left = fn(right, left)\\n    else:\\n        assert False, \"join_type should be left or right\"\\n\\n    return left, right\\n\\n\\ndef geo_join(\\n    left,\\n    right,\\n    buffer_distance: Union[int, float, None] = None,\\n    utm_crs=\"utm\",\\n    clip_bbox=\"left\",\\n    drop_extra_geoms: bool = True,\\n    verbose: bool = False,\\n):\\n    \"\"\"Join two GeoDataFrames\\n\\n    Args:\\n        left: The left GeoDataFrame to use for the join.\\n        right: The right GeoDataFrame to use for the join.\\n        buffer_distance: The distance in meters to use for buffering before joining geometries. Defaults to None.\\n        utm_crs: The CRS used for UTM computations. Defaults to \"utm\", which infers a suitable UTM zone.\\n        clip_bbox: A bounding box used for clipping in the join step. Defaults to \"left\".\\n        drop_extra_geoms: Keep only the first geometry column. Defaults to True.\\n        verbose: Provide extra logging output. Defaults to False.\\n\\n    Returns:\\n        Joined GeoDataFrame.\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    if type(left) != gpd.GeoDataFrame:\\n        left = to_gdf(left, verbose=verbose)\\n    if type(right) != gpd.GeoDataFrame:\\n        right = to_gdf(right, verbose=verbose)\\n    left_geom_cols = get_geo_cols(left)\\n    right_geom_cols = get_geo_cols(right)\\n    if verbose:\\n        logger.debug(\\n            f\"primary geometry columns -- input left: {left_geom_cols[0]} | input right: {right_geom_cols[0]}\"\\n        )\\n    if clip_bbox:\\n        assert clip_bbox in (\\n            \"left\",\\n            \"right\",\\n        ), f\\'{clip_bbox} not in (\"left\", \"right\", \"None\").\\'\\n        left, right = clip_bbox_gdfs(\\n            left,\\n            right,\\n            buffer_distance=buffer_distance,\\n            join_type=clip_bbox,\\n            verbose=verbose,\\n        )\\n    if drop_extra_geoms:\\n        left = left.drop(columns=left_geom_cols[1:])\\n        right = right.drop(columns=right_geom_cols[1:])\\n    conflict_list = [\\n        col\\n        for col in right.columns\\n        if (col in left.columns) & (col not in (right_geom_cols[0]))\\n    ]\\n    for col in conflict_list:\\n        right[f\"{col}_right\"] = right[col]\\n    right = right.drop(columns=conflict_list)\\n    if not drop_extra_geoms:\\n        right[right_geom_cols[0] + \"_right\"] = right[right_geom_cols[0]]\\n    if buffer_distance:\\n        if not utm_crs:\\n            utm_crs = left.crs\\n            if verbose:\\n                logger.debug(\\n                    f\"No crs transform before applying buffer (left crs:{crs_display(utm_crs)}).\"\\n                )\\n        df_joined = geo_buffer(\\n            left,\\n            buffer_distance,\\n            utm_crs=utm_crs,\\n            dst_crs=right.crs,\\n            col_geom_buff=\"_fused_geom_buff_\",\\n            verbose=verbose,\\n        ).sjoin(right)\\n        df_joined = df_joined.set_geometry(left_geom_cols[0]).drop(\\n            columns=[\"_fused_geom_buff_\"]\\n        )\\n        if left.crs == right.crs:\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n        else:\\n            df_joined = df_joined.to_crs(left.crs)\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n    else:\\n        if left.crs != right.crs:\\n            df_joined = left.to_crs(right.crs).sjoin(right).to_crs(left.crs)\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n        else:\\n            df_joined = left.sjoin(right)\\n            if verbose:\\n                logger.debug(\\n                    f\"primary geometry columns -- output: {df_joined.geometry.name}\"\\n                )\\n            return df_joined\\n\\n\\ndef geo_distance(\\n    left,\\n    right,\\n    search_distance: Union[int, float] = 1000,\\n    utm_crs=\"utm\",\\n    clip_bbox=\"left\",\\n    col_distance: str = \"distance\",\\n    k_nearest: int = 1,\\n    cols_agg: Sequence[str] = (\"fused_index\",),\\n    cols_right: Sequence[str] = (),\\n    drop_extra_geoms: bool = True,\\n    verbose: bool = False,\\n):\\n    \"\"\"Compute the distance from rows in one dataframe to another.\\n\\n    First this performs an geo_join, and then finds the nearest rows in right to left.\\n\\n    Args:\\n        left: left GeoDataFrame\\n        right: right GeoDataFrame\\n        search_distance: Distance in meters used for buffering in the geo_join step. Defaults to 1000.\\n        utm_crs: The CRS used for UTM computations. Defaults to \"utm\", which infers a suitable UTM zone.\\n        clip_bbox: A bounding box used for clipping in the join step. Defaults to \"left\".\\n        col_distance: The column named for saving the output of the distance step. Defaults to \"distance\".\\n        k_nearest: The number of nearest values to keep.. Defaults to 1.\\n        cols_agg: Columns used for the aggregation before the join. Defaults to (\"fused_index\",).\\n        cols_right: Columns from the right dataframe to keep. Defaults to ().\\n        drop_extra_geoms: Keep only the first geometry column. Defaults to True.\\n        verbose: Provide extra logging output. Defaults to False.\\n\\n    Returns:\\n        _description_\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    if type(left) != gpd.GeoDataFrame:\\n        left = to_gdf(left, verbose=verbose)\\n    if type(right) != gpd.GeoDataFrame:\\n        right = to_gdf(right, verbose=verbose)\\n    left_geom_cols = get_geo_cols(left)\\n    right_geom_cols = get_geo_cols(right)\\n    cols_right = list(cols_right)\\n    if drop_extra_geoms:\\n        left = left.drop(columns=left_geom_cols[1:])\\n        right = right.drop(columns=right_geom_cols[1:])\\n        cols_right = [i for i in cols_right if i in right.columns]\\n    if right_geom_cols[0] not in cols_right:\\n        cols_right += [right_geom_cols[0]]\\n    if cols_agg:\\n        cols_agg = list(cols_agg)\\n        all_cols = set(list(left.columns) + list(cols_right))\\n        assert (\\n            len(set(cols_agg) - all_cols) == 0\\n        ), f\"{cols_agg=} not in the table. Please pass valid list or cols_agg=None if you want distance over entire datafame\"\\n    dfj = geo_join(\\n        left,\\n        right[cols_right],\\n        buffer_distance=search_distance,\\n        utm_crs=utm_crs,\\n        clip_bbox=clip_bbox,\\n        drop_extra_geoms=False,\\n        verbose=verbose,\\n    )\\n    if len(dfj) > 0:\\n        utm_crs = dfj[left_geom_cols[0]].estimate_utm_crs()\\n        dfj[col_distance] = (\\n            dfj[[left_geom_cols[0]]]\\n            .to_crs(utm_crs)\\n            .distance(dfj[f\"{right_geom_cols[0]}_right\"].to_crs(utm_crs))\\n        )\\n    else:\\n        dfj[col_distance] = 0\\n        if verbose:\\n            print(\"geo_join returned empty dataframe.\")\\n\\n    if drop_extra_geoms:\\n        dfj = dfj.drop(columns=get_geo_cols(dfj)[1:])\\n    if cols_agg:\\n        dfj = dfj.sort_values(col_distance).groupby(cols_agg).head(k_nearest)\\n    else:\\n        dfj = dfj.sort_values(col_distance).head(k_nearest)\\n    return dfj\\n\\n\\ndef geo_samples(\\n    n_samples: int, min_x: float, max_x: float, min_y: float, max_y: float\\n):\\n    \"\"\"\\n    Generate sample points in a bounding box, uniformly.\\n\\n    Args:\\n        n_samples: Number of sample points to generate\\n        min_x: Minimum x coordinate\\n        max_x: Maximum x coordinate\\n        min_y: Minimum y coordinate\\n        max_y: Maximum y coordinate\\n\\n    Returns:\\n        A GeoDataFrame with point geometry.\\n    \"\"\"\\n    import geopandas as gpd\\n    import shapely\\n    import random\\n    points = [\\n        (random.uniform(min_x, max_x), random.uniform(min_y, max_y))\\n        for _ in range(n_samples)\\n    ]\\n    return to_gdf(pd.DataFrame(points, columns=[\"lng\", \"lat\"]))[[\"geometry\"]]\\n\\n\\ndef bbox_stac_items(bounds, table):\\n    import fused\\n    import geopandas as gpd\\n    import pandas as pd\\n    import pyarrow.parquet as pq\\n    import shapely\\n\\n    df = fused.get_chunks_metadata(table)\\n    df = df[df.intersects(bounds)]\\n    if len(df) > 10 or len(df) == 0:\\n        return None  # fault\\n    matching_images = []\\n    for idx, row in df.iterrows():\\n        file_url = table + row[\"file_id\"] + \".parquet\"\\n        chunk_table = pq.ParquetFile(file_url).read_row_group(row[\"chunk_id\"])\\n        chunk_gdf = gpd.GeoDataFrame(chunk_table.to_pandas())\\n        if \"geometry\" in chunk_gdf:\\n            chunk_gdf.geometry = shapely.from_wkb(chunk_gdf[\"geometry\"])\\n        matching_images.append(chunk_gdf)\\n\\n    ret_gdf = pd.concat(matching_images)\\n    ret_gdf = ret_gdf[ret_gdf.intersects(bounds)]\\n    return ret_gdf\\n\\n\\n# todo: switch to read_tiff with requester_pays option\\ndef read_tiff_naip(\\n    bounds, input_tiff_path, crs, buffer_degree, output_shape, resample_order=0\\n):\\n    from io import BytesIO\\n\\n    import rasterio\\n    from rasterio.session import AWSSession\\n    from scipy.ndimage import zoom\\n\\n    out_buf = BytesIO()\\n    with rasterio.Env(AWSSession(requester_pays=True)):\\n        with rasterio.open(input_tiff_path) as src:\\n            if buffer_degree != 0:\\n                bounds.geometry = bounds.geometry.buffer(buffer_degree)\\n            bbox_projected = bounds.to_crs(crs)\\n            window = src.window(*bbox_projected.total_bounds)\\n            data = src.read(window=window, boundless=True)\\n            zoom_factors = np.array(output_shape) / np.array(data[0].shape)\\n            rgb = np.array(\\n                [zoom(arr, zoom_factors, order=resample_order) for arr in data]\\n            )\\n        return rgb\\n\\n\\ndef image_server_bbox(\\n    image_url,\\n    bounds=None,\\n    time=None,\\n    size=512,\\n    bbox_crs=4326,\\n    image_crs=3857,\\n    image_format=\"tiff\",\\n    return_colormap=False,\\n):\\n    if bbox_crs and bbox_crs != image_crs:\\n        import geopandas as gpd\\n        import shapely\\n\\n        gdf = gpd.GeoDataFrame(geometry=[shapely.box(*bounds)], crs=bbox_crs).to_crs(\\n            image_crs\\n        )\\n        print(gdf)\\n        minx, miny, maxx, maxy = gdf.total_bounds\\n    else:\\n        minx, miny, maxx, maxy = list(bounds)\\n    image_url = image_url.strip(\"/\")\\n    url_template = f\"{image_url}?f=image\"\\n    url_template += f\"&bounds={minx},{miny},{maxx},{maxy}\"\\n    if time:\\n        url_template += f\"&time={time}\"\\n    if image_crs:\\n        url_template += f\"&imageSR={image_crs}&bboxSR={image_crs}\"\\n    if size:\\n        url_template += f\"&size={size},{size*(miny-maxy)/(minx-maxx)}\"\\n    url_template += f\"&format={image_format}\"\\n    return url_to_arr(url_template, return_colormap=return_colormap)\\n\\n\\ndef arr_to_stats(arr, gdf, type=\"nominal\"):\\n    import numpy as np\\n\\n    minx, miny, maxx, maxy = gdf.total_bounds\\n    dx = (maxx - minx) / arr.shape[-1]\\n    dy = (maxy - miny) / arr.shape[-2]\\n    transform = [dx, 0.0, minx, 0.0, -dy, maxy, 0.0, 0.0, 1.0]\\n    geom_masks = [\\n        rasterize_geometry(geom, arr.shape[-2:], transform) for geom in gdf.geometry\\n    ]\\n    if type == \"nominal\":\\n        counts_per_mask = []\\n        for geom_mask in geom_masks:\\n            masked_arr = arr[geom_mask]\\n            unique_values, counts = np.unique(masked_arr, return_counts=True)\\n            counts_dict = {value: count for value, count in zip(unique_values, counts)}\\n            counts_per_mask.append(counts_dict)\\n        gdf[\"stats\"] = counts_per_mask\\n        return gdf\\n    elif type == \"numerical\":\\n        stats_per_mask = []\\n        for geom_mask in geom_masks:\\n            masked_arr = arr[geom_mask]\\n            stats_per_mask.append(\\n                {\\n                    \"min\": np.nanmin(masked_arr),\\n                    \"max\": np.nanmax(masked_arr),\\n                    \"mean\": np.nanmean(masked_arr),\\n                    \"median\": np.nanmedian(masked_arr),\\n                    \"std\": np.nanstd(masked_arr),\\n                }\\n            )\\n        gdf[\"stats\"] = stats_per_mask\\n        return gdf\\n    else:\\n        raise ValueError(\\n            f\\'{type} is not supported. Type options are \"nominal\" and \"numerical\"\\'\\n        )\\n\\n\\ndef ask_openai(prompt, openai_api_key, role=\"user\", model=\"gpt-4-turbo-preview\"):\\n    # ref: https://github.com/openai/openai-python\\n    # ref: https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\\n    from openai import OpenAI\\n\\n    client = OpenAI(\\n        api_key=openai_api_key,\\n    )\\n    messages = [\\n        {\\n            \"role\": role,\\n            \"content\": prompt,\\n        }\\n    ]\\n    chat_completion = client.chat.completions.create(\\n        messages=messages,\\n        model=model,\\n    )\\n    return [i.message.content for i in chat_completion.choices]\\n\\n\\ndef ee_initialize(service_account_name=\"\", key_path=\"\"):\\n    \"\"\"\\n    Authenticate with Google Earth Engine using service account credentials.\\n\\n    This function initializes the Earth Engine API by authenticating with the\\n    provided service account credentials. The authenticated session allows for\\n    accessing and manipulating data within Google Earth Engine.\\n\\n    Args:\\n        service_account_name (str): The email address of the service account.\\n        key_path (str): The path to the private key file for the service account.\\n\\n    See documentation: https://docs.fused.io/basics/in/gee/\\n\\n    Example:\\n        ee_initialize(\\'your-service-account@your-project.iam.gserviceaccount.com\\', \\'path/to/your-private-key.json\\')\\n    \"\"\"\\n    import ee\\n\\n    credentials = ee.ServiceAccountCredentials(service_account_name, key_path)\\n    ee.Initialize(\\n        opt_url=\"https://earthengine-highvolume.googleapis.com\", credentials=credentials\\n    )\\n\\n\\n@fused.cache\\ndef run_pool_tiffs(bounds, df_tiffs, output_shape):\\n    import numpy as np\\n\\n    columns = df_tiffs.columns\\n\\n    @fused.cache\\n    def fn_read_tiff(tiff_url, bounds=bounds, output_shape=output_shape):\\n        read_tiff = fused.load(\\n            \"https://github.com/fusedio/udfs/tree/3c4bc47/public/common/\"\\n        ).utils.read_tiff\\n        return read_tiff(bounds, tiff_url, output_shape=output_shape)\\n\\n    tiff_list = []\\n    for band in columns:\\n        for i in range(len(df_tiffs)):\\n            tiff_list.append(df_tiffs[band].iloc[i])\\n\\n    arrs_tmp = run_pool(fn_read_tiff, tiff_list)\\n    arrs_out = np.stack(arrs_tmp)\\n    arrs_out = arrs_out.reshape(\\n        len(columns), len(df_tiffs), output_shape[0], output_shape[1]\\n    )\\n    return arrs_out\\n\\n\\ndef search_pc_catalog(\\n    bounds,\\n    time_of_interest,\\n    query={\"eo:cloud_cover\": {\"lt\": 5}},\\n    collection=\"sentinel-2-l2a\",\\n):\\n    import planetary_computer\\n    import pystac_client\\n\\n    # Instantiate PC client\\n    catalog = pystac_client.Client.open(\\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\\n        modifier=planetary_computer.sign_inplace,\\n    )\\n\\n    # Search catalog\\n    items = catalog.search(\\n        collections=[collection],\\n        bbox=bounds.total_bounds,\\n        datetime=time_of_interest,\\n        query=query,\\n    ).item_collection()\\n\\n    if len(items) == 0:\\n        print(f\"empty for {time_of_interest}\")\\n        return\\n\\n    return items\\n\\n\\ndef create_tiffs_catalog(stac_items, band_list):\\n    import pandas as pd\\n\\n    input_paths = []\\n    for selected_item in stac_items:\\n        max_key_length = len(max(selected_item.assets, key=len))\\n        input_paths.append([selected_item.assets[band].href for band in band_list])\\n    return pd.DataFrame(input_paths, columns=band_list)\\n\\ndef create_chunk_metadata(df, chunk_size=10_000):\\n    total_rows = len(df)\\n    num_chunks = (total_rows + chunk_size - 1) // chunk_size\\n    meta = []\\n    for idx in range(num_chunks):\\n        chunk = df.iloc[idx * chunk_size:min((idx + 1) * chunk_size, total_rows)]\\n        meta.append({\\n            \"bbox_minx\": chunk.lng.min(),\\n            \"bbox_maxx\": chunk.lng.max(),\\n            \"bbox_miny\": chunk.lat.min(),\\n            \"bbox_maxy\": chunk.lat.max(),\\n            \"chunk_id\": idx,\\n            \"num_rows\": len(chunk)  # Additional stat showing number of rows in chunk\\n        })\\n    import pandas as pd\\n    return pd.DataFrame.from_records(meta)\\n\\ndef chunked_tiff_to_points(tiff_path, i: int = 0, x_chunks: int = 2, y_chunks: int = 2):\\n    import numpy as np\\n    import pandas as pd\\n    import rasterio\\n\\n    with rasterio.open(tiff_path) as src:\\n        x_list, y_list = shape_transform_to_xycoor(src.shape[-2:], src.transform)\\n        x_slice, y_slice = get_chunk_slices_from_shape(\\n            src.shape[-2:], x_chunks, y_chunks, i\\n        )\\n        x_list = x_list[x_slice]\\n        y_list = y_list[y_slice]\\n        X, Y = np.meshgrid(x_list, y_list)\\n        arr = src.read(window=(y_slice, x_slice)).flatten()\\n        df = pd.DataFrame(X.flatten(), columns=[\"lng\"])\\n        df[\"lat\"] = Y.flatten()\\n        df[\"data\"] = arr\\n    return df\\n\\n\\ndef shape_transform_to_xycoor(shape, transform):\\n    import numpy as np\\n\\n    n_y = shape[-2]\\n    n_x = shape[-1]\\n    w, _, x, _, h, y, _, _, _ = transform\\n    x_list = np.arange(x + w / 2, x + n_x * w + w / 2, w)[:n_x]\\n    y_list = np.arange(y + h / 2, y + n_y * h + h / 2, h)[:n_y]\\n    return x_list, y_list\\n\\n\\ndef get_chunk_slices_from_shape(array_shape, x_chunks, y_chunks, i):\\n    # Unpack the dimensions of the array shape\\n    rows, cols = array_shape\\n\\n    # Calculate the size of each chunk\\n    chunk_height = rows // y_chunks\\n    chunk_width = cols // x_chunks\\n\\n    # Calculate row and column index for the i-th chunk\\n    row_start = (i // x_chunks) * chunk_height\\n    col_start = (i % x_chunks) * chunk_width\\n\\n    # Create slice objects for the i-th chunk\\n    y_slice = slice(row_start, row_start + chunk_height)\\n    x_slice = slice(col_start, col_start + chunk_width)\\n\\n    return x_slice, y_slice\\n\\ndef arr_to_latlng(arr, bounds):\\n    import numpy as np\\n    import pandas as pd\\n    from rasterio.transform import from_bounds\\n    transform = from_bounds(*bounds, arr.shape[-1], arr.shape[-2])\\n    x_list, y_list = shape_transform_to_xycoor(arr.shape[-2:], transform)\\n    X, Y = np.meshgrid(x_list, y_list)\\n    df = pd.DataFrame(X.flatten(), columns=[\"lng\"])\\n    df[\"lat\"] = Y.flatten()\\n    df[\"data\"] = arr.flatten()\\n    return df\\n\\n# @fused.cache\\ndef df_to_h3(df, res, latlng_cols=(\"lat\", \"lng\"), ordered=False):\\n    qr = f\"\"\"\\n            SELECT h3_latlng_to_cell({latlng_cols[0]}, {latlng_cols[1]}, {res}) AS hex, ARRAY_AGG(data) as agg_data\\n            FROM df\\n            group by 1\\n        \"\"\"\\n    if ordered:\\n        qr+=\"  order by 1\"\\n    con = duckdb_connect()\\n    return con.query(qr).df()\\n\\ndef arr_to_h3(arr, bounds, res, ordered=False):\\n    return df_to_h3(arr_to_latlng(arr, bounds), res=res, ordered=ordered)\\n\\ndef duckdb_connect(home_directory=\\'/tmp/\\'):\\n    import duckdb \\n    con = duckdb.connect()\\n    con.sql(\\n    f\"\"\"SET home_directory=\\'{home_directory}\\';\\n    INSTALL h3 FROM community;\\n    LOAD h3;\\n    install \\'httpfs\\';\\n    load \\'httpfs\\';\\n    INSTALL spatial;\\n    LOAD spatial;\\n    \"\"\")\\n    print(\"duckdb version:\", duckdb.__version__)\\n    return con\\n\\n\\n# @fused.cache\\ndef run_query(query, return_arrow=False):\\n    con = duckdb_connect()\\n    if return_arrow:\\n        return con.sql(query).fetch_arrow_table()\\n    else:\\n        return con.sql(query).df()\\n\\n\\ndef ds_to_tile(ds, variable, bounds, na_values=0, cols_lonlat=(\\'x\\', \\'y\\')):\\n    da = ds[variable]\\n    x_slice, y_slice = bbox_to_xy_slice(\\n        bounds.total_bounds, ds.rio.shape, ds.rio.transform()\\n    )\\n    window = bbox_to_window(bounds.total_bounds, ds.rio.shape, ds.rio.transform())\\n    py0 = py1 = px0 = px1 = 0\\n    if window.col_off < 0:\\n        px0 = -window.col_off\\n    if window.col_off + window.width > da.shape[-2]:\\n        px1 = window.col_off + window.width - da.shape[-2]\\n    if window.row_off < 0:\\n        py0 = -window.row_off\\n    if window.row_off + window.height > da.shape[-1]:\\n        py1 = window.row_off + window.height - da.shape[-1]\\n    # data = da.isel(x=x_slice, y=y_slice, time=0).fillna(0)\\n    data = da.isel({cols_lonlat[0]:x_slice, cols_lonlat[1]:y_slice}).fillna(0)\\n    data = data.pad({cols_lonlat[0]:(px0, px1), cols_lonlat[1]:(py0, py1)},\\n                     mode=\"constant\", constant_values=na_values\\n    )\\n    return data\\n\\n# @fused.cache(cache_max_age=\\'24h\\')\\ndef tiff_to_xyz(input_tiff, output_xyz, xoff, x_block_size, yoff, y_block_size):\\n    cmd = f\"gdal_translate -srcwin {xoff*x_block_size} {yoff*y_block_size} {x_block_size} {y_block_size} -of XYZ {input_tiff} {output_xyz}\"\\n    r = run_cmd(cmd, communicate=True)\\n    # assert r[1] == b\"\"   # cehck if there is an error (r[1] != b\"\")\\n    return r\\n\\ndef xy_transform(df, src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\", \\n                         cols_src_xy=[\\'x\\',\\'y\\'], cols_dst_xy=[\\'lng\\', \\'lat\\']):\\n    from pyproj import Proj, transform\\n\\n    # Define the source (original CRS) and target (EPSG:4326) CRSs\\n    source_proj = Proj(init=src_crs)  # Replace with original CRS\\n    target_proj = Proj(init=dst_crs)  # EPSG:4326 for WGS84\\n\\n    # Transform the X, Y coordinates to EPSG:4326 (WGS84)\\n    x_coords = df[cols_src_xy[0]].values\\n    y_coords = df[cols_src_xy[1]].values\\n    lon, lat = transform(source_proj, target_proj, x_coords, y_coords)\\n    df[cols_dst_xy[0]] = lon\\n    df[cols_dst_xy[1]] = lat\\n    return df\\n\\ndef bbox_to_xy_slice(bounds, shape, transform):\\n    import rasterio\\n    from affine import Affine\\n\\n    if transform[4] < 0:  # if pixel_height is negative\\n        original_window = rasterio.windows.from_bounds(*bounds, transform=transform)\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        y_slice, x_slice = gridded_window.toslices()\\n        return x_slice, y_slice\\n    else:  # if pixel_height is not negative\\n        original_window = rasterio.windows.from_bounds(\\n            *bounds,\\n            transform=Affine(\\n                transform[0],\\n                transform[1],\\n                transform[2],\\n                transform[3],\\n                -transform[4],\\n                -transform[5],\\n            ),\\n        )\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        y_slice, x_slice = gridded_window.toslices()\\n        y_slice = slice(shape[0] - y_slice.stop, shape[0] - y_slice.start + 0)\\n        return x_slice, y_slice\\n\\n\\ndef bbox_to_window(bounds, shape, transform):\\n    import rasterio\\n    from affine import Affine\\n\\n    if transform[4] < 0:  # if pixel_height is negative\\n        original_window = rasterio.windows.from_bounds(*bounds, transform=transform)\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        return gridded_window\\n    else:  # if pixel_height is not negative\\n        original_window = rasterio.windows.from_bounds(\\n            *bounds,\\n            transform=Affine(\\n                transform[0],\\n                transform[1],\\n                transform[2],\\n                transform[3],\\n                -transform[4],\\n                -transform[5],\\n            ),\\n        )\\n        gridded_window = rasterio.windows.round_window_to_full_blocks(\\n            original_window, [(1, 1)]\\n        )\\n        return gridded_window\\n\\n\\ndef bounds_to_gdf(bounds_list, crs=4326):\\n    import geopandas as gpd\\n    import shapely\\n\\n    box = shapely.box(*bounds_list)\\n    return gpd.GeoDataFrame(geometry=[box], crs=crs)\\n\\n\\ndef mercantile_polyfill(geom, zooms=[15], compact=True, k=None):\\n    import geopandas as gpd\\n    import mercantile\\n    import shapely\\n\\n    gdf = to_gdf(geom , crs = 4326)\\n    geometry = gdf.geometry[0]\\n\\n    tile_list = list(mercantile.tiles(*geometry.bounds, zooms=zooms))\\n    gdf_tiles = gpd.GeoDataFrame(\\n        tile_list,\\n        geometry=[shapely.box(*mercantile.bounds(i)) for i in tile_list],\\n        crs=4326,\\n    )\\n    gdf_tiles_intersecting = gdf_tiles[gdf_tiles.intersects(geometry)]\\n\\n    if k:\\n        temp_list = gdf_tiles_intersecting.apply(\\n            lambda row: mercantile.Tile(row.x, row.y, row.z), 1\\n        )\\n        clip_list = mercantile_kring_list(temp_list, k)\\n        if not compact:\\n            gdf = gpd.GeoDataFrame(\\n                clip_list,\\n                geometry=[shapely.box(*mercantile.bounds(i)) for i in clip_list],\\n                crs=4326,\\n            )\\n            return gdf\\n    else:\\n        if not compact:\\n            return gdf_tiles_intersecting\\n        clip_list = gdf_tiles_intersecting.apply(\\n            lambda row: mercantile.Tile(row.x, row.y, row.z), 1\\n        )\\n    simple_list = mercantile.simplify(clip_list)\\n    gdf = gpd.GeoDataFrame(\\n        simple_list,\\n        geometry=[shapely.box(*mercantile.bounds(i)) for i in simple_list],\\n        crs=4326,\\n    )\\n    return gdf  # .reset_index(drop=True)\\n\\n\\ndef mercantile_kring(tile, k):\\n    # ToDo: Remove invalid tiles in the globe boundries (e.g. negative values)\\n    import mercantile\\n\\n    result = []\\n    for x in range(tile.x - k, tile.x + k + 1):\\n        for y in range(tile.y - k, tile.y + k + 1):\\n            result.append(mercantile.Tile(x, y, tile.z))\\n    return result\\n\\n\\ndef mercantile_kring_list(tiles, k):\\n    a = []\\n    for tile in tiles:\\n        a.extend(mercantile_kring(tile, k))\\n    return list(set(a))\\n\\n\\ndef make_tiles_gdf(bounds, zoom=14, k=0, compact=0):\\n    import shapely\\n\\n    df_tiles = mercantile_polyfill(\\n        shapely.box(*bounds), zooms=[zoom], compact=compact, k=k\\n    )\\n    df_tiles[\"bounds\"] = df_tiles[\"geometry\"].apply(lambda x: x.bounds, 1)\\n    return df_tiles\\n\\n\\ndef get_masked_array(gdf_aoi, arr_aoi):\\n    import numpy as np\\n    from rasterio.features import geometry_mask\\n    from rasterio.transform import from_bounds\\n\\n    transform = from_bounds(*gdf_aoi.total_bounds, arr_aoi.shape[-1], arr_aoi.shape[-2])\\n    geom_mask = geometry_mask(\\n        gdf_aoi.geometry,\\n        transform=transform,\\n        invert=True,\\n        out_shape=arr_aoi.shape[-2:],\\n    )\\n    masked_value = np.ma.MaskedArray(data=arr_aoi, mask=[~geom_mask])\\n    return masked_value\\n\\n\\ndef get_da(path, coarsen_factor=1, variable_index=0, xy_cols=[\"longitude\", \"latitude\"]):\\n    # Load data\\n    import xarray\\n\\n    path = fused.download(path, path.split(\\'/\\')[-1]) \\n    ds = xarray.open_dataset(path, engine=\\'h5netcdf\\')\\n    try:\\n        var = list(ds.data_vars)[variable_index]\\n        print(var)\\n        if coarsen_factor > 1:\\n            da = ds.coarsen(\\n                {xy_cols[0]: coarsen_factor, xy_cols[1]: coarsen_factor},\\n                boundary=\"trim\",\\n            ).max()[var]\\n        else:\\n            da = ds[var]\\n        print(\"done\")\\n        return da\\n    except Exception as e:\\n        print(e)\\n        ValueError()\\n\\n\\ndef get_da_bounds(da, xy_cols=(\"longitude\", \"latitude\"), pixel_position=\\'center\\'):\\n    x_list = da[xy_cols[0]].values\\n    y_list = da[xy_cols[1]].values\\n    if pixel_position==\\'center\\':\\n        pixel_width = x_list[1] - x_list[0]\\n        pixel_height = y_list[1] - y_list[0]\\n        minx = x_list[0] - pixel_width / 2\\n        miny = y_list[-1] + pixel_height / 2\\n        maxx = x_list[-1] + pixel_width / 2\\n        maxy = y_list[0] - pixel_height / 2\\n        return (minx, miny, maxx, maxy)\\n    else:\\n        return (x_list[0], y_list[-1], x_list[-1], y_list[0])\\n        \\n        \\ndef da_fit_to_resolution(da, target_shape):\\n    import numpy as np\\n    dims = da.dims\\n    new_coords = {\\n        dim: np.linspace(da[dim].min(), da[dim].max(), target_shape[i])\\n        for i, dim in enumerate(dims)\\n    }\\n    return da.interp(new_coords)    \\n\\n\\ndef clip_arr(arr, bounds_aoi, bounds_total=(-180, -90, 180, 90)):\\n    # ToDo: fix antimeridian issue by spliting and merging arr around lng=360\\n    from rasterio.transform import from_bounds\\n\\n    transform = from_bounds(*bounds_total, arr.shape[-1], arr.shape[-2])\\n    if bounds_total[2] > 180 and bounds_total[0] >= 0:\\n        print(\"Normalized longitude for bounds_aoi to (0,360) to match bounds_total\")\\n        bounds_aoi = (\\n            (bounds_aoi[0] + 360) % 360,\\n            bounds_aoi[1],\\n            (bounds_aoi[2] + 360) % 360,\\n            bounds_aoi[3],\\n        )\\n    x_slice, y_slice = bbox_to_xy_slice(bounds_aoi, arr.shape, transform)\\n    arr_aoi = arr[y_slice, x_slice]\\n    if bounds_total[1] > bounds_total[3]:\\n        if len(arr_aoi.shape) == 3:\\n            arr_aoi = arr_aoi[:, ::-1]\\n        else:\\n            arr_aoi = arr_aoi[::-1]\\n    return arr_aoi\\n\\n\\ndef visualize(\\n    data,\\n    mask: float | np.ndarray = None,\\n    min: float = 0,\\n    max: float = 1,\\n    opacity: float = 1,\\n    colormap = None,\\n):\\n    \"\"\"Convert objects into visualization tiles.\"\"\"\\n    import xarray as xr\\n    import palettable\\n    from matplotlib.colors import LinearSegmentedColormap\\n    from matplotlib.colors import Normalize   \\n    \\n    if data is None:\\n        return\\n    \\n    if colormap is None:\\n        # Set a default colormap\\n        colormap = palettable.colorbrewer.sequential.Greys_9_r\\n        cm = colormap.mpl_colormap\\n    elif isinstance(colormap, palettable.palette.Palette):\\n        cm = colormap.mpl_colormap\\n    elif isinstance(colormap, (list, tuple)):\\n        cm = LinearSegmentedColormap.from_list(\\'custom\\', colormap)\\n    else:\\n        print(\\'visualize: no type match for colormap\\')\\n\\n    if isinstance(data, xr.DataArray):\\n        # Convert from an Xarray DataArray to a Numpy ND Array\\n        data = data.values\\n\\n    if isinstance(data, np.ndarray):\\n\\n        if isinstance(data, np.ma.MaskedArray):\\n            boolean_mask = data.mask\\n            if mask is None:\\n                mask = boolean_mask\\n            else:\\n                # Combine the two masks.\\n                mask = mask * boolean_mask\\n        \\n        norm_data = Normalize(vmin=min, vmax=max, clip=False)(data)\\n        mapped_colors = cm(norm_data)\\n        if isinstance(mask, (float, np.ndarray)):\\n            mapped_colors[:,:,3] = mask * opacity\\n        else:\\n            mapped_colors[:,:,3] = opacity\\n        \\n        # Convert to unsigned 8-bit ints for visualization.\\n        vis_dtype = np.uint8\\n        max_color_value = np.iinfo(vis_dtype).max\\n        norm_data255 = (mapped_colors * max_color_value).astype(vis_dtype)\\n        \\n        # Reshape array to 4 x nRow x nCol.\\n        shaped = norm_data255.transpose(2,0,1)\\n    \\n        return shaped\\n    else:\\n        print(\\'visualize: data instance type not recognized\\')\\n\\n    \\nclass AsyncRunner:\\n    \\'\\'\\'\\n    ## Usage example:\\n    async def fn(n): return n**2\\n    runner = AsyncRunner(fn, range(10))\\n    runner.get_result_now()\\n    \\'\\'\\'\\n    def __init__(self, func, args_list, delay_second=0, verbose=True):\\n        import asyncio\\n        if isinstance(args_list, pd.DataFrame):\\n            self.args_list=args_list.T.to_dict().values()\\n        elif isinstance(args_list, list) or isinstance(args_list, range):     \\n            self.args_list=args_list\\n        else:\\n            raise ValueError(\\'args_list need to be list, pd.DataFrame, or range\\')\\n        self.func = func\\n        self.verbose = verbose\\n        self.delay_second = delay_second\\n        self.loop = asyncio.get_running_loop()\\n        self.run_async()\\n    \\n    def create_task(self, args):\\n        import time\\n        import json\\n        time.sleep(self.delay_second)\\n        if type(args)==str:\\n            args=json.loads(args)\\n        if isinstance(args, dict):\\n            task = self.loop.create_task(self.func(**args))\\n        else:\\n            task = self.loop.create_task(self.func(args))\\n        task.set_name(json.dumps(args))\\n        return task\\n        \\n    def run_async(self):\\n        tasks = []\\n        for args in self.args_list:\\n            tasks.append(self.create_task(args))\\n        self.tasks=tasks\\n    \\n    def is_done(self):\\n        return [task.done() for task in self.tasks]\\n    \\n    def get_task_result(self, r):\\n        if r.done():\\n            import pandas as pd\\n            try:\\n                return r.result()\\n            except Exception as e:\\n                return str(e)\\n        else:\\n            return \\'pending\\'\\n        \\n    def get_result_now(self, retry=True):\\n        if retry:\\n            self.retry()\\n        if self.verbose:\\n            print(f\"{sum(self.is_done())} out of {len(self.is_done())} are done!\")\\n        import json\\n        import pandas as pd\\n        df = pd.DataFrame([json.loads(task.get_name()) for task in self.tasks])\\n        df[\\'result\\']= [self.get_task_result(task) for task in self.tasks]\\n        def fn(r):\\n            if type(r)==str:\\n                if r==\\'pending\\':\\n                    return \\'running\\'\\n                else:\\n                    return \\'faild\\'\\n            else:\\n                return \\'done\\'\\n        df[\\'status\\']=df[\\'result\\'].map(fn)\\n        return df            \\n    \\n    def retry(self):\\n        def _retry_task(task, verbose):\\n            if task.done():\\n                task_exception = task.exception()\\n                if task_exception:\\n                    if verbose: print(task_exception)\\n                    return self.create_task(task.get_name()) \\n                else:\\n                    return task\\n            else:\\n                return task            \\n        self.tasks = [_retry_task(task, self.verbose) for task in self.tasks]\\n    \\n    async def get_result_async(self):\\n        import asyncio\\n        return await asyncio.gather(*self.tasks)\\n\\n    def __repr__(self):\\n        if self.verbose:\\n            print(f\\'tasks_done={self.is_done()}\\')\\n        if (sum(self.is_done())/len(self.is_done()))==1:\\n            return f\"done!\"\\n        else:\\n            return \"running...\"\\n    \\nclass PoolRunner:\\n    \\'\\'\\'\\n    ## Usage example:\\n    def fn(n): return n**2\\n    runner = PoolRunner(fn, range(10))\\n    runner.get_result_now()\\n    runner.get_result_all()\\n    \\'\\'\\'\\n    def __init__(self, func, args_list, delay_second=0.01, verbose=True, max_retry=3):\\n        import asyncio\\n        import pandas as pd\\n        import concurrent.futures\\n        if isinstance(args_list, pd.DataFrame):\\n            self.args_list=args_list.T.to_dict().values()\\n        elif isinstance(args_list, list) or isinstance(args_list, range):     \\n            self.args_list=args_list\\n        else:\\n            raise ValueError(\\'args_list need to be list, pd.DataFrame, or range\\')\\n        self.func = func\\n        self.delay_second = delay_second\\n        self.verbose = verbose\\n        self.max_retry=max_retry\\n        self.pool = concurrent.futures.ThreadPoolExecutor(max_workers=1024)\\n        self.run_pool()\\n        self.result=[]\\n\\n    def create_task(self, args, n_retry):\\n        import time\\n        time.sleep(self.delay_second)\\n        if isinstance(args, dict):\\n            task = self.pool.submit(self.func, **args)\\n        else:\\n            task = self.pool.submit(self.func, args)\\n        return [task, args, n_retry]\\n        \\n    def run_pool(self):\\n        tasks = []\\n        for args in self.args_list:\\n            tasks.append(self.create_task(args, n_retry=0))\\n        self.tasks=tasks\\n    \\n    def is_success(self):\\n        return [task[0].done() for task in self.tasks]\\n    \\n    def get_task_result(self, task):\\n        if task[0].done() or task[2]>self.max_retry:\\n            try:\\n                return task[0].result()\\n            except Exception as e:\\n                if task[2]<self.max_retry: \\n                    return str(e)\\n                else:\\n                    return f\\'Exceeded max_retry ({task[2]-1}|{self.max_retry}): \\'+str(e)\\n        else:\\n            # if task[2]<self.max_retry: \\n                return f\\'pending\\'\\n            # else:\\n            #     return f\\'Exceeded max_retry\\'\\n        \\n    def get_result_now(self, retry=True, sleep_second=1):\\n        if retry:\\n            self.retry()\\n        else:\\n            import time\\n            time.sleep(sleep_second)\\n        if self.verbose:\\n            n1=sum(self.is_success())\\n            n2=len(self.is_success())\\n            print(f\"\\\\r{n1/n2*100:.1f}% ({n1}|{n2}) complete\", end=\\'\\\\n\\')\\n        import json\\n        import pandas as pd\\n        df = pd.DataFrame([task[1] for task in self.tasks])\\n        self.result=[self.get_task_result(task) for task in self.tasks]\\n        df[\\'result\\']=self.result\\n        def fn(r):\\n            if type(r)==str:\\n                if str.startswith(r, \\'Exceeded max_retry\\'):\\n                    return \\'error\\'\\n                else:#elif r==\\'pending\\':\\n                    return \\'running\\'#\\'error_retry\\'\\n            else:\\n                return \\'success\\'\\n        df[\\'status\\']=df[\\'result\\'].map(fn)\\n        return df            \\n    \\n    def retry(self):\\n        def _retry_task(task, verbose):\\n            if task[0].done():\\n                task_exception = task[0].exception()\\n                if task_exception:\\n                    if verbose: \\n                        print(task_exception)\\n                    if (task[2]<self.max_retry):\\n                        if verbose: \\n                            print(f\\'Retry {task[2]+1}|{self.max_retry} for {task[1]}.\\')\\n                        return self.create_task(task[1], n_retry=task[2]+1) \\n                    else:\\n                        if verbose: \\n                            print(f\\'Exceeded Max retry {self.max_retry} for {task[1]}.\\')\\n                        return [task[0], task[1], task[2]+1]\\n                else:\\n                    return task\\n            else:\\n                return task            \\n        self.tasks = [_retry_task(task, self.verbose) for task in self.tasks]\\n    \\n    def get_result_all(self, timeout=120):\\n        import time\\n        for i in range(timeout):\\n            df=self.get_result_now(retry=True)\\n            # if (df.status==\\'success\\').mean()==1:\\n            if (df.status==\\'running\\').mean()==0:\\n                break\\n            else:\\n                time.sleep(1)\\n        if self.verbose:\\n            print(f\"Done!\")\\n        return df\\n\\n    def get_error(self, sleep_second=3):\\n        df=self.get_result_now(retry=False, sleep_second=sleep_second)\\n        if self.verbose:\\n            print(\\'Status Summary:\\', df.status.value_counts().to_dict())\\n            error_mask=df.status==\\'error\\'\\n            if sum(error_mask)==0:\\n                print(\\'No error.\\')\\n            else:\\n                df_error = df[error_mask]\\n                for i in range(len(df_error)): \\n                    print(f\\'\\\\nROW: {i} | ERROR {\"=\"*30}\\')\\n                    for col in df_error.columns:  \\n                        print(f\\'{str(col).upper()}: {df_error.iloc[i][col]}\\')\\n        return df\\n    def get_concat(self, sleep_second=0, verbose=None):\\n        if verbose != None:\\n            self.verbose=verbose\\n        import pandas as pd\\n        df = self.get_error(sleep_second=sleep_second)\\n        mask=df.status==\\'success\\'\\n        if mask.sum()==0:\\n            return \\n        else:\\n            results=[i for i in df[mask].result if i is not None]\\n            if self.verbose:\\n                print(f\"{100*mask.mean().round(3)} percent success.\")\\n                if len(results)!=mask.sum():\\n                    print(f\"Warnning: {mask.sum()-len(results)}/{mask.sum()} are None values.\")\\n        return pd.concat(results)\\n    def __repr__(self):\\n        if self.verbose:\\n            print(f\\'tasks_success={self.is_success()}\\')\\n        if (sum(self.is_success())/len(self.is_success()))==1:\\n            return f\"success!\"\\n        else:\\n            return \"running...\"\\n            \\n@fused.cache\\ndef get_parquet_stats(path):\\n    import pyarrow.parquet as pq\\n    import pandas as pd\\n    # Load Parquet file\\n    parquet_file = pq.ParquetFile(path)\\n    \\n    # List to store the metadata for each row group\\n    stats_list = []\\n\\n    # Iterate through row groups\\n    for i in range(parquet_file.num_row_groups):\\n        row_group = parquet_file.metadata.row_group(i)\\n        \\n        # Dictionary to store row group\\'s statistics\\n        row_stats = {\\'row_group\\': i, \\'num_rows\\': row_group.num_rows}\\n        \\n        # Iterate through columns and gather statistics\\n        for j in range(row_group.num_columns):\\n            column = row_group.column(j)\\n            stats = column.statistics\\n            \\n            if stats:\\n                col_name = column.path_in_schema\\n                row_stats[f\"{col_name}_min\"] = stats.min\\n                row_stats[f\"{col_name}_max\"] = stats.max\\n                row_stats[f\"{col_name}_null_count\"] = stats.null_count\\n        \\n        # Append the row group stats to the list\\n        stats_list.append(row_stats)\\n    \\n    # Convert the list to a DataFrame\\n    df_stats = pd.DataFrame(stats_list)\\n    \\n    return df_stats\\n\\n@fused.cache\\ndef get_row_groups(key, value, file_path):\\n    version=\\'1.0\\'\\n    df = get_parquet_stats(file_path)[[\\'row_group\\', key+\\'_min\\', key+\\'_max\\']]\\n    con = duckdb_connect()\\n    df = con.query(f\\'select * from df where {value} between {key}_min and {key}_max\\').df()\\n    return df.row_group.values\\n\\ndef read_row_groups(file_path, chunk_ids, columns=None):\\n    import pyarrow.parquet as pq   \\n    table=pq.ParquetFile(file_path)   \\n    if columns:\\n        return table.read_row_groups(chunk_ids, columns=columns).to_pandas()   \\n    else: \\n        print(\\'available columns:\\', table.schema.names)\\n        return table.read_row_groups(chunk_ids).to_pandas() \\n\\n\\ndef tiff_bbox(url):\\n    import rasterio\\n    import shapely\\n    import geopandas as gpd\\n    with rasterio.open(url) as dataset:\\n        gpd.GeoDataFrame(geometry=[shapely.box(*dataset.bounds)],crs=dataset.crs)\\n        return list(dataset.bounds)\\n    \\ndef s3_to_https(path):\\n    arr = path[5:].split(\\'/\\')\\n    out = \\'https://\\'+arr[0]+\\'.s3.amazonaws.com/\\'+\\'/\\'.join(arr[1:])\\n    return out\\n\\ndef get_ip():\\n    import socket\\n    hostname=socket.gethostname()\\n    IPAddr=socket.gethostbyname(hostname)\\n    return IPAddr\\n    \\n\\ndef scipy_voronoi(gdf):\\n    \"\"\"\\n    Scipy based Voronoi function. Built because fused version at time is on geopandas 0.14.4 which \\n    doesnt\\' have `gdf.geometry.voronoi_polygons()`\\n    Probably not the most optimised funciton but it gets the job done. \\n    Irrelevant once we move to geopandas 1.0+\\n    \"\"\"\\n    from shapely.geometry import Polygon, Point\\n    from scipy.spatial import Voronoi\\n\\n    points = np.array([(geom.x, geom.y) for geom in gdf.geometry])\\n    vor = Voronoi(points)\\n    \\n    polygons = []\\n    for region in vor.regions:\\n        if not region or -1 in region:  # Ignore regions with open boundaries\\n            continue\\n        \\n        # Get the vertices for the region and construct a polygon\\n        polygon = Polygon([vor.vertices[i] for i in region])\\n        polygons.append(polygon)\\n\\n    voronoi_gdf = gpd.GeoDataFrame(geometry=polygons, crs=gdf.crs)\\n    return voronoi_gdf\\n\\n\\n\\n\\n\\ndef estimate_zoom(bounds, target_num_tiles=1):\\n    \"\"\"\\n    Estimate the zoom level for a given bounding box.\\n\\n    This method returns the zoom level at which a tile exists that, potentially\\n    shifted slightly, fully covers the bounding box.\\n    \\n    Args:\\n        bounds: A list of 4 coordinates (minx, miny, maxx, maxy), a\\n                GeoDataFrame or Shapely geometry, or a mercantile Tile.\\n        target_num_tiles: Target number of tiles to cover the bounds (default=1).\\n                      If 1, finds the zoom where a single tile covers the bounds.\\n                      If >1, estimates zoom to achieve approximately this many tiles.\\n    \\n    Returns:\\n        The estimated zoom level (0-20).\\n    \"\"\"\\n    from fused._optional_deps import (\\n        GPD_GEODATAFRAME,\\n        HAS_GEOPANDAS,\\n        HAS_MERCANTILE,\\n        HAS_SHAPELY,\\n        MERCANTILE_TILE,\\n        SHAPELY_GEOMETRY,\\n    )\\n\\n    # Process input bounds to get standard format\\n    if HAS_GEOPANDAS and isinstance(bounds, GPD_GEODATAFRAME):\\n        bounds = bounds.total_bounds\\n    elif HAS_SHAPELY and isinstance(bounds, SHAPELY_GEOMETRY):\\n        bounds = bounds.bounds\\n    elif HAS_MERCANTILE and isinstance(bounds, MERCANTILE_TILE):\\n        return bounds.z\\n    elif not isinstance(bounds, list):\\n        raise TypeError(f\"Invalid bounds type: {type(bounds)}\")\\n\\n    if not HAS_MERCANTILE:\\n        raise ImportError(\"This function requires the mercantile package.\")\\n    \\n    import mercantile\\n    import math\\n    \\n\\n    if target_num_tiles == 1:\\n        minx, miny, maxx, maxy = bounds\\n        centroid = (minx + maxx) / 2, (miny + maxy) / 2\\n        width = (maxx - minx) - 1e-11\\n        height = (maxy - miny) - 1e-11\\n        \\n        for z in range(20, 0, -1):\\n            tile = mercantile.tile(*centroid, zoom=z)\\n            west, south, east, north = mercantile.bounds(tile)\\n            if width <= (east - west) and height <= (north - south):\\n                break\\n        return z\\n    \\n\\n    else:\\n        minx, miny, maxx, maxy = bounds\\n        miny = max(miny,-89.9999993) #there is a bug in the mercentile that adds an epsilon to lat lngs and causes issue\\n        maxy = min(maxy,89.9999993) #there is a bug in the mercentile that adds an epsilon to lat lngs and causes issue\\n        max_zoom = 20\\n        x_min, y_min, _ = mercantile.tile(minx, maxy, max_zoom)\\n        x_max, y_max, _ = mercantile.tile(maxx, miny, max_zoom)\\n        delta_x = x_max - x_min + 1\\n        delta_y = y_max - y_min + 1\\n\\n        zoom = math.log2(math.sqrt(target_num_tiles) / max(delta_x, delta_y)) + max_zoom\\n        zoom = int(math.floor(zoom)) \\n        current_num_tiles = len(mercantile_polyfill(bounds, zooms=[zoom], compact=False))\\n        if current_num_tiles>=target_num_tiles:\\n            return zoom\\n        else:\\n            return zoom+1\\n\\n\\ndef get_tiles(\\n    bounds=None, target_num_tiles=1, zoom=None, max_tile_recursion=6, as_gdf=True\\n):\\n    bounds = to_gdf(bounds)\\n    import mercantile\\n\\n    if zoom is not None:\\n        print(\"zoom is provided; target_num_tiles will be ignored.\")\\n        target_num_tiles = None\\n\\n    if target_num_tiles is not None and target_num_tiles < 1:\\n        raise ValueError(\"target_num_tiles should be more than zero.\")\\n\\n    if target_num_tiles == 1:\\n        \\n        tile = mercantile.bounding_tile(*bounds.total_bounds)\\n        print(to_gdf((0,0,0)))\\n        gdf = to_gdf((tile.x, tile.y, tile.z))\\n    else:\\n        zoom_level = (\\n            zoom\\n            if zoom is not None\\n            else estimate_zoom(bounds, target_num_tiles=target_num_tiles)\\n        )\\n        base_zoom = estimate_zoom(bounds, target_num_tiles=1)\\n        if zoom_level > (base_zoom + max_tile_recursion + 1):\\n            zoom_level = base_zoom + max_tile_recursion + 1\\n            if zoom:\\n                print(\\n                    f\"Warning: Maximum number of tiles is reached ({zoom=} > {base_zoom+max_tile_recursion+1=} tiles). Increase {max_tile_recursion=} to allow for deeper tile recursion\"\\n                )\\n            else:\\n                print(\\n                    f\"Warning: Maximum number of tiles is reached ({target_num_tiles} > {4**max_tile_recursion-1} tiles). Increase {max_tile_recursion=} to allow for deeper tile recursion\"\\n                )\\n\\n        gdf = mercantile_polyfill(bounds, zooms=[zoom_level], compact=False)\\n        print(f\"Generated {len(gdf)} tiles at zoom level {zoom_level}\")\\n\\n    return gdf if as_gdf else gdf[[\"x\", \"y\", \"z\"]].values\\n\\ndef get_utm_epsg(geometry):\\n    utm_zone = int((geometry.centroid.x + 180) / 6) + 1\\n    return 32600 + utm_zone if geometry.centroid.y >= 0 else 32700 + utm_zone  # 326XX for Northern Hemisphere, 327XX for Southern\\n\\n\\ndef add_utm_area(gdf, utm_col=\\'utm_epsg\\', utm_area_col=\\'utm_area_sqm\\'):\\n    import geopandas as gpd\\n\\n    # Step 1: Compute UTM zones\\n    gdf[utm_col] = gdf.geometry.apply(get_utm_epsg)\\n\\n    # Step 2: Compute areas in batches while preserving order\\n    areas_dict = {}\\n\\n    for utm_zone, group in gdf.groupby(utm_col, group_keys=False):\\n        utm_crs = f\"EPSG:{utm_zone}\"\\n        reprojected = group.to_crs(utm_crs)  # Reproject all geometries in batch\\n        areas_dict.update(dict(zip(group.index, reprojected.area)))  # Store areas by index\\n\\n    # Step 3: Assign areas back to original gdf order\\n    gdf[utm_area_col] = gdf.index.map(areas_dict)\\n    return gdf\\n\\n\\ndef run_submit_with_defaults(udf_token: str, cache_length: str = \"9999d\", default_params_token: Optional[str] = None):\\n    \"\"\"\\n    Uses fused.submit() to run a UDF over:\\n    - A UDF that returns a pd.DataFrame of test arguments (`default_params_token`)\\n    - Or default params (expectes udf.utils.submit_default_params to return a pd.DataFrame)\\n    \"\"\"\\n    \\n    # Assume people know what they\\'re doing \\n    try:\\n        # arg_token is a UDF that returns a pd.DataFrame of test arguments\\n        arg_list = fused.run(default_params_token)\\n\\n        if \\'bounds\\' in arg_list.columns:\\n            # This is a hacky workaround for now as we can\\'t pass np.float bounds to `fused.run(udf, bounds) so need to convert them to float\\n            # but fused.run() returns bounds as `np.float` for whatever reason\\n            arg_list[\\'bounds\\'] = arg_list[\\'bounds\\'].apply(lambda bounds_list: [float(x) for x in bounds_list])\\n            \\n        print(f\"Loaded default params from UDF {default_params_token}... Running UDF over these\")\\n    except Exception as e:\\n        print(f\"Couldn\\'t load UDF {udf_token} with arg_token {default_params_token}, trying to load default params...\")\\n        \\n        try:\\n            udf = fused.load(udf_token)\\n            \\n            # Assume we have a funciton called \\'submit_default_params` inside the main UDF which returns a pd.DataFrame of test arguments\\n            # TODO: edit this to directly use `udf.submit_default_params()` once we remove utils\\n            if hasattr(udf.utils, \"submit_default_params\"):\\n                print(\"Found default params for UDF, using them...\")\\n                arg_list = udf.utils.submit_default_params()\\n            else:\\n                raise ValueError(\"No default params found for UDF, can\\'t run this UDF\")\\n\\n        except Exception as e:\\n            raise ValueError(\"Couldn\\'t load UDF, can\\'t run this UDF. Try with another UDF\")\\n        \\n        #TODO: Add support for using the default view state\\n\\n    return fused.submit(\\n        udf_token,\\n        arg_list,\\n        cache_max_age=cache_length,\\n        wait_on_results=True,\\n    )\\n\\ndef test_udf(udf_token: str, cache_length: str = \"9999d\", arg_token: Optional[str] = None):\\n    \"\"\"\\n    Testing a UDF:\\n    1. Does it run and return successful result for all its default parameters?\\n    2. Are the results identical to the cached results?\\n\\n    Returns:\\n    - all_passing: True if the UDF runs and returns successful result for all its default parameters\\n    - all_equal: True if the results are identical to the cached results\\n    - prev_run: Cached UDF output\\n    - current_run: New UDF output\\n    \"\"\"\\n    import pickle\\n\\n    cached_run = run_submit_with_defaults(udf_token, cache_length, arg_token)\\n    current_run = run_submit_with_defaults(udf_token, \"0s\", arg_token)\\n\\n    # Check if results are valid\\n    all_passing = (current_run[\"status\"] == \"success\").all()\\n    # Check if result matches cached result\\n    all_equal = pickle.dumps(cached_run) == pickle.dumps(current_run)\\n    return (bool(all_passing), all_equal, cached_run, current_run)\\n\\n  \\ndef save_to_agent(\\n    agent_json_path: str, udf: AnyBaseUdf, agent_name: str, udf_name: str, mcp_metadata: dict[str, Any], overwrite: bool = True,\\n):\\n    \"\"\"\\n    Save UDF to agent of udf_ai directory\\n    Args:\\n        agent_json_path (str): Absolute path to the agent.json file\\n        agent_name (str): Name of the agent\\n        udf (AnyBaseUdf): UDF to save\\n        udf_name (str): Name of the UDF\\n        mcp_metadata (dict[str, Any]): MCP metadata\\n        overwrite (bool): If True, overwrites any existing UDF directory with current `udf`\\n    \"\"\"\\n    # load agent.json\\n    if os.path.exists(agent_json_path):\\n        agent_json = json.load(open(agent_json_path))\\n    else:\\n        agent_json = {\"agents\": []}\\n    repo_dir = os.path.dirname(agent_json_path)\\n\\n    # save udf to repo\\n    udf.metadata = {}\\n    udf.name = udf_name\\n    if not mcp_metadata.get(\"description\") or not mcp_metadata.get(\"parameters\"):\\n        raise ValueError(\"mcp_metadata must have description and parameters\")\\n    udf.metadata[\"fused:mcp\"] = mcp_metadata\\n    udf.to_directory(f\"{repo_dir}/udfs/{udf_name}\", overwrite=overwrite)\\n\\n    if agent_name in [agent[\"name\"] for agent in agent_json[\"agents\"]]:\\n        for agent in agent_json[\"agents\"]:\\n            if agent[\"name\"] == agent_name:\\n                # Only append udf_name if it doesn\\'t already exist in the agent\\'s udfs list\\n                if udf_name not in agent[\"udfs\"]:\\n                    agent[\"udfs\"].append(udf_name)\\n                break\\n    else:\\n        agent_json[\"agents\"].append({\"name\": agent_name, \"udfs\": [udf_name]})\\n\\n    # save agent.json\\n    json.dump(agent_json, open(agent_json_path, \"w\"), indent=4)\\n\\ndef generate_local_mcp_config(config_path: str, agents_list: list[str], repo_path: str, uv_path: str = \\'uv\\', script_path: str = \\'main.py\\'):\\n    \"\"\"\\n    Generate MCP configuration file based on list of agents from the udf_ai directory\\n    Args:\\n        config_path (str): Absolute path to the MCP configuration file.\\n        agents_list (list[str]): List of agent names to be included in the configuration.\\n        repo_path (str): Absolute path to the locally cloned udf_ai repo directory.\\n        uv_path (str): Path to `uv`. Defaults to `uv` but might require your local path to `uv`.\\n        script_path (str): Path to the script to run. Defaults to `run.py`.\\n    \"\"\"\\n    if not os.path.exists(repo_path):\\n        raise ValueError(f\"Repository path {repo_path} does not exist\")\\n\\n    # load agent.json containing agent to udfs mapping\\n    agent_json = json.load(open(f\"{repo_path}/agents.json\", \"rt\"))\\n\\n    # create config json for all agents\\n    config_json = {\"mcpServers\": {}}\\n\\n    for agent_name in agents_list:\\n        agent = next(\\n            (agent for agent in agent_json[\"agents\"] if agent[\"name\"] == agent_name),\\n            None,\\n        )\\n        if not agent or not agent[\"udfs\"]:\\n            raise ValueError(f\"No UDFs found for agent {agent_name}\")\\n\\n        agent_config = {\\n            \"command\": uv_path,\\n            \"args\": [\\n                \"run\",\\n                \"--directory\",\\n                f\"{repo_path}\",\\n                f\"{script_path}\",\\n                \"--runtime=local\",\\n                f\"--udf-names={\\',\\'.join(agent[\\'udfs\\'])}\",\\n                f\"--name={agent_name}\",\\n            ],\\n        }\\n        config_json[\"mcpServers\"][agent_name] = agent_config\\n\\n    # save config json\\n    json.dump(config_json, open(config_path, \"w\"), indent=4)\\n\\n' source_file='utils.py'\n"
     ]
    }
   ],
   "source": [
    "# We'll load the commons folder once again to have our helper functions\n",
    "commit = \"5dda36c\"\n",
    "common = fused.load(\n",
    "    f\"https://github.com/fusedio/udfs/tree/{commit}/public/common\"\n",
    ").utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b43b873-cb47-4fdf-b73d-aceda9a69b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agents': [{'name': 'get_current_time', 'udfs': ['current_utc_time']},\n",
       "  {'name': 'fused_docs', 'udfs': ['list_public_udfs', 'reading_fused_docs']},\n",
       "  {'name': 'vancouver_open_data',\n",
       "   'udfs': ['hundred_parks_in_vancouver', 'internet_speeds_for_lat_lon']},\n",
       "  {'name': 'elevation_stats_for_lat_lon_area', 'udfs': ['elevation_stats']},\n",
       "  {'name': 'dynamic_output_vector', 'udfs': ['dynamic_output_vector_udf']},\n",
       "  {'name': 'vancouver_open_data_demo',\n",
       "   'udfs': ['hundred_parks_in_vancouver',\n",
       "    'electric_vehicle_chargers_in_vancouver',\n",
       "    'yearly_crime_amount',\n",
       "    'internet_speeds_for_lat_lon',\n",
       "    'community_gardens_vancouver']},\n",
       "  {'name': 'world_countries_info', 'udfs': ['get_country_data']},\n",
       "  {'name': 'routing_agent', 'udfs': ['single_routing_agent']},\n",
       "  {'name': 'stock_information', 'udfs': ['get_stock_details']},\n",
       "  {'name': 'tourism_info', 'udfs': ['visitors_arrival', 'visitors_purpose']}]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And see which agents we have available\n",
    "json.load(open(os.path.join(WORKING_DIR, \"agents.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a8b21b6-8a80-4637-bcae-64fe0ecd3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_NAME = \"tourism_info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6c9fe6b5-7563-4e5b-9ffd-ba7bfba4ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fused.udf\n",
    "def visitor_info(year: int = 2024, country: str = None):\n",
    "    \"\"\"\n",
    "    UDF to retrieve visitor arrival information for a given year and optional country.\n",
    "    Defaults to 2024 if no year is provided. If no country is specified, returns data for all countries.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import fused\n",
    "    from io import BytesIO\n",
    "    import requests\n",
    "    \n",
    "    drive_url = \"https://drive.google.com/uc?export=download&id=1QoookSZMXWs3eNQ-q0Bsxq4tCB_KH4ux\"\n",
    "    \n",
    "    # @fused.cache\n",
    "    def load_csv():\n",
    "        response = requests.get(drive_url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(BytesIO(response.content))\n",
    "    \n",
    "    # Load data\n",
    "    df = load_csv()\n",
    "    \n",
    "    # Convert 'Year' column to integer type\n",
    "    df['Year'] = df['Year'].astype(int)\n",
    "    \n",
    "    # Apply year filter\n",
    "    result = df[df['Year'] == year]\n",
    "    \n",
    "    # Apply country filter if provided, otherwise return all countries\n",
    "    if country:\n",
    "        result = result[result['Country'] == country]\n",
    "    \n",
    "    # Return the filtered DataFrame or an empty DataFrame if no data is found\n",
    "    return result if not result.empty else pd.DataFrame(columns=df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "38f803bb-9ac4-44fc-bb8c-0f422b981954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Growth Rate(%)</th>\n",
       "      <th>Country</th>\n",
       "      <th>Visitor Arrivals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2024</td>\n",
       "      <td>47.09021457</td>\n",
       "      <td>Japan</td>\n",
       "      <td>36,870,148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2024</td>\n",
       "      <td>18.8</td>\n",
       "      <td>India</td>\n",
       "      <td>9,500,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year Growth Rate(%) Country Visitor Arrivals\n",
       "60  2024    47.09021457   Japan       36,870,148\n",
       "66  2024           18.8   India        9,500,000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused.run(visitor_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6dd0fa85-6713-4809-8ba3-3c7df6bee55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"\\nName: visitor_info\\nPurpose and Functionality:\\nThe UDF 'visitor_info' retrieves visitor arrival data for a specified year from a CSV file. If no year is provided, it defaults to 1964. The function reads the dataset, filters the data based on the requested year, and returns a DataFrame containing visitor statistics.\\n\\nInput Parameters:\\n- year (int, optional): The year for which visitor arrival data is requested. Defaults to 1964.\\n\\nOutput:\\nThe function returns a Pandas DataFrame containing visitor arrival details for the specified year. If no records exist for the given year, an empty DataFrame with the appropriate column structure is returned.\\n\\nTechnical Details and Limitations:\\n- The function reads data from a local CSV file ('visitors_arrival.csv'). The CSV file must be available in the expected format.\\n- If the CSV file is missing or corrupt, the function may raise an error.\\n- The function ensures that the 'Year' column is treated as an integer for filtering.\\n- If an invalid year (e.g., a non-integer string) is provided, the function may fail unless additional validation is implemented.\\n- The function caches the CSV file using fused.cache to improve efficiency.\\n    \",\n",
       " 'parameters': [{'name': 'year',\n",
       "   'type': 'integer',\n",
       "   'default': 1964,\n",
       "   'description': 'The year for which visitor arrival data is requested. Defaults to 1964.'}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitor_info_metadata = {\n",
    "    \"description\": \"\"\"\n",
    "Name: visitor_info\n",
    "Purpose and Functionality:\n",
    "The UDF 'visitor_info' retrieves visitor arrival data for a specified year from a CSV file. If no year is provided, it defaults to 1964. The function reads the dataset, filters the data based on the requested year, and returns a DataFrame containing visitor statistics.\n",
    "\n",
    "Input Parameters:\n",
    "- year (int, optional): The year for which visitor arrival data is requested. Defaults to 1964.\n",
    "\n",
    "Output:\n",
    "The function returns a Pandas DataFrame containing visitor arrival details for the specified year. If no records exist for the given year, an empty DataFrame with the appropriate column structure is returned.\n",
    "\n",
    "Technical Details and Limitations:\n",
    "- The function reads data from a local CSV file ('visitors_arrival.csv'). The CSV file must be available in the expected format.\n",
    "- If the CSV file is missing or corrupt, the function may raise an error.\n",
    "- The function ensures that the 'Year' column is treated as an integer for filtering.\n",
    "- If an invalid year (e.g., a non-integer string) is provided, the function may fail unless additional validation is implemented.\n",
    "- The function caches the CSV file using fused.cache to improve efficiency.\n",
    "    \"\"\",\n",
    "    \"parameters\": [\n",
    "        {\n",
    "            \"name\": \"year\",\n",
    "            \"type\": \"integer\",\n",
    "            \"default\": 1964,\n",
    "            \"description\": \"The year for which visitor arrival data is requested. Defaults to 1964.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "visitor_info_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3fed6c00-d8d5-4316-a07f-8c7273c14129",
   "metadata": {},
   "outputs": [],
   "source": [
    "common.save_to_agent(\n",
    "    agent_json_path=os.path.join(WORKING_DIR, \"agents.json\"),\n",
    "    udf=visitor_info,\n",
    "    agent_name=AGENT_NAME,\n",
    "    udf_name=\"visitors_arrival\",\n",
    "    mcp_metadata=visitor_info_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00c45942-9794-48ac-9100-695d9cc5744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@fused.udf\n",
    "def get_visitor_data(year: int = None, country: str = None):\n",
    "    \"\"\"\n",
    "    UDF to retrieve visitor arrival data with optional filters.\n",
    "\n",
    "    Parameters:\n",
    "    - year (int): Filter by specific year (e.g., 2023)\n",
    "    - country (str): Filter by specific country name (e.g., 'Thailand')\n",
    "\n",
    "    Returns:\n",
    "    A DataFrame with the following columns:\n",
    "    - Country\n",
    "    - Purpose_of_visit_to_Japan\n",
    "    - Year\n",
    "    - Growth Rate(%)\n",
    "    - Visitor Arrivals\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import fused\n",
    "    from io import BytesIO\n",
    "    import requests\n",
    "\n",
    "    drive_url = \"https://drive.google.com/uc?export=download&id=1OQkx2fLA6oQ3fzNL-aiCFqDr65bp-nAL\"\n",
    "\n",
    "    # @fused.cache\n",
    "    def load_csv():\n",
    "        response = requests.get(drive_url)\n",
    "        response.raise_for_status()\n",
    "        return pd.read_csv(BytesIO(response.content), encoding='utf-8')\n",
    "\n",
    "    # Load data\n",
    "    df = load_csv()\n",
    "\n",
    "    # Rename for consistency\n",
    "    df.rename(columns={\"Country/Area\": \"Country\"}, inplace=True)\n",
    "\n",
    "    # Ensure Year is numeric\n",
    "    df['Year'] = pd.to_numeric(df['Year'], errors='coerce').astype('Int64')\n",
    "\n",
    "    # Optional filters\n",
    "    if year is not None:\n",
    "        df = df[df['Year'] == year]\n",
    "\n",
    "    if country:\n",
    "        df = df[df['Country'] == country]\n",
    "\n",
    "    # Columns to return\n",
    "    columns = [\n",
    "        'Country',\n",
    "        'Purpose_of_visit_to_Japan',\n",
    "        'Year',\n",
    "        'Growth Rate(%)',\n",
    "        'Visitor Arrivals'\n",
    "    ]\n",
    "\n",
    "    return df[columns] if not df.empty else pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fdb56b9b-683d-4813-8056-f30349b304df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Purpose_of_visit_to_Japan</th>\n",
       "      <th>Year</th>\n",
       "      <th>Growth Rate(%)</th>\n",
       "      <th>Visitor Arrivals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>1990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>1991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>1992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29,147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>1993</td>\n",
       "      <td>-9.239372834</td>\n",
       "      <td>26,454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China</td>\n",
       "      <td>Tourism</td>\n",
       "      <td>1994</td>\n",
       "      <td>-7.197399259</td>\n",
       "      <td>24,550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>United States</td>\n",
       "      <td>Business</td>\n",
       "      <td>2020</td>\n",
       "      <td>-86.664479278</td>\n",
       "      <td>28,857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>United States</td>\n",
       "      <td>Business</td>\n",
       "      <td>2021</td>\n",
       "      <td>-95.515819385</td>\n",
       "      <td>1,294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>United States</td>\n",
       "      <td>Business</td>\n",
       "      <td>2022</td>\n",
       "      <td>4,408.191653787</td>\n",
       "      <td>58,336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>United States</td>\n",
       "      <td>Business</td>\n",
       "      <td>2023</td>\n",
       "      <td>104.588933077</td>\n",
       "      <td>119,349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>United States</td>\n",
       "      <td>Business</td>\n",
       "      <td>2024</td>\n",
       "      <td>3.499819856</td>\n",
       "      <td>123,526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Country Purpose_of_visit_to_Japan  Year   Growth Rate(%)  \\\n",
       "0             China                   Tourism  1990              NaN   \n",
       "1             China                   Tourism  1991              NaN   \n",
       "2             China                   Tourism  1992              NaN   \n",
       "3             China                   Tourism  1993     -9.239372834   \n",
       "4             China                   Tourism  1994     -7.197399259   \n",
       "...             ...                       ...   ...              ...   \n",
       "1045  United States                  Business  2020    -86.664479278   \n",
       "1046  United States                  Business  2021    -95.515819385   \n",
       "1047  United States                  Business  2022  4,408.191653787   \n",
       "1048  United States                  Business  2023    104.588933077   \n",
       "1049  United States                  Business  2024      3.499819856   \n",
       "\n",
       "     Visitor Arrivals  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2              29,147  \n",
       "3              26,454  \n",
       "4              24,550  \n",
       "...               ...  \n",
       "1045           28,857  \n",
       "1046            1,294  \n",
       "1047           58,336  \n",
       "1048          119,349  \n",
       "1049          123,526  \n",
       "\n",
       "[1050 rows x 5 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused.run(get_visitor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4eeb93c-2b8a-4f4c-8c25-e74245e41d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': \"\\nName: get_visitor_data\\nPurpose and Functionality:\\nThe UDF 'get_visitor_data' provides detailed visitor arrival statistics from a structured dataset sourced from a CSV file hosted on Google Drive. It allows optional filtering by year and country, and returns comprehensive information including purpose of visit, growth rate, and number of visitor arrivals.\\n\\nInput Parameters:\\n- year (int, optional): The year for which detailed visitor data is requested. If not specified, data for all available years will be returned.\\n- country (str, optional): The country for which visitor data is requested. If not specified, data for all countries will be returned.\\n\\nOutput:\\nReturns a Pandas DataFrame with the following columns:\\n- Country/Area\\n- Purpose_of_visit_to_Japan\\n- Year\\n- Growth Rate(%)\\n- Visitor Arrivals\\n\\nIf no matching records are found, an empty DataFrame with the above column structure is returned.\\n\\nTechnical Details and Limitations:\\n- The CSV is fetched from a public Google Drive link using HTTP requests. Internet access is required.\\n- The 'Year' column is cast to integer for filtering.\\n- The function gracefully handles missing or empty fields, such as blank growth rates or visitor counts.\\n- If the source format of the CSV changes, the behavior may be affected.\\n    \",\n",
       " 'parameters': [{'name': 'year',\n",
       "   'type': 'integer',\n",
       "   'default': None,\n",
       "   'description': 'The year to filter visitor data. Optional.'},\n",
       "  {'name': 'country',\n",
       "   'type': 'string',\n",
       "   'default': None,\n",
       "   'description': 'The country to filter visitor data. Optional.'}]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_visitor_data_metadata = {\n",
    "    \"description\": \"\"\"\n",
    "Name: get_visitor_data\n",
    "Purpose and Functionality:\n",
    "The UDF 'get_visitor_data' provides detailed visitor arrival statistics from a structured dataset sourced from a CSV file hosted on Google Drive. It allows optional filtering by year and country, and returns comprehensive information including purpose of visit, growth rate, and number of visitor arrivals.\n",
    "\n",
    "Input Parameters:\n",
    "- year (int, optional): The year for which detailed visitor data is requested. If not specified, data for all available years will be returned.\n",
    "- country (str, optional): The country for which visitor data is requested. If not specified, data for all countries will be returned.\n",
    "\n",
    "Output:\n",
    "Returns a Pandas DataFrame with the following columns:\n",
    "- Country/Area\n",
    "- Purpose_of_visit_to_Japan\n",
    "- Year\n",
    "- Growth Rate(%)\n",
    "- Visitor Arrivals\n",
    "\n",
    "If no matching records are found, an empty DataFrame with the above column structure is returned.\n",
    "\n",
    "Technical Details and Limitations:\n",
    "- The CSV is fetched from a public Google Drive link using HTTP requests. Internet access is required.\n",
    "- The 'Year' column is cast to integer for filtering.\n",
    "- The function gracefully handles missing or empty fields, such as blank growth rates or visitor counts.\n",
    "- If the source format of the CSV changes, the behavior may be affected.\n",
    "    \"\"\",\n",
    "    \"parameters\": [\n",
    "        {\n",
    "            \"name\": \"year\",\n",
    "            \"type\": \"integer\",\n",
    "            \"default\": None,\n",
    "            \"description\": \"The year to filter visitor data. Optional.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"country\",\n",
    "            \"type\": \"string\",\n",
    "            \"default\": None,\n",
    "            \"description\": \"The country to filter visitor data. Optional.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "get_visitor_data_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2eaa769-5bf3-43ad-93de-e1b0bf101004",
   "metadata": {},
   "outputs": [],
   "source": [
    "common.save_to_agent(\n",
    "    agent_json_path=os.path.join(WORKING_DIR, \"agents.json\"),\n",
    "    udf=get_visitor_data,\n",
    "    agent_name=AGENT_NAME,\n",
    "    udf_name=\"visitors_purpose\",\n",
    "    mcp_metadata=get_visitor_data_metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb5ec84d-2d35-46ed-87db-340ec830d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"agents\": [\n",
      "        {\n",
      "            \"name\": \"get_current_time\",\n",
      "            \"udfs\": [\n",
      "                \"current_utc_time\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"fused_docs\",\n",
      "            \"udfs\": [\n",
      "                \"list_public_udfs\",\n",
      "                \"reading_fused_docs\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"vancouver_open_data\",\n",
      "            \"udfs\": [\n",
      "                \"hundred_parks_in_vancouver\",\n",
      "                \"internet_speeds_for_lat_lon\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"elevation_stats_for_lat_lon_area\",\n",
      "            \"udfs\": [\n",
      "                \"elevation_stats\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"dynamic_output_vector\",\n",
      "            \"udfs\": [\n",
      "                \"dynamic_output_vector_udf\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"vancouver_open_data_demo\",\n",
      "            \"udfs\": [\n",
      "                \"hundred_parks_in_vancouver\",\n",
      "                \"electric_vehicle_chargers_in_vancouver\",\n",
      "                \"yearly_crime_amount\",\n",
      "                \"internet_speeds_for_lat_lon\",\n",
      "                \"community_gardens_vancouver\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"world_countries_info\",\n",
      "            \"udfs\": [\n",
      "                \"get_country_data\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"routing_agent\",\n",
      "            \"udfs\": [\n",
      "                \"single_routing_agent\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"stock_information\",\n",
      "            \"udfs\": [\n",
      "                \"get_stock_details\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"tourism_info\",\n",
      "            \"udfs\": [\n",
      "                \"visitors_arrival\",\n",
      "                \"visitors_purpose\"\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure we created our agent properly, with all our UDFs\n",
    "agents = json.load(open(os.path.join(WORKING_DIR, \"agents.json\")))\n",
    "print(json.dumps(agents, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a70f7f47-8919-434e-a38d-c5f432e925d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tourism_info'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f46667e9-d383-4af3-abb8-2888a227ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can select which Agent we want to pass to Claude in our MCP server config\n",
    "common.generate_local_mcp_config(\n",
    "    config_path=PATH_TO_CLAUDE_CONFIG,\n",
    "    agents_list=[AGENT_NAME],\n",
    "    repo_path=WORKING_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba666eb9-05cc-4084-b5be-86dc75cac123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"mcpServers\": {\n",
      "        \"tourism_info\": {\n",
      "            \"args\": [\n",
      "                \"run\",\n",
      "                \"--directory\",\n",
      "                \"C:\\\\Users\\\\user\\\\fused-mcp\",\n",
      "                \"main.py\",\n",
      "                \"--runtime=local\",\n",
      "                \"--udf-names=visitors_arrival,visitors_purpose\",\n",
      "                \"--name=tourism_info\"\n",
      "            ],\n",
      "            \"command\": \"uv\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Let's read this Claude Desktop config to see what we're passing\n",
    "claude_config = json.load(open(PATH_TO_CLAUDE_CONFIG))\n",
    "print(json.dumps(claude_config, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e592c67-558e-41db-9055-60ac3b25afa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
